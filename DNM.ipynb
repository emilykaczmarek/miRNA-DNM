{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G2Wt6DsYNhH"
      },
      "source": [
        "import os\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(4)\n",
        "from tempfile import TemporaryFile\n",
        "import sklearn.datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy import stats\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZICuCyEFKd9y"
      },
      "source": [
        "#DNM Code - Adapted from Deep Neural Maps (Pesteie et al 2018) for miRNA data\n",
        "\n",
        "def load_mnist():\n",
        "    (x, _), (_,_) = mnist.load_data()\n",
        "    x = x / 255.\n",
        "    return x.reshape((-1, 1, 28, 28))\n",
        "\n",
        "def save_projection_image(memory, lattice_size, iter):\n",
        "    width = memory.shape[2]\n",
        "    height = memory.shape[1]\n",
        "    proj_map = np.zeros((lattice_size[0] * height, lattice_size[1] * width))\n",
        "    c = 0\n",
        "    for i in range(lattice_size[0]):\n",
        "        for j in range(lattice_size[1]):\n",
        "            patch = memory[c]\n",
        "            proj_map[\n",
        "                i * height:(i + 1) * height, j * width:(j + 1) * width] = patch \n",
        "            c += 1\n",
        "    plt.imshow(255*proj_map)\n",
        "    plt.show()\n",
        "    return proj_map\n",
        "\n",
        "\n",
        "class Adam(object):\n",
        "    def __init__(self, lr=1e-2, b1=9e-1, b2=0.999, e=1e-8, gamma=1 - 1e-8):\n",
        "        self.lr = lr\n",
        "        self.b1 = b1\n",
        "        self.b2 = b2\n",
        "        self.e = e\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def updates(self, cost, params):\n",
        "\n",
        "        updates = []\n",
        "        grads = theano.grad(cost, params)\n",
        "        alpha = self.lr\n",
        "        t = theano.shared(np.float32(1))\n",
        "        b1_t = self.b1 * self.gamma ** (t - 1)  # (Decay the first moment running average coefficient)\n",
        "\n",
        "        for param, grad in zip(params, grads):\n",
        "            m_previous = theano.shared(np.zeros(param.get_value().shape,\n",
        "                                                dtype=theano.config.floatX))\n",
        "            v_previous = theano.shared(np.zeros(param.get_value().shape,\n",
        "                                                dtype=theano.config.floatX))\n",
        "\n",
        "            m = b1_t * m_previous + (1 - b1_t) * grad  # (Update biased first moment estimate)\n",
        "            v = self.b2 * v_previous + (1 - self.b2) * grad ** 2  # (Update biased second raw moment estimate)\n",
        "            m_hat = 1.*m / (1 - self.b1 ** t)  # (Compute bias-corrected first moment estimate)\n",
        "            v_hat = 1.*v / (1 - self.b2 ** t)  # (Compute bias-corrected second raw moment estimate)\n",
        "            theta = param - (alpha * m_hat) / (T.sqrt(v_hat) + self.e)  # (Update parameters)\n",
        "\n",
        "            updates.append((m_previous, m))\n",
        "            updates.append((v_previous, v))\n",
        "            updates.append((param, theta))\n",
        "        updates.append((t, t + 1.))\n",
        "        return updates\n",
        "\n",
        "class BatchFactory(object):\n",
        "    def __init__(self, batch_size, nb_samples, iterations, randomizer=True):\n",
        "        self.BATCH_SIZE = batch_size\n",
        "        self.nb_samples = nb_samples \n",
        "        self.iterations = iterations \n",
        "        self.randomizer = randomizer\n",
        "\n",
        "    def _index_generator(self):\n",
        "        for i in range(self.iterations):\n",
        "            if self.randomizer:\n",
        "                indices = np.random.permutation(self.nb_samples)\n",
        "            else:\n",
        "                indices = np.arange(self.nb_samples)\n",
        "            for indexer in np.arange(self.nb_samples/self.BATCH_SIZE):\n",
        "                indexer = int(indexer)\n",
        "                yield indices[slice(indexer*self.BATCH_SIZE, (indexer+1)*self.BATCH_SIZE)]\n",
        "            if self.nb_samples % self.BATCH_SIZE != 0:\n",
        "                indexer = int(self.nb_samples / self.BATCH_SIZE)\n",
        "                yield indices[slice(indexer*self.BATCH_SIZE, self.nb_samples)]\n",
        "\n",
        "    def generate_batch(self, X, Y=None):\n",
        "        Y = None\n",
        "        for samples in self._index_generator():\n",
        "            if Y is None:\n",
        "                yield X[samples]\n",
        "            else:\n",
        "                yield X[samples], Y[samples]\n",
        "\n",
        "class activations(object):\n",
        "  def sigmoid(x):\n",
        "    return T.nnet.sigmoid(x)\n",
        "\n",
        "  def relu(x):\n",
        "      return T.maximum(0.0, x)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return T.nnet.sigmoid(x)\n",
        "\n",
        "class Ops(object):\n",
        "\n",
        "  def dense(inpt, nb_in, nb_out, layer_name='', init_params=None):\n",
        "      if init_params is None:\n",
        "          w = theano.shared(\n",
        "              np.asarray(\n",
        "                  np.random.normal(\n",
        "                      loc=0, scale=np.sqrt(1. / nb_out), size=[nb_in, nb_out]),\n",
        "                  dtype=theano.config.floatX),\n",
        "              name='w_dense_' + layer_name, borrow=True)\n",
        "          b = theano.shared(np.asarray(np.random.normal(\n",
        "              loc=0.0, scale=1.0, size=[nb_out]),\n",
        "              dtype=theano.config.floatX),\n",
        "              name='b_dense_' + layer_name, borrow=True)\n",
        "      else:\n",
        "          w = init_params[0]\n",
        "          b = init_params[1]\n",
        "      return T.dot(inpt, w) + b, [w, b]\n",
        "\n",
        "\n",
        "  def conv_2d_transpose(inpt, conv2d_params, stride=(1, 1), layer_name='', mode='valid', init_params=None):\n",
        "    output_channel, input_channel, rows, columns = conv2d_params\n",
        "    if init_params is None:\n",
        "        filter_shape = (output_channel, input_channel, rows, columns)\n",
        "        output_shape = (output_channel, input_channel, rows + 2, columns + 2)\n",
        "        receptive_field_size = rows * columns\n",
        "\n",
        "        w = theano.shared(np.asarray(\n",
        "            np.random.normal(\n",
        "                loc=0, scale=np.sqrt(2. / ((input_channel + output_channel) * receptive_field_size)),\n",
        "            size=filter_shape),\n",
        "            dtype=theano.config.floatX), name='w_conv2d_' + layer_name, borrow=True)\n",
        "\n",
        "        b = theano.shared(\n",
        "            np.asarray(\n",
        "                np.random.normal(\n",
        "                    loc=0.0, scale=1.0, size=(\n",
        "                        filter_shape[0],)), dtype=theano.config.floatX),\n",
        "            name='b_conv2d_' + layer_name, borrow=True)\n",
        "    else:\n",
        "        w = init_params[0]\n",
        "        b = init_params[1]\n",
        "    return T.nnet.conv2d_transpose(input=inpt, filters=w, output_shape=())\n",
        "\n",
        "\n",
        "  def conv_2d(inpt, conv2d_params, stride=(1, 1), layer_name='', mode='valid', init_params=None):\n",
        "      output_channel, input_channel, rows, columns = conv2d_params\n",
        "      if init_params is None:\n",
        "          filter_shape = (output_channel, input_channel, rows, columns)\n",
        "          receptive_field_size = rows * columns\n",
        "\n",
        "          w = theano.shared(np.asarray(\n",
        "              np.random.normal(\n",
        "                  loc=0, scale=np.sqrt(2. / ((input_channel + output_channel) * receptive_field_size)),\n",
        "              size=filter_shape),\n",
        "              dtype=theano.config.floatX), name='w_conv2d_' + layer_name, borrow=True)\n",
        "\n",
        "          b = theano.shared(\n",
        "              np.asarray(\n",
        "                  np.random.normal(\n",
        "                      loc=0.0, scale=1.0, size=(\n",
        "                          filter_shape[0],)), dtype=theano.config.floatX),\n",
        "              name='b_conv2d_' + layer_name, borrow=True)\n",
        "      else:\n",
        "          w = init_params[0]\n",
        "          b = init_params[1]\n",
        "      return T.nnet.conv2d(input=inpt, filters=w, border_mode=mode, subsample=stride) + b.dimshuffle('x', 0, 'x', 'x'), [w, b]\n",
        "\n",
        "\n",
        "  def flatten(inpt, ndim=2):\n",
        "    return T.flatten(inpt, ndim)\n",
        "\n",
        "\n",
        "def get_params(layer_name, par_list):\n",
        "  if par_list is None:\n",
        "    return None\n",
        "  pars = []\n",
        "  for i in par_list:\n",
        "    if i.name.split('_')[-1] == layer_name:\n",
        "      pars.append(i)\n",
        "  return None if len(pars) == 0 else pars\n",
        "\n",
        "class SIMPLE_AE(object):\n",
        "\n",
        "\n",
        "    def __init__(self, input_size, latent_size):\n",
        "        self.input_size = input_size\n",
        "        self.latent_size = latent_size\n",
        "\n",
        "    def _encode(self, X, init_params):\n",
        "        params = []\n",
        "        regs = []\n",
        "        e1, pars = Ops.conv_2d(X, (10, 1, 5, 5),\n",
        "                               layer_name='e1',\n",
        "                               mode='half',\n",
        "                               init_params=get_params('e1', init_params))\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        e2, pars = Ops.conv_2d(activations.relu(e1),\n",
        "                               (8, 10, 5, 5),\n",
        "                               layer_name='e2',\n",
        "                               mode='half',\n",
        "                               init_params=get_params('e2', init_params))\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        e3, pars = Ops.conv_2d(activations.relu(e2),\n",
        "                               (5, 8, 5, 5),\n",
        "                               layer_name='e3',\n",
        "                               mode='half',\n",
        "                               init_params=get_params('e3', init_params))\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        e4, pars = Ops.dense(Ops.flatten(activations.relu(e3)),\n",
        "                             self.input_size[0] * self.input_size[1] * 5,\n",
        "                             self.latent_size,\n",
        "                             layer_name='e4',\n",
        "                             init_params=get_params('e4', init_params))\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        return e4, params, regs\n",
        "\n",
        "    def _decode(self, X, init_params):\n",
        "        params = []\n",
        "        regs = []\n",
        "        d4, pars = Ops.dense(\n",
        "            X, self.latent_size, self.input_size[0] * self.input_size[1] * 5,\n",
        "            layer_name='d4',\n",
        "            init_params=get_params('d4', init_params))\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        d4 = T.reshape((d4),\n",
        "                       (-1, 5, self.input_size[0], self.input_size[1]))\n",
        "\n",
        "        d3, pars = Ops.conv_2d(activations.relu(d4),\n",
        "                               (8, 5, 5, 5),\n",
        "                               layer_name='d3',\n",
        "                               mode='half',\n",
        "                               init_params=get_params('d3', init_params))\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        d2, pars = Ops.conv_2d(activations.relu(d3),\n",
        "                               (10, 8, 5, 5),\n",
        "                               layer_name='d2',\n",
        "                               mode='half',\n",
        "                               init_params=get_params('d2', init_params))\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        d1, pars = Ops.conv_2d(activations.relu(d2),\n",
        "                               (1, 10, 5, 5),\n",
        "                               layer_name='d1',\n",
        "                               mode='half',\n",
        "                               init_params=get_params('d1', init_params))\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        return activations.sigmoid(d1), params, regs\n",
        "\n",
        "\n",
        "class miRNA_AE(object): #Autoencoder for 1D data\n",
        "\n",
        "    def __init__(self, input_size, latent_size):\n",
        "        ae_layers = 3.0\n",
        "        self.input_size = input_size\n",
        "        self.latent_size = latent_size\n",
        "        layer_list = []\n",
        "        layer_list.append(self.input_size)\n",
        "\n",
        "        #Arrange autoencoder steps in geometric series\n",
        "        divis = pow((self.input_size / self.latent_size), (1 / ae_layers))\n",
        "        for i in range(2):\n",
        "            layer_list.append(int(layer_list[-1] / divis))\n",
        "        self.layer_list = layer_list\n",
        "\n",
        "    def get_params(layer_name, par_list):\n",
        "      if par_list is None:\n",
        "        return None\n",
        "      pars = []\n",
        "      for i in par_list:\n",
        "        if i.name.split('_')[-1] == layer_name:\n",
        "            pars.append(i)\n",
        "      return None if len(pars) == 0 else pars\n",
        "\n",
        "\n",
        "    def _encode(self, X, init_params):\n",
        "\n",
        "        params = []\n",
        "        regs = []\n",
        "        e1, pars = Ops.dense(X, self.input_size, self.layer_list[1],\n",
        "                               layer_name='e1',\n",
        "                               init_params=get_params('e1', init_params))\n",
        "\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        e2, pars = Ops.dense(activations.relu(e1),\n",
        "                             self.layer_list[1],\n",
        "                             self.layer_list[2],\n",
        "                             layer_name='e2',\n",
        "                             init_params=get_params('e2', init_params))\n",
        "\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        e3, pars = Ops.dense(activations.relu(e2),\n",
        "                             self.layer_list[2],\n",
        "                             self.latent_size,\n",
        "                             layer_name='e3',\n",
        "                             init_params=get_params('e3', init_params))\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        return e3, params, regs\n",
        "\n",
        "    def _decode(self, X, init_params):\n",
        "\n",
        "        params = []\n",
        "        regs = []\n",
        "        d3, pars = Ops.dense(\n",
        "            X, self.latent_size, self.layer_list[2],\n",
        "            layer_name='d3',\n",
        "            init_params=get_params('d3', init_params))\n",
        "\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        d2, pars = Ops.dense(activations.relu(d3),\n",
        "            self.layer_list[2], self.layer_list[1],\n",
        "            layer_name='d2',\n",
        "            init_params=get_params('d2', init_params))\n",
        "\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        d1, pars = Ops.dense(activations.relu(d2),\n",
        "                             self.layer_list[1], self.input_size,\n",
        "                             layer_name='d1',\n",
        "                             init_params=get_params('d1', init_params))\n",
        "\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        return sigmoid(d1), params, regs\n",
        "\n",
        "\n",
        "class DNM(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_image_size,\n",
        "                 latent_size,\n",
        "                 lattice_size,\n",
        "                 ae_arch_class,\n",
        "                 name,\n",
        "                 init_params=None,\n",
        "                 sigma=None,\n",
        "                 alpha=None,\n",
        "                 BATCH_SIZE=64,\n",
        "                 ae_lr=1e-3,\n",
        "                 lmbd=1e-6,\n",
        "                 som_pretrain_lr=0.0005,\n",
        "                 dnm_map_lr=0.05):\n",
        "\n",
        "        self.lattice_size = lattice_size\n",
        "        self.name = name\n",
        "        self.latent_size = latent_size\n",
        "        self.sigma = sigma if sigma is not None else max(self.lattice_size[0],\n",
        "                                                         self.lattice_size[1]) / 2.\n",
        "        self.alpha = alpha if alpha is not None else 0.3\n",
        "        self.input_image_size = input_image_size\n",
        "        self.ae_lr = ae_lr\n",
        "        self.lmbd = lmbd\n",
        "        self.ae_arch_obj = ae_arch_class(self.input_image_size,\n",
        "                                         self.latent_size)\n",
        "        self.BATCH_SIZE = BATCH_SIZE\n",
        "        self.som_lr = som_pretrain_lr\n",
        "        self.dnm_map_lr = dnm_map_lr\n",
        "        self.ae_weights = None\n",
        "        if init_params is not None:\n",
        "            self.ae_weights = init_params[0]\n",
        "        if init_params is not None:\n",
        "            self.som_weights = init_params[1]\n",
        "        else:\n",
        "            self.som_weights = theano.shared(np.random.normal(0, 1,\n",
        "                                                              (self.lattice_size[0] *\n",
        "                                                               self.lattice_size[\n",
        "                                                                   1],\n",
        "                                                               self.latent_size), ))\n",
        "        if not os.path.exists('ckpts'):\n",
        "            os.mkdir('ckpts')\n",
        "        self._build()\n",
        "\n",
        "    def __soft_probs(self, sample, clusters):\n",
        "        alpha = 1.0\n",
        "        q = 1.0 / \\\n",
        "            (1.0 + (T.sum(T.square((sample - clusters)), axis=1) / alpha))\n",
        "        q **= (alpha + 1.0) / 2.0\n",
        "        q = T.transpose(T.transpose(q) / T.sum(q, axis=0))\n",
        "        return q\n",
        "\n",
        "    def __kld(self, p, q):\n",
        "        return T.sum(p * T.log(p / q), axis=-1)\n",
        "\n",
        "    def __SOM(self, X, W, n, update_flag):\n",
        "\n",
        "        learning_rate_op = T.exp(-1. * self.som_lr * n)\n",
        "        _alpha_op = self.alpha * learning_rate_op\n",
        "        _sigma_op = self.sigma * learning_rate_op\n",
        "\n",
        "        locations = self.locs\n",
        "        maps = T.sub(X, W)\n",
        "        measure = T.sum(T.pow(T.sub(X, W), 2), axis=1)\n",
        "        err = measure.min()\n",
        "        self.bmu_index = T.argmin(measure)\n",
        "        bmu_loc = locations[self.bmu_index]\n",
        "        dist_square = T.sum(\n",
        "            T.square(T.sub(locations, bmu_loc)), axis=1)\n",
        "        H = T.cast(T.exp(-dist_square / (2 * T.square(_sigma_op))),\n",
        "                   dtype=theano.config.floatX)\n",
        "        w_update = W + _alpha_op * \\\n",
        "            T.tile(H, [self.latent_size, 1]).T * maps\n",
        "        Qs = self.__soft_probs(X, W)\n",
        "        P = Qs ** 2 / Qs.sum()\n",
        "        P = (P.T / P.sum()).T\n",
        "        cost = self.__kld(P, Qs)\n",
        "        return [err, cost, bmu_loc], {W: update_flag * w_update + (1 - update_flag) * W}\n",
        "\n",
        "    def _build_locs(self, m, n):\n",
        "        for i in range(m):\n",
        "            for j in range(n):\n",
        "                yield(np.array([i, j]))\n",
        "\n",
        "    def _build(self):\n",
        "        # Tensors\n",
        "        n = T.scalar()\n",
        "        som_update_flag = T.scalar()\n",
        "        #self.X = T.TensorType(dtype=theano.config.floatX, broadcastable=(False,) * 4)('X')\n",
        "        self.X = T.TensorType(\n",
        "            dtype=theano.config.floatX, broadcastable=(False,) * 2)('X')  #changed for 1D data\n",
        "\n",
        "        # Architecture of the AE\n",
        "        encoder_output, encoder_params, enc_to_regs = self.ae_arch_obj._encode(\n",
        "            self.X, self.ae_weights)\n",
        "        encoder_output_sigmoid = sigmoid(encoder_output)\n",
        "        reconstructed, decoder_params, dec_to_regs = self.ae_arch_obj._decode(\n",
        "            encoder_output_sigmoid, self.ae_weights)\n",
        "\n",
        "        # Architecture of the SOM and updates\n",
        "        self.locs = T.constant(\n",
        "            [i for i in self._build_locs(self.lattice_size[0], self.lattice_size[1])])\n",
        "        [self.__SOM_batch_errs, som_kl_cost, bmus], self.__SOM_updates = theano.scan(\n",
        "            sequences=encoder_output_sigmoid,\n",
        "            non_sequences=[self.som_weights, n, som_update_flag],\n",
        "            fn=self.__SOM)\n",
        "\n",
        "        # Loss of DNM, SOM and AE\n",
        "        ae_optimizer = Adam(lr=self.ae_lr)\n",
        "        self.ae_weights = encoder_params + decoder_params\n",
        "        to_regs = enc_to_regs + dec_to_regs\n",
        "        self.map_err = T.mean(som_kl_cost)\n",
        "        #self.rec_cost = T.mean(\n",
        "        #    T.sum(T.nnet.binary_crossentropy(reconstructed, self.X), axis=1))\n",
        "        self.rec_cost = T.mean(T.sum((reconstructed - self.X) ** 2, axis=1)) #changed for 1D data\n",
        "        self.ae_cost = self.rec_cost + self.lmbd * \\\n",
        "            np.array([T.sum(i) ** 2 for i in to_regs]).sum()\n",
        "\n",
        "        self.dnm_cost = self.ae_cost + self.map_err\n",
        "\n",
        "        dnm_updates = ae_optimizer.updates(cost=self.dnm_cost,\n",
        "                                           params=self.ae_weights)\n",
        "\n",
        "        ae_updates = ae_optimizer.updates(cost=self.ae_cost,\n",
        "                                          params=self.ae_weights)\n",
        "        # Train ops\n",
        "        self.__train_op_dnm = theano.function([self.X, n, som_update_flag],\n",
        "                                              outputs=[self.dnm_cost, T.mean(\n",
        "                                                  self.__SOM_batch_errs)],\n",
        "                                              updates=dnm_updates)\n",
        "        self.__train_op_ae = theano.function([self.X],\n",
        "                                             outputs=self.ae_cost,\n",
        "                                             updates=ae_updates)\n",
        "        self.__train_op_som = theano.function([self.X, n, som_update_flag],\n",
        "                                              outputs=[self.__SOM_batch_errs, bmus],\n",
        "                                              updates=self.__SOM_updates)\n",
        "\n",
        "        self.__get_ae_output = theano.function([self.X],\n",
        "                                               encoder_output_sigmoid)\n",
        "        self.__get_reconstructed = theano.function([self.X],\n",
        "                                                   reconstructed)\n",
        "        self.__decode_embedding = theano.function([encoder_output_sigmoid],\n",
        "                                                  reconstructed)\n",
        "        self.__get_backprojection = theano.function([],\n",
        "                                                     outputs=reconstructed,\n",
        "                                                     givens={encoder_output_sigmoid: self.som_weights.eval().astype('float64')})\n",
        "\n",
        "        self._get_bmu_locs = theano.function(\n",
        "            [self.X, n, som_update_flag], bmus)\n",
        "\n",
        "    def get_bmu_array(self, X):\n",
        "        res = []\n",
        "        weight_array = self.som_weights.get_value()\n",
        "        for i in X:\n",
        "            measure = np.sum((i - weight_array) ** 2, axis=1)\n",
        "            bmu_index = np.argmin(measure)\n",
        "            res.append(weight_array[bmu_index])\n",
        "        return np.array(res)\n",
        "\n",
        "    def train(self, x_train, \n",
        "              dnm_epochs, trial_name, name, location_name,\n",
        "              pre_train_epochs=None):\n",
        "\n",
        "        if pre_train_epochs is not None:\n",
        "            self._pretrain(x_train, pre_train_epochs[\n",
        "                           0], pre_train_epochs[1], trial_name, name, location_name, self.BATCH_SIZE)\n",
        "            \n",
        "        hold_inter = TemporaryFile() #save weights (deletes at the end of session)\n",
        "        hold_int = [self.ae_weights, self.som_weights]\n",
        "        np.save(hold_inter, hold_int)\n",
        "\n",
        "\n",
        "        self.som_lr = self.dnm_map_lr\n",
        "        print('\\nTraining DNM')\n",
        "        print(50 * '=')\n",
        "        B = BatchFactory(self.BATCH_SIZE, len(x_train), dnm_epochs)\n",
        "        batcher = B.generate_batch(x_train, None)\n",
        "        iteration = 0\n",
        "        nb_batches = np.ceil(1. * len(x_train) / self.BATCH_SIZE)\n",
        "        errs = []\n",
        "        Ds = []\n",
        "        for idx, batch in enumerate(batcher):\n",
        "            self.__train_op_som(batch, iteration, 1)\n",
        "            er, dists = self.__train_op_dnm(batch, iteration, 0)\n",
        "            errs.append(er)\n",
        "            Ds.append(dists)\n",
        "            if (idx + 1) % (nb_batches) == 0:\n",
        "                print(\n",
        "                    'iter: {} \\tloss: {}, dist: {}'.format(iteration + 1, np.array(errs).mean(),\n",
        "                                                           np.array(Ds).mean()))\n",
        "                print(50 * '-')\n",
        "                iteration += 1\n",
        "                Ds = []\n",
        "                errs = []\n",
        "        \n",
        "        hold_final = TemporaryFile() #save weights (deletes at the end of session)\n",
        "        hold_fin = [self.ae_weights, self.som_weights]\n",
        "        np.save(hold_final, hold_fin)\n",
        "\n",
        "\n",
        "    def get_locations(self, input_vects):\n",
        "        to_return = []\n",
        "        weight_values = list(np.array(self.som_weights.eval()))\n",
        "        locations = list(np.array(self.locs.eval()))\n",
        "        test = self.__get_ae_output(input_vects.astype('float32'))\n",
        "        for vect in test:\n",
        "            #v = vect.reshape(1,1, self.input_image_size[0],self.input_image_size[1])\n",
        "\n",
        "            v=vect\n",
        "            min_index = min([i for i in range(len(weight_values))],\n",
        "                             key=lambda x: np.linalg.norm(v -\n",
        "                                                          weight_values[x]))\n",
        "            \n",
        "            to_return.append(locations[min_index])\n",
        "        return to_return\n",
        "\n",
        "    def get_reconstructed(self, X):\n",
        "        return self.__get_reconstructed(X.astype('float32'))\n",
        "\n",
        "    def decode_embedding(self, z):\n",
        "        return self.__decode_embedding(z)\n",
        "\n",
        "    def backproject_map(self):\n",
        "        return self.__get_backprojection()\n",
        "\n",
        "    def _pretrain(self, x_train, AE_epochs, SOM_epochs, trial_name, name, location_name, BATCH_SIZE=64):\n",
        "        print('\\nPre-training AE')\n",
        "        print(50 * '=')\n",
        "        B = BatchFactory(BATCH_SIZE, len(x_train), AE_epochs)\n",
        "        batcher = B.generate_batch(x_train, None)\n",
        "        iteration = 0\n",
        "        nb_batches = np.ceil(1. * len(x_train) / BATCH_SIZE)\n",
        "        errs = []\n",
        "        for idx, batch in enumerate(batcher): \n",
        "            er = self.__train_op_ae(batch)\n",
        "            errs.append(er)\n",
        "            if (idx + 1) % (nb_batches) == 0:\n",
        "                print('iter: {} \\ttrain loss: {}'.format(iteration + 1,\n",
        "                                                         np.array(errs).mean()))\n",
        "                print(50 * '-')\n",
        "                iteration += 1\n",
        "                errs = []\n",
        "      \n",
        "        print('\\nPre-training SOM')\n",
        "        print(50 * '=')\n",
        "        B = BatchFactory(BATCH_SIZE, len(x_train), SOM_epochs)\n",
        "        batcher = B.generate_batch(x_train, None)\n",
        "        iteration = 0\n",
        "        nb_batches = np.ceil(1. * len(x_train) / BATCH_SIZE)\n",
        "        errs = []\n",
        "        \n",
        "        for idx, batch in enumerate(batcher):\n",
        "            ers, dists = self.__train_op_som(batch, iteration, 1)\n",
        "            errs.append(np.mean(ers))\n",
        "            if (idx + 1) % nb_batches == 0:\n",
        "                print('iter: {} \\ttrain loss: {}'.format(iteration + 1,\n",
        "                                                         np.array(errs).mean()))\n",
        "                print(50 * '-')\n",
        "                iteration += 1\n",
        "                errs = []\n",
        "                "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjflA4NJY-LM",
        "outputId": "82e798ee-7561-4b57-b83d-2711eb1ac9ef"
      },
      "source": [
        "X = sklearn.datasets.load_breast_cancer().data\n",
        "Y = sklearn.datasets.load_breast_cancer().target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=42)\n",
        " \n",
        "nrm = MinMaxScaler()\n",
        "X_train = nrm.fit_transform(X_train)\n",
        "X_test = nrm.transform(X_test) \n",
        "\n",
        "lattice_size = (16,16)\n",
        "latent_size = 4\n",
        "\n",
        "deep_map = DNM(input_image_size= len(X_train[0]),\n",
        "                        latent_size= latent_size,\n",
        "                        lattice_size= lattice_size,\n",
        "                        ae_arch_class= miRNA_AE, \n",
        "                        name='miRNA_test',\n",
        "                        init_params=None, \n",
        "                        sigma=None,\n",
        "                        alpha=None,\n",
        "                        BATCH_SIZE=16,\n",
        "                        ae_lr=1e-3,\n",
        "                        lmbd=1e-6,\n",
        "                        som_pretrain_lr=0.005,\n",
        "                        dnm_map_lr=0.05)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "deep_map.train(x_train=X_train.astype('float32'),  \n",
        "                                dnm_epochs=1500, trial_name='test', name='test', location_name=None,\n",
        "                                pre_train_epochs=[2500, 1500]) \n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iter: 1686 \ttrain loss: 0.15750808812679054\n",
            "--------------------------------------------------\n",
            "iter: 1687 \ttrain loss: 0.16069216038453485\n",
            "--------------------------------------------------\n",
            "iter: 1688 \ttrain loss: 0.14831622763742255\n",
            "--------------------------------------------------\n",
            "iter: 1689 \ttrain loss: 0.1667635371582135\n",
            "--------------------------------------------------\n",
            "iter: 1690 \ttrain loss: 0.16504882260926734\n",
            "--------------------------------------------------\n",
            "iter: 1691 \ttrain loss: 0.15514299790356198\n",
            "--------------------------------------------------\n",
            "iter: 1692 \ttrain loss: 0.15444738948090153\n",
            "--------------------------------------------------\n",
            "iter: 1693 \ttrain loss: 0.19944662734059693\n",
            "--------------------------------------------------\n",
            "iter: 1694 \ttrain loss: 0.15916774284691132\n",
            "--------------------------------------------------\n",
            "iter: 1695 \ttrain loss: 0.14166216177572893\n",
            "--------------------------------------------------\n",
            "iter: 1696 \ttrain loss: 0.1792959725576803\n",
            "--------------------------------------------------\n",
            "iter: 1697 \ttrain loss: 0.17006107660114794\n",
            "--------------------------------------------------\n",
            "iter: 1698 \ttrain loss: 0.15541627630919055\n",
            "--------------------------------------------------\n",
            "iter: 1699 \ttrain loss: 0.15514579260894007\n",
            "--------------------------------------------------\n",
            "iter: 1700 \ttrain loss: 0.1637814032977445\n",
            "--------------------------------------------------\n",
            "iter: 1701 \ttrain loss: 0.1608083831980324\n",
            "--------------------------------------------------\n",
            "iter: 1702 \ttrain loss: 0.15592783598123394\n",
            "--------------------------------------------------\n",
            "iter: 1703 \ttrain loss: 0.16081546925600473\n",
            "--------------------------------------------------\n",
            "iter: 1704 \ttrain loss: 0.15905706458842742\n",
            "--------------------------------------------------\n",
            "iter: 1705 \ttrain loss: 0.16527758793961997\n",
            "--------------------------------------------------\n",
            "iter: 1706 \ttrain loss: 0.17416820106526257\n",
            "--------------------------------------------------\n",
            "iter: 1707 \ttrain loss: 0.18603185621125784\n",
            "--------------------------------------------------\n",
            "iter: 1708 \ttrain loss: 0.1610076837518305\n",
            "--------------------------------------------------\n",
            "iter: 1709 \ttrain loss: 0.15222203745392343\n",
            "--------------------------------------------------\n",
            "iter: 1710 \ttrain loss: 0.15253852137864016\n",
            "--------------------------------------------------\n",
            "iter: 1711 \ttrain loss: 0.1713981693251604\n",
            "--------------------------------------------------\n",
            "iter: 1712 \ttrain loss: 0.15939403607398683\n",
            "--------------------------------------------------\n",
            "iter: 1713 \ttrain loss: 0.1567646229872971\n",
            "--------------------------------------------------\n",
            "iter: 1714 \ttrain loss: 0.15629436269301897\n",
            "--------------------------------------------------\n",
            "iter: 1715 \ttrain loss: 0.15460020607521952\n",
            "--------------------------------------------------\n",
            "iter: 1716 \ttrain loss: 0.16352065277512493\n",
            "--------------------------------------------------\n",
            "iter: 1717 \ttrain loss: 0.15675453850661197\n",
            "--------------------------------------------------\n",
            "iter: 1718 \ttrain loss: 0.14997483806219664\n",
            "--------------------------------------------------\n",
            "iter: 1719 \ttrain loss: 0.17359318927820325\n",
            "--------------------------------------------------\n",
            "iter: 1720 \ttrain loss: 0.15106091057872942\n",
            "--------------------------------------------------\n",
            "iter: 1721 \ttrain loss: 0.15629636796375176\n",
            "--------------------------------------------------\n",
            "iter: 1722 \ttrain loss: 0.1628798071068012\n",
            "--------------------------------------------------\n",
            "iter: 1723 \ttrain loss: 0.15683860749146103\n",
            "--------------------------------------------------\n",
            "iter: 1724 \ttrain loss: 0.16045814316428214\n",
            "--------------------------------------------------\n",
            "iter: 1725 \ttrain loss: 0.1641634852954219\n",
            "--------------------------------------------------\n",
            "iter: 1726 \ttrain loss: 0.1621653618559726\n",
            "--------------------------------------------------\n",
            "iter: 1727 \ttrain loss: 0.16355442841115744\n",
            "--------------------------------------------------\n",
            "iter: 1728 \ttrain loss: 0.15527652013384896\n",
            "--------------------------------------------------\n",
            "iter: 1729 \ttrain loss: 0.15771722495383841\n",
            "--------------------------------------------------\n",
            "iter: 1730 \ttrain loss: 0.16384916647572156\n",
            "--------------------------------------------------\n",
            "iter: 1731 \ttrain loss: 0.14942375773760397\n",
            "--------------------------------------------------\n",
            "iter: 1732 \ttrain loss: 0.1748706217519265\n",
            "--------------------------------------------------\n",
            "iter: 1733 \ttrain loss: 0.15040777396873997\n",
            "--------------------------------------------------\n",
            "iter: 1734 \ttrain loss: 0.18754234348010057\n",
            "--------------------------------------------------\n",
            "iter: 1735 \ttrain loss: 0.15941187729993092\n",
            "--------------------------------------------------\n",
            "iter: 1736 \ttrain loss: 0.15372567609674728\n",
            "--------------------------------------------------\n",
            "iter: 1737 \ttrain loss: 0.15659286301205394\n",
            "--------------------------------------------------\n",
            "iter: 1738 \ttrain loss: 0.15888856264509615\n",
            "--------------------------------------------------\n",
            "iter: 1739 \ttrain loss: 0.15562964043158417\n",
            "--------------------------------------------------\n",
            "iter: 1740 \ttrain loss: 0.17279111587736506\n",
            "--------------------------------------------------\n",
            "iter: 1741 \ttrain loss: 0.16358802372030287\n",
            "--------------------------------------------------\n",
            "iter: 1742 \ttrain loss: 0.1641158502775305\n",
            "--------------------------------------------------\n",
            "iter: 1743 \ttrain loss: 0.14714056739841555\n",
            "--------------------------------------------------\n",
            "iter: 1744 \ttrain loss: 0.15683391565933874\n",
            "--------------------------------------------------\n",
            "iter: 1745 \ttrain loss: 0.1531837258703219\n",
            "--------------------------------------------------\n",
            "iter: 1746 \ttrain loss: 0.16814436163823\n",
            "--------------------------------------------------\n",
            "iter: 1747 \ttrain loss: 0.15756329975397335\n",
            "--------------------------------------------------\n",
            "iter: 1748 \ttrain loss: 0.15762984254944282\n",
            "--------------------------------------------------\n",
            "iter: 1749 \ttrain loss: 0.16230934499899352\n",
            "--------------------------------------------------\n",
            "iter: 1750 \ttrain loss: 0.16248135214416062\n",
            "--------------------------------------------------\n",
            "iter: 1751 \ttrain loss: 0.1589760217991947\n",
            "--------------------------------------------------\n",
            "iter: 1752 \ttrain loss: 0.16217749005996954\n",
            "--------------------------------------------------\n",
            "iter: 1753 \ttrain loss: 0.17350811222956786\n",
            "--------------------------------------------------\n",
            "iter: 1754 \ttrain loss: 0.15561850350131756\n",
            "--------------------------------------------------\n",
            "iter: 1755 \ttrain loss: 0.16364509922548864\n",
            "--------------------------------------------------\n",
            "iter: 1756 \ttrain loss: 0.15580322047186954\n",
            "--------------------------------------------------\n",
            "iter: 1757 \ttrain loss: 0.1538745061628578\n",
            "--------------------------------------------------\n",
            "iter: 1758 \ttrain loss: 0.19234427018935438\n",
            "--------------------------------------------------\n",
            "iter: 1759 \ttrain loss: 0.15024890660992815\n",
            "--------------------------------------------------\n",
            "iter: 1760 \ttrain loss: 0.16402655549000705\n",
            "--------------------------------------------------\n",
            "iter: 1761 \ttrain loss: 0.14986950822911674\n",
            "--------------------------------------------------\n",
            "iter: 1762 \ttrain loss: 0.16484088662452662\n",
            "--------------------------------------------------\n",
            "iter: 1763 \ttrain loss: 0.13987303299728165\n",
            "--------------------------------------------------\n",
            "iter: 1764 \ttrain loss: 0.1728103151052603\n",
            "--------------------------------------------------\n",
            "iter: 1765 \ttrain loss: 0.15731555009245257\n",
            "--------------------------------------------------\n",
            "iter: 1766 \ttrain loss: 0.15437630900824292\n",
            "--------------------------------------------------\n",
            "iter: 1767 \ttrain loss: 0.15231824040969952\n",
            "--------------------------------------------------\n",
            "iter: 1768 \ttrain loss: 0.1674736757071954\n",
            "--------------------------------------------------\n",
            "iter: 1769 \ttrain loss: 0.15873575197155876\n",
            "--------------------------------------------------\n",
            "iter: 1770 \ttrain loss: 0.16286892370974654\n",
            "--------------------------------------------------\n",
            "iter: 1771 \ttrain loss: 0.15834651400852098\n",
            "--------------------------------------------------\n",
            "iter: 1772 \ttrain loss: 0.15860199278411657\n",
            "--------------------------------------------------\n",
            "iter: 1773 \ttrain loss: 0.15702947536983525\n",
            "--------------------------------------------------\n",
            "iter: 1774 \ttrain loss: 0.16523817196282756\n",
            "--------------------------------------------------\n",
            "iter: 1775 \ttrain loss: 0.17702713758765495\n",
            "--------------------------------------------------\n",
            "iter: 1776 \ttrain loss: 0.15909472895149648\n",
            "--------------------------------------------------\n",
            "iter: 1777 \ttrain loss: 0.15344059729859058\n",
            "--------------------------------------------------\n",
            "iter: 1778 \ttrain loss: 0.1608523061804248\n",
            "--------------------------------------------------\n",
            "iter: 1779 \ttrain loss: 0.1468094135842381\n",
            "--------------------------------------------------\n",
            "iter: 1780 \ttrain loss: 0.16404028389713202\n",
            "--------------------------------------------------\n",
            "iter: 1781 \ttrain loss: 0.14972688585010463\n",
            "--------------------------------------------------\n",
            "iter: 1782 \ttrain loss: 0.16985535937163224\n",
            "--------------------------------------------------\n",
            "iter: 1783 \ttrain loss: 0.14548382701649318\n",
            "--------------------------------------------------\n",
            "iter: 1784 \ttrain loss: 0.16063543364955024\n",
            "--------------------------------------------------\n",
            "iter: 1785 \ttrain loss: 0.16644808209240983\n",
            "--------------------------------------------------\n",
            "iter: 1786 \ttrain loss: 0.14574550609322653\n",
            "--------------------------------------------------\n",
            "iter: 1787 \ttrain loss: 0.16507941147071356\n",
            "--------------------------------------------------\n",
            "iter: 1788 \ttrain loss: 0.16940713559648604\n",
            "--------------------------------------------------\n",
            "iter: 1789 \ttrain loss: 0.15388070643116014\n",
            "--------------------------------------------------\n",
            "iter: 1790 \ttrain loss: 0.16925631505902416\n",
            "--------------------------------------------------\n",
            "iter: 1791 \ttrain loss: 0.17149141629959055\n",
            "--------------------------------------------------\n",
            "iter: 1792 \ttrain loss: 0.17092203025528996\n",
            "--------------------------------------------------\n",
            "iter: 1793 \ttrain loss: 0.17017514061901023\n",
            "--------------------------------------------------\n",
            "iter: 1794 \ttrain loss: 0.15520421791273067\n",
            "--------------------------------------------------\n",
            "iter: 1795 \ttrain loss: 0.15895248477430265\n",
            "--------------------------------------------------\n",
            "iter: 1796 \ttrain loss: 0.15943497642803597\n",
            "--------------------------------------------------\n",
            "iter: 1797 \ttrain loss: 0.15607545589218788\n",
            "--------------------------------------------------\n",
            "iter: 1798 \ttrain loss: 0.15552622399729368\n",
            "--------------------------------------------------\n",
            "iter: 1799 \ttrain loss: 0.16216575492925298\n",
            "--------------------------------------------------\n",
            "iter: 1800 \ttrain loss: 0.1632379731559801\n",
            "--------------------------------------------------\n",
            "iter: 1801 \ttrain loss: 0.14723344900786664\n",
            "--------------------------------------------------\n",
            "iter: 1802 \ttrain loss: 0.16423536278776327\n",
            "--------------------------------------------------\n",
            "iter: 1803 \ttrain loss: 0.1527771882730019\n",
            "--------------------------------------------------\n",
            "iter: 1804 \ttrain loss: 0.15708064714913444\n",
            "--------------------------------------------------\n",
            "iter: 1805 \ttrain loss: 0.1731293850739943\n",
            "--------------------------------------------------\n",
            "iter: 1806 \ttrain loss: 0.14567882836511725\n",
            "--------------------------------------------------\n",
            "iter: 1807 \ttrain loss: 0.16970393379672322\n",
            "--------------------------------------------------\n",
            "iter: 1808 \ttrain loss: 0.15442567123677564\n",
            "--------------------------------------------------\n",
            "iter: 1809 \ttrain loss: 0.16061540957263054\n",
            "--------------------------------------------------\n",
            "iter: 1810 \ttrain loss: 0.15470747689576678\n",
            "--------------------------------------------------\n",
            "iter: 1811 \ttrain loss: 0.16008443996939273\n",
            "--------------------------------------------------\n",
            "iter: 1812 \ttrain loss: 0.16198114230956118\n",
            "--------------------------------------------------\n",
            "iter: 1813 \ttrain loss: 0.15956167249878017\n",
            "--------------------------------------------------\n",
            "iter: 1814 \ttrain loss: 0.16254407182628364\n",
            "--------------------------------------------------\n",
            "iter: 1815 \ttrain loss: 0.15820614216327009\n",
            "--------------------------------------------------\n",
            "iter: 1816 \ttrain loss: 0.16098575012095664\n",
            "--------------------------------------------------\n",
            "iter: 1817 \ttrain loss: 0.15846336143526155\n",
            "--------------------------------------------------\n",
            "iter: 1818 \ttrain loss: 0.1575017937837241\n",
            "--------------------------------------------------\n",
            "iter: 1819 \ttrain loss: 0.1564563216905649\n",
            "--------------------------------------------------\n",
            "iter: 1820 \ttrain loss: 0.1538048812444206\n",
            "--------------------------------------------------\n",
            "iter: 1821 \ttrain loss: 0.16827693302218408\n",
            "--------------------------------------------------\n",
            "iter: 1822 \ttrain loss: 0.15202689426160965\n",
            "--------------------------------------------------\n",
            "iter: 1823 \ttrain loss: 0.16007081039621082\n",
            "--------------------------------------------------\n",
            "iter: 1824 \ttrain loss: 0.16615085025526233\n",
            "--------------------------------------------------\n",
            "iter: 1825 \ttrain loss: 0.1644225227588469\n",
            "--------------------------------------------------\n",
            "iter: 1826 \ttrain loss: 0.1503746536180149\n",
            "--------------------------------------------------\n",
            "iter: 1827 \ttrain loss: 0.1552823211017547\n",
            "--------------------------------------------------\n",
            "iter: 1828 \ttrain loss: 0.16714454510388235\n",
            "--------------------------------------------------\n",
            "iter: 1829 \ttrain loss: 0.15308861306146715\n",
            "--------------------------------------------------\n",
            "iter: 1830 \ttrain loss: 0.1594527476297095\n",
            "--------------------------------------------------\n",
            "iter: 1831 \ttrain loss: 0.17053142387485312\n",
            "--------------------------------------------------\n",
            "iter: 1832 \ttrain loss: 0.15518215845709635\n",
            "--------------------------------------------------\n",
            "iter: 1833 \ttrain loss: 0.16780542609878024\n",
            "--------------------------------------------------\n",
            "iter: 1834 \ttrain loss: 0.1698528038274745\n",
            "--------------------------------------------------\n",
            "iter: 1835 \ttrain loss: 0.1688408529087024\n",
            "--------------------------------------------------\n",
            "iter: 1836 \ttrain loss: 0.1403557200955041\n",
            "--------------------------------------------------\n",
            "iter: 1837 \ttrain loss: 0.16936564255301964\n",
            "--------------------------------------------------\n",
            "iter: 1838 \ttrain loss: 0.15840500224505563\n",
            "--------------------------------------------------\n",
            "iter: 1839 \ttrain loss: 0.15923477716852882\n",
            "--------------------------------------------------\n",
            "iter: 1840 \ttrain loss: 0.15279155445284662\n",
            "--------------------------------------------------\n",
            "iter: 1841 \ttrain loss: 0.16220083543050182\n",
            "--------------------------------------------------\n",
            "iter: 1842 \ttrain loss: 0.16303706887294675\n",
            "--------------------------------------------------\n",
            "iter: 1843 \ttrain loss: 0.1648428578877729\n",
            "--------------------------------------------------\n",
            "iter: 1844 \ttrain loss: 0.15604004863169738\n",
            "--------------------------------------------------\n",
            "iter: 1845 \ttrain loss: 0.18473954501041787\n",
            "--------------------------------------------------\n",
            "iter: 1846 \ttrain loss: 0.15490129967416696\n",
            "--------------------------------------------------\n",
            "iter: 1847 \ttrain loss: 0.15241944876697433\n",
            "--------------------------------------------------\n",
            "iter: 1848 \ttrain loss: 0.16876800011811427\n",
            "--------------------------------------------------\n",
            "iter: 1849 \ttrain loss: 0.16240965489627956\n",
            "--------------------------------------------------\n",
            "iter: 1850 \ttrain loss: 0.14938270760109806\n",
            "--------------------------------------------------\n",
            "iter: 1851 \ttrain loss: 0.16906719978787652\n",
            "--------------------------------------------------\n",
            "iter: 1852 \ttrain loss: 0.14918586386193475\n",
            "--------------------------------------------------\n",
            "iter: 1853 \ttrain loss: 0.15315228689484794\n",
            "--------------------------------------------------\n",
            "iter: 1854 \ttrain loss: 0.1497387810551118\n",
            "--------------------------------------------------\n",
            "iter: 1855 \ttrain loss: 0.16077249471103763\n",
            "--------------------------------------------------\n",
            "iter: 1856 \ttrain loss: 0.1640986168464195\n",
            "--------------------------------------------------\n",
            "iter: 1857 \ttrain loss: 0.15880943576965575\n",
            "--------------------------------------------------\n",
            "iter: 1858 \ttrain loss: 0.15196613069935608\n",
            "--------------------------------------------------\n",
            "iter: 1859 \ttrain loss: 0.17102496514999357\n",
            "--------------------------------------------------\n",
            "iter: 1860 \ttrain loss: 0.16950820238424022\n",
            "--------------------------------------------------\n",
            "iter: 1861 \ttrain loss: 0.15479486343253115\n",
            "--------------------------------------------------\n",
            "iter: 1862 \ttrain loss: 0.16708783731862817\n",
            "--------------------------------------------------\n",
            "iter: 1863 \ttrain loss: 0.15583459735342087\n",
            "--------------------------------------------------\n",
            "iter: 1864 \ttrain loss: 0.15934859462833587\n",
            "--------------------------------------------------\n",
            "iter: 1865 \ttrain loss: 0.1594462630081054\n",
            "--------------------------------------------------\n",
            "iter: 1866 \ttrain loss: 0.15989128839674246\n",
            "--------------------------------------------------\n",
            "iter: 1867 \ttrain loss: 0.15073203444651792\n",
            "--------------------------------------------------\n",
            "iter: 1868 \ttrain loss: 0.16560121294034513\n",
            "--------------------------------------------------\n",
            "iter: 1869 \ttrain loss: 0.1523315799235074\n",
            "--------------------------------------------------\n",
            "iter: 1870 \ttrain loss: 0.16955220467796328\n",
            "--------------------------------------------------\n",
            "iter: 1871 \ttrain loss: 0.15427196086858622\n",
            "--------------------------------------------------\n",
            "iter: 1872 \ttrain loss: 0.1610203328395542\n",
            "--------------------------------------------------\n",
            "iter: 1873 \ttrain loss: 0.1692434644385124\n",
            "--------------------------------------------------\n",
            "iter: 1874 \ttrain loss: 0.14320196051350928\n",
            "--------------------------------------------------\n",
            "iter: 1875 \ttrain loss: 0.16654784945194487\n",
            "--------------------------------------------------\n",
            "iter: 1876 \ttrain loss: 0.1555353197727862\n",
            "--------------------------------------------------\n",
            "iter: 1877 \ttrain loss: 0.15082363063796056\n",
            "--------------------------------------------------\n",
            "iter: 1878 \ttrain loss: 0.16838572250601394\n",
            "--------------------------------------------------\n",
            "iter: 1879 \ttrain loss: 0.18126733627583802\n",
            "--------------------------------------------------\n",
            "iter: 1880 \ttrain loss: 0.13379507029305446\n",
            "--------------------------------------------------\n",
            "iter: 1881 \ttrain loss: 0.17423478399268227\n",
            "--------------------------------------------------\n",
            "iter: 1882 \ttrain loss: 0.1511256464623013\n",
            "--------------------------------------------------\n",
            "iter: 1883 \ttrain loss: 0.16235201521418674\n",
            "--------------------------------------------------\n",
            "iter: 1884 \ttrain loss: 0.16133496384401785\n",
            "--------------------------------------------------\n",
            "iter: 1885 \ttrain loss: 0.166679318665826\n",
            "--------------------------------------------------\n",
            "iter: 1886 \ttrain loss: 0.15346436652612316\n",
            "--------------------------------------------------\n",
            "iter: 1887 \ttrain loss: 0.16216583922765898\n",
            "--------------------------------------------------\n",
            "iter: 1888 \ttrain loss: 0.16426642454902948\n",
            "--------------------------------------------------\n",
            "iter: 1889 \ttrain loss: 0.1558400906820664\n",
            "--------------------------------------------------\n",
            "iter: 1890 \ttrain loss: 0.15533666258377746\n",
            "--------------------------------------------------\n",
            "iter: 1891 \ttrain loss: 0.16206977381802287\n",
            "--------------------------------------------------\n",
            "iter: 1892 \ttrain loss: 0.16827964651395447\n",
            "--------------------------------------------------\n",
            "iter: 1893 \ttrain loss: 0.15922958856264094\n",
            "--------------------------------------------------\n",
            "iter: 1894 \ttrain loss: 0.15953937723454398\n",
            "--------------------------------------------------\n",
            "iter: 1895 \ttrain loss: 0.14568468207523924\n",
            "--------------------------------------------------\n",
            "iter: 1896 \ttrain loss: 0.17805667222224753\n",
            "--------------------------------------------------\n",
            "iter: 1897 \ttrain loss: 0.15747899674265964\n",
            "--------------------------------------------------\n",
            "iter: 1898 \ttrain loss: 0.15629313063992648\n",
            "--------------------------------------------------\n",
            "iter: 1899 \ttrain loss: 0.1815905029671431\n",
            "--------------------------------------------------\n",
            "iter: 1900 \ttrain loss: 0.15601476480899631\n",
            "--------------------------------------------------\n",
            "iter: 1901 \ttrain loss: 0.1580484371595826\n",
            "--------------------------------------------------\n",
            "iter: 1902 \ttrain loss: 0.16582157359847297\n",
            "--------------------------------------------------\n",
            "iter: 1903 \ttrain loss: 0.1645807999751051\n",
            "--------------------------------------------------\n",
            "iter: 1904 \ttrain loss: 0.15597907898126695\n",
            "--------------------------------------------------\n",
            "iter: 1905 \ttrain loss: 0.15571845206335264\n",
            "--------------------------------------------------\n",
            "iter: 1906 \ttrain loss: 0.16489662232835983\n",
            "--------------------------------------------------\n",
            "iter: 1907 \ttrain loss: 0.16952987584499318\n",
            "--------------------------------------------------\n",
            "iter: 1908 \ttrain loss: 0.1595461533324725\n",
            "--------------------------------------------------\n",
            "iter: 1909 \ttrain loss: 0.1565517601432533\n",
            "--------------------------------------------------\n",
            "iter: 1910 \ttrain loss: 0.16194668453183897\n",
            "--------------------------------------------------\n",
            "iter: 1911 \ttrain loss: 0.16437080758090908\n",
            "--------------------------------------------------\n",
            "iter: 1912 \ttrain loss: 0.15299841652001941\n",
            "--------------------------------------------------\n",
            "iter: 1913 \ttrain loss: 0.1552395186927275\n",
            "--------------------------------------------------\n",
            "iter: 1914 \ttrain loss: 0.14814109789913088\n",
            "--------------------------------------------------\n",
            "iter: 1915 \ttrain loss: 0.1651475202087229\n",
            "--------------------------------------------------\n",
            "iter: 1916 \ttrain loss: 0.1584047580340382\n",
            "--------------------------------------------------\n",
            "iter: 1917 \ttrain loss: 0.1572658110335814\n",
            "--------------------------------------------------\n",
            "iter: 1918 \ttrain loss: 0.15332169360142262\n",
            "--------------------------------------------------\n",
            "iter: 1919 \ttrain loss: 0.16435773914247082\n",
            "--------------------------------------------------\n",
            "iter: 1920 \ttrain loss: 0.16403814702700156\n",
            "--------------------------------------------------\n",
            "iter: 1921 \ttrain loss: 0.1408829325465789\n",
            "--------------------------------------------------\n",
            "iter: 1922 \ttrain loss: 0.17815152251071184\n",
            "--------------------------------------------------\n",
            "iter: 1923 \ttrain loss: 0.16509042012854466\n",
            "--------------------------------------------------\n",
            "iter: 1924 \ttrain loss: 0.16401204908474434\n",
            "--------------------------------------------------\n",
            "iter: 1925 \ttrain loss: 0.154344352761872\n",
            "--------------------------------------------------\n",
            "iter: 1926 \ttrain loss: 0.16529359816761904\n",
            "--------------------------------------------------\n",
            "iter: 1927 \ttrain loss: 0.15689712369308267\n",
            "--------------------------------------------------\n",
            "iter: 1928 \ttrain loss: 0.15527161603235626\n",
            "--------------------------------------------------\n",
            "iter: 1929 \ttrain loss: 0.15956631654537146\n",
            "--------------------------------------------------\n",
            "iter: 1930 \ttrain loss: 0.1586149465589189\n",
            "--------------------------------------------------\n",
            "iter: 1931 \ttrain loss: 0.15803560662907287\n",
            "--------------------------------------------------\n",
            "iter: 1932 \ttrain loss: 0.1564693437489309\n",
            "--------------------------------------------------\n",
            "iter: 1933 \ttrain loss: 0.15661099576346935\n",
            "--------------------------------------------------\n",
            "iter: 1934 \ttrain loss: 0.15381114464968806\n",
            "--------------------------------------------------\n",
            "iter: 1935 \ttrain loss: 0.16945450706651696\n",
            "--------------------------------------------------\n",
            "iter: 1936 \ttrain loss: 0.15222366154705805\n",
            "--------------------------------------------------\n",
            "iter: 1937 \ttrain loss: 0.1608407443735339\n",
            "--------------------------------------------------\n",
            "iter: 1938 \ttrain loss: 0.15178010399770403\n",
            "--------------------------------------------------\n",
            "iter: 1939 \ttrain loss: 0.16476177115250318\n",
            "--------------------------------------------------\n",
            "iter: 1940 \ttrain loss: 0.14692433530395557\n",
            "--------------------------------------------------\n",
            "iter: 1941 \ttrain loss: 0.16336299345696484\n",
            "--------------------------------------------------\n",
            "iter: 1942 \ttrain loss: 0.15760827536826294\n",
            "--------------------------------------------------\n",
            "iter: 1943 \ttrain loss: 0.1560202062037147\n",
            "--------------------------------------------------\n",
            "iter: 1944 \ttrain loss: 0.17144201398570807\n",
            "--------------------------------------------------\n",
            "iter: 1945 \ttrain loss: 0.15071996904105392\n",
            "--------------------------------------------------\n",
            "iter: 1946 \ttrain loss: 0.16307537786572326\n",
            "--------------------------------------------------\n",
            "iter: 1947 \ttrain loss: 0.15459494102419694\n",
            "--------------------------------------------------\n",
            "iter: 1948 \ttrain loss: 0.15972127393248636\n",
            "--------------------------------------------------\n",
            "iter: 1949 \ttrain loss: 0.15978786072804133\n",
            "--------------------------------------------------\n",
            "iter: 1950 \ttrain loss: 0.15933257672843062\n",
            "--------------------------------------------------\n",
            "iter: 1951 \ttrain loss: 0.1538875686145587\n",
            "--------------------------------------------------\n",
            "iter: 1952 \ttrain loss: 0.15983812884985626\n",
            "--------------------------------------------------\n",
            "iter: 1953 \ttrain loss: 0.15606460243491985\n",
            "--------------------------------------------------\n",
            "iter: 1954 \ttrain loss: 0.16597286094496957\n",
            "--------------------------------------------------\n",
            "iter: 1955 \ttrain loss: 0.16832257302753087\n",
            "--------------------------------------------------\n",
            "iter: 1956 \ttrain loss: 0.15808272456287528\n",
            "--------------------------------------------------\n",
            "iter: 1957 \ttrain loss: 0.15640024441960831\n",
            "--------------------------------------------------\n",
            "iter: 1958 \ttrain loss: 0.16190702339305185\n",
            "--------------------------------------------------\n",
            "iter: 1959 \ttrain loss: 0.16526128029319717\n",
            "--------------------------------------------------\n",
            "iter: 1960 \ttrain loss: 0.151063901057583\n",
            "--------------------------------------------------\n",
            "iter: 1961 \ttrain loss: 0.16819877187382984\n",
            "--------------------------------------------------\n",
            "iter: 1962 \ttrain loss: 0.1618342804707538\n",
            "--------------------------------------------------\n",
            "iter: 1963 \ttrain loss: 0.17960547458587192\n",
            "--------------------------------------------------\n",
            "iter: 1964 \ttrain loss: 0.16647864304029095\n",
            "--------------------------------------------------\n",
            "iter: 1965 \ttrain loss: 0.15946375695023962\n",
            "--------------------------------------------------\n",
            "iter: 1966 \ttrain loss: 0.1523443468216718\n",
            "--------------------------------------------------\n",
            "iter: 1967 \ttrain loss: 0.15462910904238525\n",
            "--------------------------------------------------\n",
            "iter: 1968 \ttrain loss: 0.18110479266298704\n",
            "--------------------------------------------------\n",
            "iter: 1969 \ttrain loss: 0.1576069744689129\n",
            "--------------------------------------------------\n",
            "iter: 1970 \ttrain loss: 0.16527445857002834\n",
            "--------------------------------------------------\n",
            "iter: 1971 \ttrain loss: 0.16247097795784213\n",
            "--------------------------------------------------\n",
            "iter: 1972 \ttrain loss: 0.1496056665531619\n",
            "--------------------------------------------------\n",
            "iter: 1973 \ttrain loss: 0.15452463355114354\n",
            "--------------------------------------------------\n",
            "iter: 1974 \ttrain loss: 0.15057959543929955\n",
            "--------------------------------------------------\n",
            "iter: 1975 \ttrain loss: 0.1550927752925262\n",
            "--------------------------------------------------\n",
            "iter: 1976 \ttrain loss: 0.17721851863832558\n",
            "--------------------------------------------------\n",
            "iter: 1977 \ttrain loss: 0.1591924134002005\n",
            "--------------------------------------------------\n",
            "iter: 1978 \ttrain loss: 0.15854830730308558\n",
            "--------------------------------------------------\n",
            "iter: 1979 \ttrain loss: 0.1579847961750643\n",
            "--------------------------------------------------\n",
            "iter: 1980 \ttrain loss: 0.15480919084457362\n",
            "--------------------------------------------------\n",
            "iter: 1981 \ttrain loss: 0.16856284740518135\n",
            "--------------------------------------------------\n",
            "iter: 1982 \ttrain loss: 0.17170607032137047\n",
            "--------------------------------------------------\n",
            "iter: 1983 \ttrain loss: 0.1592544162576087\n",
            "--------------------------------------------------\n",
            "iter: 1984 \ttrain loss: 0.15009910834076126\n",
            "--------------------------------------------------\n",
            "iter: 1985 \ttrain loss: 0.16047215515112842\n",
            "--------------------------------------------------\n",
            "iter: 1986 \ttrain loss: 0.16805732444124546\n",
            "--------------------------------------------------\n",
            "iter: 1987 \ttrain loss: 0.15901873874298722\n",
            "--------------------------------------------------\n",
            "iter: 1988 \ttrain loss: 0.15551526868550003\n",
            "--------------------------------------------------\n",
            "iter: 1989 \ttrain loss: 0.14566269393377673\n",
            "--------------------------------------------------\n",
            "iter: 1990 \ttrain loss: 0.17228580612897923\n",
            "--------------------------------------------------\n",
            "iter: 1991 \ttrain loss: 0.15649993590272354\n",
            "--------------------------------------------------\n",
            "iter: 1992 \ttrain loss: 0.18472338826049448\n",
            "--------------------------------------------------\n",
            "iter: 1993 \ttrain loss: 0.1555229821135273\n",
            "--------------------------------------------------\n",
            "iter: 1994 \ttrain loss: 0.1544540813471163\n",
            "--------------------------------------------------\n",
            "iter: 1995 \ttrain loss: 0.15948573721000667\n",
            "--------------------------------------------------\n",
            "iter: 1996 \ttrain loss: 0.15930484524711022\n",
            "--------------------------------------------------\n",
            "iter: 1997 \ttrain loss: 0.15509965461220473\n",
            "--------------------------------------------------\n",
            "iter: 1998 \ttrain loss: 0.15563977839530366\n",
            "--------------------------------------------------\n",
            "iter: 1999 \ttrain loss: 0.1622485406639992\n",
            "--------------------------------------------------\n",
            "iter: 2000 \ttrain loss: 0.15818434123987016\n",
            "--------------------------------------------------\n",
            "iter: 2001 \ttrain loss: 0.15975593983182768\n",
            "--------------------------------------------------\n",
            "iter: 2002 \ttrain loss: 0.1562870650118347\n",
            "--------------------------------------------------\n",
            "iter: 2003 \ttrain loss: 0.15568074237709098\n",
            "--------------------------------------------------\n",
            "iter: 2004 \ttrain loss: 0.15265703502258096\n",
            "--------------------------------------------------\n",
            "iter: 2005 \ttrain loss: 0.17092833768824825\n",
            "--------------------------------------------------\n",
            "iter: 2006 \ttrain loss: 0.15548405603092427\n",
            "--------------------------------------------------\n",
            "iter: 2007 \ttrain loss: 0.16822384941275692\n",
            "--------------------------------------------------\n",
            "iter: 2008 \ttrain loss: 0.15789705583589808\n",
            "--------------------------------------------------\n",
            "iter: 2009 \ttrain loss: 0.1719990907115329\n",
            "--------------------------------------------------\n",
            "iter: 2010 \ttrain loss: 0.1449091216033145\n",
            "--------------------------------------------------\n",
            "iter: 2011 \ttrain loss: 0.16103524328130916\n",
            "--------------------------------------------------\n",
            "iter: 2012 \ttrain loss: 0.1562234484747716\n",
            "--------------------------------------------------\n",
            "iter: 2013 \ttrain loss: 0.17078884005750028\n",
            "--------------------------------------------------\n",
            "iter: 2014 \ttrain loss: 0.14407238863242158\n",
            "--------------------------------------------------\n",
            "iter: 2015 \ttrain loss: 0.16961181904831613\n",
            "--------------------------------------------------\n",
            "iter: 2016 \ttrain loss: 0.16810202105905725\n",
            "--------------------------------------------------\n",
            "iter: 2017 \ttrain loss: 0.15542561324082982\n",
            "--------------------------------------------------\n",
            "iter: 2018 \ttrain loss: 0.1520011770845579\n",
            "--------------------------------------------------\n",
            "iter: 2019 \ttrain loss: 0.15774596532936036\n",
            "--------------------------------------------------\n",
            "iter: 2020 \ttrain loss: 0.14956706020281477\n",
            "--------------------------------------------------\n",
            "iter: 2021 \ttrain loss: 0.16029254507998025\n",
            "--------------------------------------------------\n",
            "iter: 2022 \ttrain loss: 0.1601013969788022\n",
            "--------------------------------------------------\n",
            "iter: 2023 \ttrain loss: 0.15206734133221902\n",
            "--------------------------------------------------\n",
            "iter: 2024 \ttrain loss: 0.16441818035116784\n",
            "--------------------------------------------------\n",
            "iter: 2025 \ttrain loss: 0.16053725813730454\n",
            "--------------------------------------------------\n",
            "iter: 2026 \ttrain loss: 0.1601859074253176\n",
            "--------------------------------------------------\n",
            "iter: 2027 \ttrain loss: 0.1583275487868706\n",
            "--------------------------------------------------\n",
            "iter: 2028 \ttrain loss: 0.15887097786990217\n",
            "--------------------------------------------------\n",
            "iter: 2029 \ttrain loss: 0.15443493297028354\n",
            "--------------------------------------------------\n",
            "iter: 2030 \ttrain loss: 0.15904690030064805\n",
            "--------------------------------------------------\n",
            "iter: 2031 \ttrain loss: 0.16978099549308548\n",
            "--------------------------------------------------\n",
            "iter: 2032 \ttrain loss: 0.149255466074745\n",
            "--------------------------------------------------\n",
            "iter: 2033 \ttrain loss: 0.15948469176141347\n",
            "--------------------------------------------------\n",
            "iter: 2034 \ttrain loss: 0.16461715194423146\n",
            "--------------------------------------------------\n",
            "iter: 2035 \ttrain loss: 0.15340172013266548\n",
            "--------------------------------------------------\n",
            "iter: 2036 \ttrain loss: 0.1639896445445317\n",
            "--------------------------------------------------\n",
            "iter: 2037 \ttrain loss: 0.15364879540039006\n",
            "--------------------------------------------------\n",
            "iter: 2038 \ttrain loss: 0.15838936347912091\n",
            "--------------------------------------------------\n",
            "iter: 2039 \ttrain loss: 0.14682153170395984\n",
            "--------------------------------------------------\n",
            "iter: 2040 \ttrain loss: 0.1494139191022529\n",
            "--------------------------------------------------\n",
            "iter: 2041 \ttrain loss: 0.16069579246450585\n",
            "--------------------------------------------------\n",
            "iter: 2042 \ttrain loss: 0.1704548644005361\n",
            "--------------------------------------------------\n",
            "iter: 2043 \ttrain loss: 0.15185538062898024\n",
            "--------------------------------------------------\n",
            "iter: 2044 \ttrain loss: 0.15953455644057163\n",
            "--------------------------------------------------\n",
            "iter: 2045 \ttrain loss: 0.16070370701655856\n",
            "--------------------------------------------------\n",
            "iter: 2046 \ttrain loss: 0.15463551491340752\n",
            "--------------------------------------------------\n",
            "iter: 2047 \ttrain loss: 0.15979485228569534\n",
            "--------------------------------------------------\n",
            "iter: 2048 \ttrain loss: 0.16286075972669065\n",
            "--------------------------------------------------\n",
            "iter: 2049 \ttrain loss: 0.16551016244495495\n",
            "--------------------------------------------------\n",
            "iter: 2050 \ttrain loss: 0.15636361139146485\n",
            "--------------------------------------------------\n",
            "iter: 2051 \ttrain loss: 0.15014785997402658\n",
            "--------------------------------------------------\n",
            "iter: 2052 \ttrain loss: 0.17117049199200263\n",
            "--------------------------------------------------\n",
            "iter: 2053 \ttrain loss: 0.15675339975442737\n",
            "--------------------------------------------------\n",
            "iter: 2054 \ttrain loss: 0.15226673873180574\n",
            "--------------------------------------------------\n",
            "iter: 2055 \ttrain loss: 0.15422582057530698\n",
            "--------------------------------------------------\n",
            "iter: 2056 \ttrain loss: 0.15179960703288292\n",
            "--------------------------------------------------\n",
            "iter: 2057 \ttrain loss: 0.16030621123603012\n",
            "--------------------------------------------------\n",
            "iter: 2058 \ttrain loss: 0.1522706257756912\n",
            "--------------------------------------------------\n",
            "iter: 2059 \ttrain loss: 0.1682532149444427\n",
            "--------------------------------------------------\n",
            "iter: 2060 \ttrain loss: 0.15278648201311068\n",
            "--------------------------------------------------\n",
            "iter: 2061 \ttrain loss: 0.16137256056937377\n",
            "--------------------------------------------------\n",
            "iter: 2062 \ttrain loss: 0.17772638599668247\n",
            "--------------------------------------------------\n",
            "iter: 2063 \ttrain loss: 0.156900579372081\n",
            "--------------------------------------------------\n",
            "iter: 2064 \ttrain loss: 0.1545421946576846\n",
            "--------------------------------------------------\n",
            "iter: 2065 \ttrain loss: 0.15801432410846394\n",
            "--------------------------------------------------\n",
            "iter: 2066 \ttrain loss: 0.1606753840027821\n",
            "--------------------------------------------------\n",
            "iter: 2067 \ttrain loss: 0.16237987230606185\n",
            "--------------------------------------------------\n",
            "iter: 2068 \ttrain loss: 0.16294862292790577\n",
            "--------------------------------------------------\n",
            "iter: 2069 \ttrain loss: 0.15903984717696448\n",
            "--------------------------------------------------\n",
            "iter: 2070 \ttrain loss: 0.15627597693711465\n",
            "--------------------------------------------------\n",
            "iter: 2071 \ttrain loss: 0.15916336081309973\n",
            "--------------------------------------------------\n",
            "iter: 2072 \ttrain loss: 0.15881491880757473\n",
            "--------------------------------------------------\n",
            "iter: 2073 \ttrain loss: 0.15737595047362457\n",
            "--------------------------------------------------\n",
            "iter: 2074 \ttrain loss: 0.1761001045678082\n",
            "--------------------------------------------------\n",
            "iter: 2075 \ttrain loss: 0.1521768877764684\n",
            "--------------------------------------------------\n",
            "iter: 2076 \ttrain loss: 0.166279140397244\n",
            "--------------------------------------------------\n",
            "iter: 2077 \ttrain loss: 0.14503856604263954\n",
            "--------------------------------------------------\n",
            "iter: 2078 \ttrain loss: 0.17261725010555626\n",
            "--------------------------------------------------\n",
            "iter: 2079 \ttrain loss: 0.15358815913898358\n",
            "--------------------------------------------------\n",
            "iter: 2080 \ttrain loss: 0.15983830502478416\n",
            "--------------------------------------------------\n",
            "iter: 2081 \ttrain loss: 0.15176240694843018\n",
            "--------------------------------------------------\n",
            "iter: 2082 \ttrain loss: 0.17597005105486654\n",
            "--------------------------------------------------\n",
            "iter: 2083 \ttrain loss: 0.15699695707857902\n",
            "--------------------------------------------------\n",
            "iter: 2084 \ttrain loss: 0.16496694794218317\n",
            "--------------------------------------------------\n",
            "iter: 2085 \ttrain loss: 0.14524488641543087\n",
            "--------------------------------------------------\n",
            "iter: 2086 \ttrain loss: 0.15010896852924244\n",
            "--------------------------------------------------\n",
            "iter: 2087 \ttrain loss: 0.15709739983820922\n",
            "--------------------------------------------------\n",
            "iter: 2088 \ttrain loss: 0.1692904324399903\n",
            "--------------------------------------------------\n",
            "iter: 2089 \ttrain loss: 0.14322369001257773\n",
            "--------------------------------------------------\n",
            "iter: 2090 \ttrain loss: 0.15745320728262394\n",
            "--------------------------------------------------\n",
            "iter: 2091 \ttrain loss: 0.16598448325542742\n",
            "--------------------------------------------------\n",
            "iter: 2092 \ttrain loss: 0.1496862135961365\n",
            "--------------------------------------------------\n",
            "iter: 2093 \ttrain loss: 0.15902799705356985\n",
            "--------------------------------------------------\n",
            "iter: 2094 \ttrain loss: 0.1602267365303948\n",
            "--------------------------------------------------\n",
            "iter: 2095 \ttrain loss: 0.16008283697690961\n",
            "--------------------------------------------------\n",
            "iter: 2096 \ttrain loss: 0.15541105679627262\n",
            "--------------------------------------------------\n",
            "iter: 2097 \ttrain loss: 0.16933095696679526\n",
            "--------------------------------------------------\n",
            "iter: 2098 \ttrain loss: 0.14750946171993998\n",
            "--------------------------------------------------\n",
            "iter: 2099 \ttrain loss: 0.15520640366460506\n",
            "--------------------------------------------------\n",
            "iter: 2100 \ttrain loss: 0.1724621955362213\n",
            "--------------------------------------------------\n",
            "iter: 2101 \ttrain loss: 0.16539728925072705\n",
            "--------------------------------------------------\n",
            "iter: 2102 \ttrain loss: 0.14766877336029435\n",
            "--------------------------------------------------\n",
            "iter: 2103 \ttrain loss: 0.16028857465268737\n",
            "--------------------------------------------------\n",
            "iter: 2104 \ttrain loss: 0.17923180816022774\n",
            "--------------------------------------------------\n",
            "iter: 2105 \ttrain loss: 0.14689157029453256\n",
            "--------------------------------------------------\n",
            "iter: 2106 \ttrain loss: 0.1705978907832165\n",
            "--------------------------------------------------\n",
            "iter: 2107 \ttrain loss: 0.15362099102024365\n",
            "--------------------------------------------------\n",
            "iter: 2108 \ttrain loss: 0.16006609519286347\n",
            "--------------------------------------------------\n",
            "iter: 2109 \ttrain loss: 0.14593370266535588\n",
            "--------------------------------------------------\n",
            "iter: 2110 \ttrain loss: 0.16557237404155842\n",
            "--------------------------------------------------\n",
            "iter: 2111 \ttrain loss: 0.1625777345365535\n",
            "--------------------------------------------------\n",
            "iter: 2112 \ttrain loss: 0.15560403418394111\n",
            "--------------------------------------------------\n",
            "iter: 2113 \ttrain loss: 0.1537140285686239\n",
            "--------------------------------------------------\n",
            "iter: 2114 \ttrain loss: 0.155066556115091\n",
            "--------------------------------------------------\n",
            "iter: 2115 \ttrain loss: 0.15541758093103888\n",
            "--------------------------------------------------\n",
            "iter: 2116 \ttrain loss: 0.15676790823604667\n",
            "--------------------------------------------------\n",
            "iter: 2117 \ttrain loss: 0.1552370363511644\n",
            "--------------------------------------------------\n",
            "iter: 2118 \ttrain loss: 0.1507697441167297\n",
            "--------------------------------------------------\n",
            "iter: 2119 \ttrain loss: 0.1792453211518933\n",
            "--------------------------------------------------\n",
            "iter: 2120 \ttrain loss: 0.1636764595568866\n",
            "--------------------------------------------------\n",
            "iter: 2121 \ttrain loss: 0.16151395035917726\n",
            "--------------------------------------------------\n",
            "iter: 2122 \ttrain loss: 0.16117540460963473\n",
            "--------------------------------------------------\n",
            "iter: 2123 \ttrain loss: 0.15281328610449862\n",
            "--------------------------------------------------\n",
            "iter: 2124 \ttrain loss: 0.14597139940008427\n",
            "--------------------------------------------------\n",
            "iter: 2125 \ttrain loss: 0.15252645915532603\n",
            "--------------------------------------------------\n",
            "iter: 2126 \ttrain loss: 0.158660650092444\n",
            "--------------------------------------------------\n",
            "iter: 2127 \ttrain loss: 0.1515527653825546\n",
            "--------------------------------------------------\n",
            "iter: 2128 \ttrain loss: 0.1595492299074191\n",
            "--------------------------------------------------\n",
            "iter: 2129 \ttrain loss: 0.17658846961724625\n",
            "--------------------------------------------------\n",
            "iter: 2130 \ttrain loss: 0.149834408914006\n",
            "--------------------------------------------------\n",
            "iter: 2131 \ttrain loss: 0.15849817686456225\n",
            "--------------------------------------------------\n",
            "iter: 2132 \ttrain loss: 0.157025438321702\n",
            "--------------------------------------------------\n",
            "iter: 2133 \ttrain loss: 0.15807934716058478\n",
            "--------------------------------------------------\n",
            "iter: 2134 \ttrain loss: 0.17453378612948087\n",
            "--------------------------------------------------\n",
            "iter: 2135 \ttrain loss: 0.16149072221308028\n",
            "--------------------------------------------------\n",
            "iter: 2136 \ttrain loss: 0.14978286308915248\n",
            "--------------------------------------------------\n",
            "iter: 2137 \ttrain loss: 0.1573323804189509\n",
            "--------------------------------------------------\n",
            "iter: 2138 \ttrain loss: 0.15853399726502457\n",
            "--------------------------------------------------\n",
            "iter: 2139 \ttrain loss: 0.15752711046024295\n",
            "--------------------------------------------------\n",
            "iter: 2140 \ttrain loss: 0.15950169538713796\n",
            "--------------------------------------------------\n",
            "iter: 2141 \ttrain loss: 0.15700495930076272\n",
            "--------------------------------------------------\n",
            "iter: 2142 \ttrain loss: 0.1618152336576636\n",
            "--------------------------------------------------\n",
            "iter: 2143 \ttrain loss: 0.15563312404778953\n",
            "--------------------------------------------------\n",
            "iter: 2144 \ttrain loss: 0.14657615187861028\n",
            "--------------------------------------------------\n",
            "iter: 2145 \ttrain loss: 0.1633114120483394\n",
            "--------------------------------------------------\n",
            "iter: 2146 \ttrain loss: 0.15360101769735393\n",
            "--------------------------------------------------\n",
            "iter: 2147 \ttrain loss: 0.14659054545539155\n",
            "--------------------------------------------------\n",
            "iter: 2148 \ttrain loss: 0.14908050841991485\n",
            "--------------------------------------------------\n",
            "iter: 2149 \ttrain loss: 0.15675167854488492\n",
            "--------------------------------------------------\n",
            "iter: 2150 \ttrain loss: 0.17130365241447515\n",
            "--------------------------------------------------\n",
            "iter: 2151 \ttrain loss: 0.14901075643893147\n",
            "--------------------------------------------------\n",
            "iter: 2152 \ttrain loss: 0.16330224822993414\n",
            "--------------------------------------------------\n",
            "iter: 2153 \ttrain loss: 0.16662839906393218\n",
            "--------------------------------------------------\n",
            "iter: 2154 \ttrain loss: 0.15009147732995562\n",
            "--------------------------------------------------\n",
            "iter: 2155 \ttrain loss: 0.1795042113272286\n",
            "--------------------------------------------------\n",
            "iter: 2156 \ttrain loss: 0.14879287001638386\n",
            "--------------------------------------------------\n",
            "iter: 2157 \ttrain loss: 0.15164729404544175\n",
            "--------------------------------------------------\n",
            "iter: 2158 \ttrain loss: 0.1579730650056253\n",
            "--------------------------------------------------\n",
            "iter: 2159 \ttrain loss: 0.15557095148284808\n",
            "--------------------------------------------------\n",
            "iter: 2160 \ttrain loss: 0.15649346462820654\n",
            "--------------------------------------------------\n",
            "iter: 2161 \ttrain loss: 0.15178701475592052\n",
            "--------------------------------------------------\n",
            "iter: 2162 \ttrain loss: 0.16310079243785197\n",
            "--------------------------------------------------\n",
            "iter: 2163 \ttrain loss: 0.15616103771956122\n",
            "--------------------------------------------------\n",
            "iter: 2164 \ttrain loss: 0.15792896923273683\n",
            "--------------------------------------------------\n",
            "iter: 2165 \ttrain loss: 0.15035924390218444\n",
            "--------------------------------------------------\n",
            "iter: 2166 \ttrain loss: 0.14677670547034996\n",
            "--------------------------------------------------\n",
            "iter: 2167 \ttrain loss: 0.15571456488954397\n",
            "--------------------------------------------------\n",
            "iter: 2168 \ttrain loss: 0.15927779235856107\n",
            "--------------------------------------------------\n",
            "iter: 2169 \ttrain loss: 0.14672169364680385\n",
            "--------------------------------------------------\n",
            "iter: 2170 \ttrain loss: 0.16140545264569658\n",
            "--------------------------------------------------\n",
            "iter: 2171 \ttrain loss: 0.15401413784968424\n",
            "--------------------------------------------------\n",
            "iter: 2172 \ttrain loss: 0.1487970029069277\n",
            "--------------------------------------------------\n",
            "iter: 2173 \ttrain loss: 0.16021927278841072\n",
            "--------------------------------------------------\n",
            "iter: 2174 \ttrain loss: 0.15429133327407288\n",
            "--------------------------------------------------\n",
            "iter: 2175 \ttrain loss: 0.15070885254032063\n",
            "--------------------------------------------------\n",
            "iter: 2176 \ttrain loss: 0.1475723234822117\n",
            "--------------------------------------------------\n",
            "iter: 2177 \ttrain loss: 0.14976350352504392\n",
            "--------------------------------------------------\n",
            "iter: 2178 \ttrain loss: 0.1532613415765696\n",
            "--------------------------------------------------\n",
            "iter: 2179 \ttrain loss: 0.15850511164186423\n",
            "--------------------------------------------------\n",
            "iter: 2180 \ttrain loss: 0.1535511524378937\n",
            "--------------------------------------------------\n",
            "iter: 2181 \ttrain loss: 0.15382508924701124\n",
            "--------------------------------------------------\n",
            "iter: 2182 \ttrain loss: 0.15158147921265844\n",
            "--------------------------------------------------\n",
            "iter: 2183 \ttrain loss: 0.15263688242397658\n",
            "--------------------------------------------------\n",
            "iter: 2184 \ttrain loss: 0.1494370395922281\n",
            "--------------------------------------------------\n",
            "iter: 2185 \ttrain loss: 0.16171800614004117\n",
            "--------------------------------------------------\n",
            "iter: 2186 \ttrain loss: 0.1530943049013166\n",
            "--------------------------------------------------\n",
            "iter: 2187 \ttrain loss: 0.1509527379293455\n",
            "--------------------------------------------------\n",
            "iter: 2188 \ttrain loss: 0.15316583147627374\n",
            "--------------------------------------------------\n",
            "iter: 2189 \ttrain loss: 0.15621910782193205\n",
            "--------------------------------------------------\n",
            "iter: 2190 \ttrain loss: 0.1459616515453991\n",
            "--------------------------------------------------\n",
            "iter: 2191 \ttrain loss: 0.16189681443694837\n",
            "--------------------------------------------------\n",
            "iter: 2192 \ttrain loss: 0.1508120486654803\n",
            "--------------------------------------------------\n",
            "iter: 2193 \ttrain loss: 0.15798484224854234\n",
            "--------------------------------------------------\n",
            "iter: 2194 \ttrain loss: 0.14427753598099813\n",
            "--------------------------------------------------\n",
            "iter: 2195 \ttrain loss: 0.16094735475093808\n",
            "--------------------------------------------------\n",
            "iter: 2196 \ttrain loss: 0.14978637862943622\n",
            "--------------------------------------------------\n",
            "iter: 2197 \ttrain loss: 0.15077751556728283\n",
            "--------------------------------------------------\n",
            "iter: 2198 \ttrain loss: 0.1504416450620506\n",
            "--------------------------------------------------\n",
            "iter: 2199 \ttrain loss: 0.15858655942659752\n",
            "--------------------------------------------------\n",
            "iter: 2200 \ttrain loss: 0.1470411686669746\n",
            "--------------------------------------------------\n",
            "iter: 2201 \ttrain loss: 0.14071919636261623\n",
            "--------------------------------------------------\n",
            "iter: 2202 \ttrain loss: 0.1604325098323812\n",
            "--------------------------------------------------\n",
            "iter: 2203 \ttrain loss: 0.1431460125371961\n",
            "--------------------------------------------------\n",
            "iter: 2204 \ttrain loss: 0.1564527866697793\n",
            "--------------------------------------------------\n",
            "iter: 2205 \ttrain loss: 0.14697800391801583\n",
            "--------------------------------------------------\n",
            "iter: 2206 \ttrain loss: 0.1570192489921536\n",
            "--------------------------------------------------\n",
            "iter: 2207 \ttrain loss: 0.15170056604225282\n",
            "--------------------------------------------------\n",
            "iter: 2208 \ttrain loss: 0.14840720561383403\n",
            "--------------------------------------------------\n",
            "iter: 2209 \ttrain loss: 0.15477559431123533\n",
            "--------------------------------------------------\n",
            "iter: 2210 \ttrain loss: 0.1574299536484096\n",
            "--------------------------------------------------\n",
            "iter: 2211 \ttrain loss: 0.15270553248459395\n",
            "--------------------------------------------------\n",
            "iter: 2212 \ttrain loss: 0.14871491274812254\n",
            "--------------------------------------------------\n",
            "iter: 2213 \ttrain loss: 0.14507644905366826\n",
            "--------------------------------------------------\n",
            "iter: 2214 \ttrain loss: 0.13840880072742967\n",
            "--------------------------------------------------\n",
            "iter: 2215 \ttrain loss: 0.15834766737486572\n",
            "--------------------------------------------------\n",
            "iter: 2216 \ttrain loss: 0.1622942815808396\n",
            "--------------------------------------------------\n",
            "iter: 2217 \ttrain loss: 0.13944490675090848\n",
            "--------------------------------------------------\n",
            "iter: 2218 \ttrain loss: 0.1530263355299343\n",
            "--------------------------------------------------\n",
            "iter: 2219 \ttrain loss: 0.15293379622838743\n",
            "--------------------------------------------------\n",
            "iter: 2220 \ttrain loss: 0.14981380693200688\n",
            "--------------------------------------------------\n",
            "iter: 2221 \ttrain loss: 0.1507664712180713\n",
            "--------------------------------------------------\n",
            "iter: 2222 \ttrain loss: 0.15204808054181898\n",
            "--------------------------------------------------\n",
            "iter: 2223 \ttrain loss: 0.13695445583638668\n",
            "--------------------------------------------------\n",
            "iter: 2224 \ttrain loss: 0.15344111022009405\n",
            "--------------------------------------------------\n",
            "iter: 2225 \ttrain loss: 0.1454180288946888\n",
            "--------------------------------------------------\n",
            "iter: 2226 \ttrain loss: 0.15930255404274135\n",
            "--------------------------------------------------\n",
            "iter: 2227 \ttrain loss: 0.1551010453041498\n",
            "--------------------------------------------------\n",
            "iter: 2228 \ttrain loss: 0.13719690226018544\n",
            "--------------------------------------------------\n",
            "iter: 2229 \ttrain loss: 0.15224545787891117\n",
            "--------------------------------------------------\n",
            "iter: 2230 \ttrain loss: 0.14660735025056157\n",
            "--------------------------------------------------\n",
            "iter: 2231 \ttrain loss: 0.14828857857843983\n",
            "--------------------------------------------------\n",
            "iter: 2232 \ttrain loss: 0.15081016676622758\n",
            "--------------------------------------------------\n",
            "iter: 2233 \ttrain loss: 0.15110128816118637\n",
            "--------------------------------------------------\n",
            "iter: 2234 \ttrain loss: 0.15165884836736884\n",
            "--------------------------------------------------\n",
            "iter: 2235 \ttrain loss: 0.1496283842645097\n",
            "--------------------------------------------------\n",
            "iter: 2236 \ttrain loss: 0.14349068772654855\n",
            "--------------------------------------------------\n",
            "iter: 2237 \ttrain loss: 0.14637028783800418\n",
            "--------------------------------------------------\n",
            "iter: 2238 \ttrain loss: 0.15052360220309538\n",
            "--------------------------------------------------\n",
            "iter: 2239 \ttrain loss: 0.1463377441978017\n",
            "--------------------------------------------------\n",
            "iter: 2240 \ttrain loss: 0.14136247627988185\n",
            "--------------------------------------------------\n",
            "iter: 2241 \ttrain loss: 0.14057924757637963\n",
            "--------------------------------------------------\n",
            "iter: 2242 \ttrain loss: 0.14432798578745304\n",
            "--------------------------------------------------\n",
            "iter: 2243 \ttrain loss: 0.14114535441578777\n",
            "--------------------------------------------------\n",
            "iter: 2244 \ttrain loss: 0.1750077366103908\n",
            "--------------------------------------------------\n",
            "iter: 2245 \ttrain loss: 0.15711703113513648\n",
            "--------------------------------------------------\n",
            "iter: 2246 \ttrain loss: 0.1369444414916404\n",
            "--------------------------------------------------\n",
            "iter: 2247 \ttrain loss: 0.16231222589726288\n",
            "--------------------------------------------------\n",
            "iter: 2248 \ttrain loss: 0.1429750044813432\n",
            "--------------------------------------------------\n",
            "iter: 2249 \ttrain loss: 0.16211121928694838\n",
            "--------------------------------------------------\n",
            "iter: 2250 \ttrain loss: 0.15128020241173265\n",
            "--------------------------------------------------\n",
            "iter: 2251 \ttrain loss: 0.14801808724896548\n",
            "--------------------------------------------------\n",
            "iter: 2252 \ttrain loss: 0.14229913662222538\n",
            "--------------------------------------------------\n",
            "iter: 2253 \ttrain loss: 0.15570582070157538\n",
            "--------------------------------------------------\n",
            "iter: 2254 \ttrain loss: 0.1449414427724329\n",
            "--------------------------------------------------\n",
            "iter: 2255 \ttrain loss: 0.14368704181819356\n",
            "--------------------------------------------------\n",
            "iter: 2256 \ttrain loss: 0.14422871773609883\n",
            "--------------------------------------------------\n",
            "iter: 2257 \ttrain loss: 0.14204886841364414\n",
            "--------------------------------------------------\n",
            "iter: 2258 \ttrain loss: 0.1372037290377931\n",
            "--------------------------------------------------\n",
            "iter: 2259 \ttrain loss: 0.14633756402999443\n",
            "--------------------------------------------------\n",
            "iter: 2260 \ttrain loss: 0.1355705983345702\n",
            "--------------------------------------------------\n",
            "iter: 2261 \ttrain loss: 0.13881439027791528\n",
            "--------------------------------------------------\n",
            "iter: 2262 \ttrain loss: 0.14013230308836322\n",
            "--------------------------------------------------\n",
            "iter: 2263 \ttrain loss: 0.1590834329145027\n",
            "--------------------------------------------------\n",
            "iter: 2264 \ttrain loss: 0.13604664023561927\n",
            "--------------------------------------------------\n",
            "iter: 2265 \ttrain loss: 0.1446189444323969\n",
            "--------------------------------------------------\n",
            "iter: 2266 \ttrain loss: 0.14179104577324528\n",
            "--------------------------------------------------\n",
            "iter: 2267 \ttrain loss: 0.13111135559597192\n",
            "--------------------------------------------------\n",
            "iter: 2268 \ttrain loss: 0.1476242576389484\n",
            "--------------------------------------------------\n",
            "iter: 2269 \ttrain loss: 0.13315987712730856\n",
            "--------------------------------------------------\n",
            "iter: 2270 \ttrain loss: 0.1407259954165046\n",
            "--------------------------------------------------\n",
            "iter: 2271 \ttrain loss: 0.1494142209609318\n",
            "--------------------------------------------------\n",
            "iter: 2272 \ttrain loss: 0.14080922217132133\n",
            "--------------------------------------------------\n",
            "iter: 2273 \ttrain loss: 0.1476350584298087\n",
            "--------------------------------------------------\n",
            "iter: 2274 \ttrain loss: 0.1328022371088576\n",
            "--------------------------------------------------\n",
            "iter: 2275 \ttrain loss: 0.1375930420698589\n",
            "--------------------------------------------------\n",
            "iter: 2276 \ttrain loss: 0.14711490541489025\n",
            "--------------------------------------------------\n",
            "iter: 2277 \ttrain loss: 0.1355174557337991\n",
            "--------------------------------------------------\n",
            "iter: 2278 \ttrain loss: 0.14531276721546543\n",
            "--------------------------------------------------\n",
            "iter: 2279 \ttrain loss: 0.14813798451783647\n",
            "--------------------------------------------------\n",
            "iter: 2280 \ttrain loss: 0.13855847405454805\n",
            "--------------------------------------------------\n",
            "iter: 2281 \ttrain loss: 0.14340841354337475\n",
            "--------------------------------------------------\n",
            "iter: 2282 \ttrain loss: 0.13004607819100905\n",
            "--------------------------------------------------\n",
            "iter: 2283 \ttrain loss: 0.16205151832788223\n",
            "--------------------------------------------------\n",
            "iter: 2284 \ttrain loss: 0.12930197501310894\n",
            "--------------------------------------------------\n",
            "iter: 2285 \ttrain loss: 0.15266453188902157\n",
            "--------------------------------------------------\n",
            "iter: 2286 \ttrain loss: 0.13929785661926028\n",
            "--------------------------------------------------\n",
            "iter: 2287 \ttrain loss: 0.1307480694550283\n",
            "--------------------------------------------------\n",
            "iter: 2288 \ttrain loss: 0.1484058310421916\n",
            "--------------------------------------------------\n",
            "iter: 2289 \ttrain loss: 0.12741840468630228\n",
            "--------------------------------------------------\n",
            "iter: 2290 \ttrain loss: 0.1359656501977056\n",
            "--------------------------------------------------\n",
            "iter: 2291 \ttrain loss: 0.14660312961176697\n",
            "--------------------------------------------------\n",
            "iter: 2292 \ttrain loss: 0.13816336231602974\n",
            "--------------------------------------------------\n",
            "iter: 2293 \ttrain loss: 0.12847029371171906\n",
            "--------------------------------------------------\n",
            "iter: 2294 \ttrain loss: 0.1358123667874792\n",
            "--------------------------------------------------\n",
            "iter: 2295 \ttrain loss: 0.13025466910220265\n",
            "--------------------------------------------------\n",
            "iter: 2296 \ttrain loss: 0.13862699098311698\n",
            "--------------------------------------------------\n",
            "iter: 2297 \ttrain loss: 0.1265160405958135\n",
            "--------------------------------------------------\n",
            "iter: 2298 \ttrain loss: 0.13568034750537264\n",
            "--------------------------------------------------\n",
            "iter: 2299 \ttrain loss: 0.12597836920626462\n",
            "--------------------------------------------------\n",
            "iter: 2300 \ttrain loss: 0.13288371849690808\n",
            "--------------------------------------------------\n",
            "iter: 2301 \ttrain loss: 0.13361958841019994\n",
            "--------------------------------------------------\n",
            "iter: 2302 \ttrain loss: 0.13358778192172546\n",
            "--------------------------------------------------\n",
            "iter: 2303 \ttrain loss: 0.1335820655011224\n",
            "--------------------------------------------------\n",
            "iter: 2304 \ttrain loss: 0.1274353974975684\n",
            "--------------------------------------------------\n",
            "iter: 2305 \ttrain loss: 0.1340950760466488\n",
            "--------------------------------------------------\n",
            "iter: 2306 \ttrain loss: 0.1282396557440428\n",
            "--------------------------------------------------\n",
            "iter: 2307 \ttrain loss: 0.13035832706400027\n",
            "--------------------------------------------------\n",
            "iter: 2308 \ttrain loss: 0.1358978249719585\n",
            "--------------------------------------------------\n",
            "iter: 2309 \ttrain loss: 0.12300103801371867\n",
            "--------------------------------------------------\n",
            "iter: 2310 \ttrain loss: 0.1469735688178209\n",
            "--------------------------------------------------\n",
            "iter: 2311 \ttrain loss: 0.12275827132034288\n",
            "--------------------------------------------------\n",
            "iter: 2312 \ttrain loss: 0.1394856651388544\n",
            "--------------------------------------------------\n",
            "iter: 2313 \ttrain loss: 0.12352795247104709\n",
            "--------------------------------------------------\n",
            "iter: 2314 \ttrain loss: 0.13960942317754033\n",
            "--------------------------------------------------\n",
            "iter: 2315 \ttrain loss: 0.13175216996823533\n",
            "--------------------------------------------------\n",
            "iter: 2316 \ttrain loss: 0.1286108216710695\n",
            "--------------------------------------------------\n",
            "iter: 2317 \ttrain loss: 0.1241922007319777\n",
            "--------------------------------------------------\n",
            "iter: 2318 \ttrain loss: 0.14159490745729825\n",
            "--------------------------------------------------\n",
            "iter: 2319 \ttrain loss: 0.12869473478021562\n",
            "--------------------------------------------------\n",
            "iter: 2320 \ttrain loss: 0.13369209073094954\n",
            "--------------------------------------------------\n",
            "iter: 2321 \ttrain loss: 0.13765561085166095\n",
            "--------------------------------------------------\n",
            "iter: 2322 \ttrain loss: 0.1310033052812229\n",
            "--------------------------------------------------\n",
            "iter: 2323 \ttrain loss: 0.12691438295306204\n",
            "--------------------------------------------------\n",
            "iter: 2324 \ttrain loss: 0.12977872605737378\n",
            "--------------------------------------------------\n",
            "iter: 2325 \ttrain loss: 0.13117100720321268\n",
            "--------------------------------------------------\n",
            "iter: 2326 \ttrain loss: 0.12721726030258929\n",
            "--------------------------------------------------\n",
            "iter: 2327 \ttrain loss: 0.13637815787490257\n",
            "--------------------------------------------------\n",
            "iter: 2328 \ttrain loss: 0.12648646268135869\n",
            "--------------------------------------------------\n",
            "iter: 2329 \ttrain loss: 0.12325085274349198\n",
            "--------------------------------------------------\n",
            "iter: 2330 \ttrain loss: 0.13121230522413796\n",
            "--------------------------------------------------\n",
            "iter: 2331 \ttrain loss: 0.1209945777557191\n",
            "--------------------------------------------------\n",
            "iter: 2332 \ttrain loss: 0.13749160740781663\n",
            "--------------------------------------------------\n",
            "iter: 2333 \ttrain loss: 0.1270284163851961\n",
            "--------------------------------------------------\n",
            "iter: 2334 \ttrain loss: 0.13055190655550306\n",
            "--------------------------------------------------\n",
            "iter: 2335 \ttrain loss: 0.13571141412548626\n",
            "--------------------------------------------------\n",
            "iter: 2336 \ttrain loss: 0.12543979137431244\n",
            "--------------------------------------------------\n",
            "iter: 2337 \ttrain loss: 0.11898760630775315\n",
            "--------------------------------------------------\n",
            "iter: 2338 \ttrain loss: 0.13795448637773425\n",
            "--------------------------------------------------\n",
            "iter: 2339 \ttrain loss: 0.14448365745248945\n",
            "--------------------------------------------------\n",
            "iter: 2340 \ttrain loss: 0.12432090149379847\n",
            "--------------------------------------------------\n",
            "iter: 2341 \ttrain loss: 0.12996826865599054\n",
            "--------------------------------------------------\n",
            "iter: 2342 \ttrain loss: 0.12787784655348944\n",
            "--------------------------------------------------\n",
            "iter: 2343 \ttrain loss: 0.1290678318676497\n",
            "--------------------------------------------------\n",
            "iter: 2344 \ttrain loss: 0.12261740776548548\n",
            "--------------------------------------------------\n",
            "iter: 2345 \ttrain loss: 0.12734938269052007\n",
            "--------------------------------------------------\n",
            "iter: 2346 \ttrain loss: 0.12793281879250126\n",
            "--------------------------------------------------\n",
            "iter: 2347 \ttrain loss: 0.12940535326663705\n",
            "--------------------------------------------------\n",
            "iter: 2348 \ttrain loss: 0.1268542943226716\n",
            "--------------------------------------------------\n",
            "iter: 2349 \ttrain loss: 0.1319948465856671\n",
            "--------------------------------------------------\n",
            "iter: 2350 \ttrain loss: 0.12498911741943743\n",
            "--------------------------------------------------\n",
            "iter: 2351 \ttrain loss: 0.1280956577885597\n",
            "--------------------------------------------------\n",
            "iter: 2352 \ttrain loss: 0.15758065207001629\n",
            "--------------------------------------------------\n",
            "iter: 2353 \ttrain loss: 0.12127455291403474\n",
            "--------------------------------------------------\n",
            "iter: 2354 \ttrain loss: 0.12308060986970705\n",
            "--------------------------------------------------\n",
            "iter: 2355 \ttrain loss: 0.14077156917681422\n",
            "--------------------------------------------------\n",
            "iter: 2356 \ttrain loss: 0.11989887840402268\n",
            "--------------------------------------------------\n",
            "iter: 2357 \ttrain loss: 0.1294428548191677\n",
            "--------------------------------------------------\n",
            "iter: 2358 \ttrain loss: 0.12877045909161056\n",
            "--------------------------------------------------\n",
            "iter: 2359 \ttrain loss: 0.11923015395066285\n",
            "--------------------------------------------------\n",
            "iter: 2360 \ttrain loss: 0.12763776691161957\n",
            "--------------------------------------------------\n",
            "iter: 2361 \ttrain loss: 0.12782972163615727\n",
            "--------------------------------------------------\n",
            "iter: 2362 \ttrain loss: 0.12856784111623976\n",
            "--------------------------------------------------\n",
            "iter: 2363 \ttrain loss: 0.12106349024323987\n",
            "--------------------------------------------------\n",
            "iter: 2364 \ttrain loss: 0.13514242153048636\n",
            "--------------------------------------------------\n",
            "iter: 2365 \ttrain loss: 0.12409343532056051\n",
            "--------------------------------------------------\n",
            "iter: 2366 \ttrain loss: 0.12442889321299674\n",
            "--------------------------------------------------\n",
            "iter: 2367 \ttrain loss: 0.15011414048914531\n",
            "--------------------------------------------------\n",
            "iter: 2368 \ttrain loss: 0.12659061415636383\n",
            "--------------------------------------------------\n",
            "iter: 2369 \ttrain loss: 0.13408218250572665\n",
            "--------------------------------------------------\n",
            "iter: 2370 \ttrain loss: 0.12529847112119263\n",
            "--------------------------------------------------\n",
            "iter: 2371 \ttrain loss: 0.12518540025198813\n",
            "--------------------------------------------------\n",
            "iter: 2372 \ttrain loss: 0.12129494672957244\n",
            "--------------------------------------------------\n",
            "iter: 2373 \ttrain loss: 0.1245834579463684\n",
            "--------------------------------------------------\n",
            "iter: 2374 \ttrain loss: 0.12925545984935619\n",
            "--------------------------------------------------\n",
            "iter: 2375 \ttrain loss: 0.1180523284137096\n",
            "--------------------------------------------------\n",
            "iter: 2376 \ttrain loss: 0.12211994644801795\n",
            "--------------------------------------------------\n",
            "iter: 2377 \ttrain loss: 0.13262616116375942\n",
            "--------------------------------------------------\n",
            "iter: 2378 \ttrain loss: 0.1335297861343539\n",
            "--------------------------------------------------\n",
            "iter: 2379 \ttrain loss: 0.1215917025969508\n",
            "--------------------------------------------------\n",
            "iter: 2380 \ttrain loss: 0.1266147577850615\n",
            "--------------------------------------------------\n",
            "iter: 2381 \ttrain loss: 0.13835930785273087\n",
            "--------------------------------------------------\n",
            "iter: 2382 \ttrain loss: 0.11655732443280199\n",
            "--------------------------------------------------\n",
            "iter: 2383 \ttrain loss: 0.12445921886184541\n",
            "--------------------------------------------------\n",
            "iter: 2384 \ttrain loss: 0.1295435022132304\n",
            "--------------------------------------------------\n",
            "iter: 2385 \ttrain loss: 0.12437995305428871\n",
            "--------------------------------------------------\n",
            "iter: 2386 \ttrain loss: 0.12606221756932925\n",
            "--------------------------------------------------\n",
            "iter: 2387 \ttrain loss: 0.14210037180105647\n",
            "--------------------------------------------------\n",
            "iter: 2388 \ttrain loss: 0.12244866663981768\n",
            "--------------------------------------------------\n",
            "iter: 2389 \ttrain loss: 0.125047896582462\n",
            "--------------------------------------------------\n",
            "iter: 2390 \ttrain loss: 0.12252763193888382\n",
            "--------------------------------------------------\n",
            "iter: 2391 \ttrain loss: 0.12750919858258727\n",
            "--------------------------------------------------\n",
            "iter: 2392 \ttrain loss: 0.11799612661228945\n",
            "--------------------------------------------------\n",
            "iter: 2393 \ttrain loss: 0.12736317964157087\n",
            "--------------------------------------------------\n",
            "iter: 2394 \ttrain loss: 0.12708796591960067\n",
            "--------------------------------------------------\n",
            "iter: 2395 \ttrain loss: 0.12366403396090166\n",
            "--------------------------------------------------\n",
            "iter: 2396 \ttrain loss: 0.11729639068346899\n",
            "--------------------------------------------------\n",
            "iter: 2397 \ttrain loss: 0.13021281797965586\n",
            "--------------------------------------------------\n",
            "iter: 2398 \ttrain loss: 0.12888824796392176\n",
            "--------------------------------------------------\n",
            "iter: 2399 \ttrain loss: 0.126391391784402\n",
            "--------------------------------------------------\n",
            "iter: 2400 \ttrain loss: 0.12204924613378104\n",
            "--------------------------------------------------\n",
            "iter: 2401 \ttrain loss: 0.12405498775270316\n",
            "--------------------------------------------------\n",
            "iter: 2402 \ttrain loss: 0.12634931604035768\n",
            "--------------------------------------------------\n",
            "iter: 2403 \ttrain loss: 0.12902252253625127\n",
            "--------------------------------------------------\n",
            "iter: 2404 \ttrain loss: 0.11822307569979834\n",
            "--------------------------------------------------\n",
            "iter: 2405 \ttrain loss: 0.12413636957811947\n",
            "--------------------------------------------------\n",
            "iter: 2406 \ttrain loss: 0.11579707728192173\n",
            "--------------------------------------------------\n",
            "iter: 2407 \ttrain loss: 0.13095847484251807\n",
            "--------------------------------------------------\n",
            "iter: 2408 \ttrain loss: 0.11829520360759921\n",
            "--------------------------------------------------\n",
            "iter: 2409 \ttrain loss: 0.11735535246422145\n",
            "--------------------------------------------------\n",
            "iter: 2410 \ttrain loss: 0.12478570447394687\n",
            "--------------------------------------------------\n",
            "iter: 2411 \ttrain loss: 0.11621780695937284\n",
            "--------------------------------------------------\n",
            "iter: 2412 \ttrain loss: 0.12406725602811858\n",
            "--------------------------------------------------\n",
            "iter: 2413 \ttrain loss: 0.12469607101766508\n",
            "--------------------------------------------------\n",
            "iter: 2414 \ttrain loss: 0.13555128846403502\n",
            "--------------------------------------------------\n",
            "iter: 2415 \ttrain loss: 0.12971080370666171\n",
            "--------------------------------------------------\n",
            "iter: 2416 \ttrain loss: 0.12814695446270444\n",
            "--------------------------------------------------\n",
            "iter: 2417 \ttrain loss: 0.1298805053572291\n",
            "--------------------------------------------------\n",
            "iter: 2418 \ttrain loss: 0.12829110983037095\n",
            "--------------------------------------------------\n",
            "iter: 2419 \ttrain loss: 0.11677210383913793\n",
            "--------------------------------------------------\n",
            "iter: 2420 \ttrain loss: 0.1257990760727736\n",
            "--------------------------------------------------\n",
            "iter: 2421 \ttrain loss: 0.12027208628267193\n",
            "--------------------------------------------------\n",
            "iter: 2422 \ttrain loss: 0.12394511533820708\n",
            "--------------------------------------------------\n",
            "iter: 2423 \ttrain loss: 0.11673303137234456\n",
            "--------------------------------------------------\n",
            "iter: 2424 \ttrain loss: 0.1349969219904439\n",
            "--------------------------------------------------\n",
            "iter: 2425 \ttrain loss: 0.11690402780692971\n",
            "--------------------------------------------------\n",
            "iter: 2426 \ttrain loss: 0.12054016805945413\n",
            "--------------------------------------------------\n",
            "iter: 2427 \ttrain loss: 0.12882089290321047\n",
            "--------------------------------------------------\n",
            "iter: 2428 \ttrain loss: 0.13259962111372617\n",
            "--------------------------------------------------\n",
            "iter: 2429 \ttrain loss: 0.10382118437994774\n",
            "--------------------------------------------------\n",
            "iter: 2430 \ttrain loss: 0.1361698139924574\n",
            "--------------------------------------------------\n",
            "iter: 2431 \ttrain loss: 0.11541110162559617\n",
            "--------------------------------------------------\n",
            "iter: 2432 \ttrain loss: 0.1256376166239281\n",
            "--------------------------------------------------\n",
            "iter: 2433 \ttrain loss: 0.12392371907346827\n",
            "--------------------------------------------------\n",
            "iter: 2434 \ttrain loss: 0.12473184595610626\n",
            "--------------------------------------------------\n",
            "iter: 2435 \ttrain loss: 0.12043644403474146\n",
            "--------------------------------------------------\n",
            "iter: 2436 \ttrain loss: 0.12578087604996038\n",
            "--------------------------------------------------\n",
            "iter: 2437 \ttrain loss: 0.13382349744483388\n",
            "--------------------------------------------------\n",
            "iter: 2438 \ttrain loss: 0.1202130027573284\n",
            "--------------------------------------------------\n",
            "iter: 2439 \ttrain loss: 0.12291877360063515\n",
            "--------------------------------------------------\n",
            "iter: 2440 \ttrain loss: 0.1228052919725088\n",
            "--------------------------------------------------\n",
            "iter: 2441 \ttrain loss: 0.12081828764926622\n",
            "--------------------------------------------------\n",
            "iter: 2442 \ttrain loss: 0.13482269218099321\n",
            "--------------------------------------------------\n",
            "iter: 2443 \ttrain loss: 0.11843852466311504\n",
            "--------------------------------------------------\n",
            "iter: 2444 \ttrain loss: 0.1272001924731871\n",
            "--------------------------------------------------\n",
            "iter: 2445 \ttrain loss: 0.11853559113045083\n",
            "--------------------------------------------------\n",
            "iter: 2446 \ttrain loss: 0.1244214253615013\n",
            "--------------------------------------------------\n",
            "iter: 2447 \ttrain loss: 0.12607270506277668\n",
            "--------------------------------------------------\n",
            "iter: 2448 \ttrain loss: 0.11427934314542153\n",
            "--------------------------------------------------\n",
            "iter: 2449 \ttrain loss: 0.13947053848833388\n",
            "--------------------------------------------------\n",
            "iter: 2450 \ttrain loss: 0.1248922180592981\n",
            "--------------------------------------------------\n",
            "iter: 2451 \ttrain loss: 0.12229353797890839\n",
            "--------------------------------------------------\n",
            "iter: 2452 \ttrain loss: 0.12596988519113436\n",
            "--------------------------------------------------\n",
            "iter: 2453 \ttrain loss: 0.14242103313862386\n",
            "--------------------------------------------------\n",
            "iter: 2454 \ttrain loss: 0.11995826001691495\n",
            "--------------------------------------------------\n",
            "iter: 2455 \ttrain loss: 0.12600682253769643\n",
            "--------------------------------------------------\n",
            "iter: 2456 \ttrain loss: 0.11171336603438893\n",
            "--------------------------------------------------\n",
            "iter: 2457 \ttrain loss: 0.13534533867740858\n",
            "--------------------------------------------------\n",
            "iter: 2458 \ttrain loss: 0.11914996561540958\n",
            "--------------------------------------------------\n",
            "iter: 2459 \ttrain loss: 0.12003017146218754\n",
            "--------------------------------------------------\n",
            "iter: 2460 \ttrain loss: 0.1277517433666383\n",
            "--------------------------------------------------\n",
            "iter: 2461 \ttrain loss: 0.12593494509061268\n",
            "--------------------------------------------------\n",
            "iter: 2462 \ttrain loss: 0.12335642918911965\n",
            "--------------------------------------------------\n",
            "iter: 2463 \ttrain loss: 0.12328548540762442\n",
            "--------------------------------------------------\n",
            "iter: 2464 \ttrain loss: 0.11763831616371699\n",
            "--------------------------------------------------\n",
            "iter: 2465 \ttrain loss: 0.11558037842368295\n",
            "--------------------------------------------------\n",
            "iter: 2466 \ttrain loss: 0.12169273847394373\n",
            "--------------------------------------------------\n",
            "iter: 2467 \ttrain loss: 0.1155110299153915\n",
            "--------------------------------------------------\n",
            "iter: 2468 \ttrain loss: 0.1285754095365874\n",
            "--------------------------------------------------\n",
            "iter: 2469 \ttrain loss: 0.11659824751759026\n",
            "--------------------------------------------------\n",
            "iter: 2470 \ttrain loss: 0.12484751158808625\n",
            "--------------------------------------------------\n",
            "iter: 2471 \ttrain loss: 0.12696287840318013\n",
            "--------------------------------------------------\n",
            "iter: 2472 \ttrain loss: 0.12917594592549503\n",
            "--------------------------------------------------\n",
            "iter: 2473 \ttrain loss: 0.11607191095106616\n",
            "--------------------------------------------------\n",
            "iter: 2474 \ttrain loss: 0.11398281247422964\n",
            "--------------------------------------------------\n",
            "iter: 2475 \ttrain loss: 0.13225999553112577\n",
            "--------------------------------------------------\n",
            "iter: 2476 \ttrain loss: 0.12125986033139831\n",
            "--------------------------------------------------\n",
            "iter: 2477 \ttrain loss: 0.14687083697343808\n",
            "--------------------------------------------------\n",
            "iter: 2478 \ttrain loss: 0.12857211161150287\n",
            "--------------------------------------------------\n",
            "iter: 2479 \ttrain loss: 0.12152780799307768\n",
            "--------------------------------------------------\n",
            "iter: 2480 \ttrain loss: 0.1189824486583023\n",
            "--------------------------------------------------\n",
            "iter: 2481 \ttrain loss: 0.1254293565299119\n",
            "--------------------------------------------------\n",
            "iter: 2482 \ttrain loss: 0.1258779427140129\n",
            "--------------------------------------------------\n",
            "iter: 2483 \ttrain loss: 0.1232318123177794\n",
            "--------------------------------------------------\n",
            "iter: 2484 \ttrain loss: 0.11678046853762232\n",
            "--------------------------------------------------\n",
            "iter: 2485 \ttrain loss: 0.12386620643996803\n",
            "--------------------------------------------------\n",
            "iter: 2486 \ttrain loss: 0.12244359126145957\n",
            "--------------------------------------------------\n",
            "iter: 2487 \ttrain loss: 0.13186713383023388\n",
            "--------------------------------------------------\n",
            "iter: 2488 \ttrain loss: 0.12452940851294003\n",
            "--------------------------------------------------\n",
            "iter: 2489 \ttrain loss: 0.1195050470903011\n",
            "--------------------------------------------------\n",
            "iter: 2490 \ttrain loss: 0.11195610744979133\n",
            "--------------------------------------------------\n",
            "iter: 2491 \ttrain loss: 0.1272581108670006\n",
            "--------------------------------------------------\n",
            "iter: 2492 \ttrain loss: 0.12407788822832876\n",
            "--------------------------------------------------\n",
            "iter: 2493 \ttrain loss: 0.11542936874347319\n",
            "--------------------------------------------------\n",
            "iter: 2494 \ttrain loss: 0.1326586618121038\n",
            "--------------------------------------------------\n",
            "iter: 2495 \ttrain loss: 0.12188852979764397\n",
            "--------------------------------------------------\n",
            "iter: 2496 \ttrain loss: 0.12857972999050962\n",
            "--------------------------------------------------\n",
            "iter: 2497 \ttrain loss: 0.12408692620637178\n",
            "--------------------------------------------------\n",
            "iter: 2498 \ttrain loss: 0.13684393042112458\n",
            "--------------------------------------------------\n",
            "iter: 2499 \ttrain loss: 0.11914071786927201\n",
            "--------------------------------------------------\n",
            "iter: 2500 \ttrain loss: 0.11663156579594562\n",
            "--------------------------------------------------\n",
            "iter: 2501 \ttrain loss: 0.13025701640851425\n",
            "--------------------------------------------------\n",
            "iter: 2502 \ttrain loss: 0.11873671126756834\n",
            "--------------------------------------------------\n",
            "iter: 2503 \ttrain loss: 0.11960705648011377\n",
            "--------------------------------------------------\n",
            "iter: 2504 \ttrain loss: 0.12695319402988664\n",
            "--------------------------------------------------\n",
            "iter: 2505 \ttrain loss: 0.12061941971890994\n",
            "--------------------------------------------------\n",
            "iter: 2506 \ttrain loss: 0.13049102939255064\n",
            "--------------------------------------------------\n",
            "iter: 2507 \ttrain loss: 0.1442588155876527\n",
            "--------------------------------------------------\n",
            "iter: 2508 \ttrain loss: 0.12113171275236766\n",
            "--------------------------------------------------\n",
            "iter: 2509 \ttrain loss: 0.1192477553711993\n",
            "--------------------------------------------------\n",
            "iter: 2510 \ttrain loss: 0.12121093425405821\n",
            "--------------------------------------------------\n",
            "iter: 2511 \ttrain loss: 0.12111194188266655\n",
            "--------------------------------------------------\n",
            "iter: 2512 \ttrain loss: 0.134904203924875\n",
            "--------------------------------------------------\n",
            "iter: 2513 \ttrain loss: 0.11053311778175345\n",
            "--------------------------------------------------\n",
            "iter: 2514 \ttrain loss: 0.12997440010803346\n",
            "--------------------------------------------------\n",
            "iter: 2515 \ttrain loss: 0.12159755574500787\n",
            "--------------------------------------------------\n",
            "iter: 2516 \ttrain loss: 0.11582678828360639\n",
            "--------------------------------------------------\n",
            "iter: 2517 \ttrain loss: 0.1307155420711347\n",
            "--------------------------------------------------\n",
            "iter: 2518 \ttrain loss: 0.11465052041563767\n",
            "--------------------------------------------------\n",
            "iter: 2519 \ttrain loss: 0.12430770772995761\n",
            "--------------------------------------------------\n",
            "iter: 2520 \ttrain loss: 0.11939531484248196\n",
            "--------------------------------------------------\n",
            "iter: 2521 \ttrain loss: 0.11691629415505776\n",
            "--------------------------------------------------\n",
            "iter: 2522 \ttrain loss: 0.1231484442580868\n",
            "--------------------------------------------------\n",
            "iter: 2523 \ttrain loss: 0.12094827919267416\n",
            "--------------------------------------------------\n",
            "iter: 2524 \ttrain loss: 0.12666081028207285\n",
            "--------------------------------------------------\n",
            "iter: 2525 \ttrain loss: 0.12423957851960522\n",
            "--------------------------------------------------\n",
            "iter: 2526 \ttrain loss: 0.11898542324517349\n",
            "--------------------------------------------------\n",
            "iter: 2527 \ttrain loss: 0.11934058345693468\n",
            "--------------------------------------------------\n",
            "iter: 2528 \ttrain loss: 0.12176489557693237\n",
            "--------------------------------------------------\n",
            "iter: 2529 \ttrain loss: 0.11596802194816393\n",
            "--------------------------------------------------\n",
            "iter: 2530 \ttrain loss: 0.13057630905240383\n",
            "--------------------------------------------------\n",
            "iter: 2531 \ttrain loss: 0.12219595164883691\n",
            "--------------------------------------------------\n",
            "iter: 2532 \ttrain loss: 0.1208562354146527\n",
            "--------------------------------------------------\n",
            "iter: 2533 \ttrain loss: 0.11907221912091243\n",
            "--------------------------------------------------\n",
            "iter: 2534 \ttrain loss: 0.1244085216535292\n",
            "--------------------------------------------------\n",
            "iter: 2535 \ttrain loss: 0.12190142479376323\n",
            "--------------------------------------------------\n",
            "iter: 2536 \ttrain loss: 0.12090661941466684\n",
            "--------------------------------------------------\n",
            "iter: 2537 \ttrain loss: 0.12466121187422995\n",
            "--------------------------------------------------\n",
            "iter: 2538 \ttrain loss: 0.13412318579373517\n",
            "--------------------------------------------------\n",
            "iter: 2539 \ttrain loss: 0.11707353732271895\n",
            "--------------------------------------------------\n",
            "iter: 2540 \ttrain loss: 0.128677021779312\n",
            "--------------------------------------------------\n",
            "iter: 2541 \ttrain loss: 0.11073982443243592\n",
            "--------------------------------------------------\n",
            "iter: 2542 \ttrain loss: 0.12022218967815648\n",
            "--------------------------------------------------\n",
            "iter: 2543 \ttrain loss: 0.12914475984477886\n",
            "--------------------------------------------------\n",
            "iter: 2544 \ttrain loss: 0.11385930362953825\n",
            "--------------------------------------------------\n",
            "iter: 2545 \ttrain loss: 0.12941366169015958\n",
            "--------------------------------------------------\n",
            "iter: 2546 \ttrain loss: 0.11327483596989957\n",
            "--------------------------------------------------\n",
            "iter: 2547 \ttrain loss: 0.12138723835345414\n",
            "--------------------------------------------------\n",
            "iter: 2548 \ttrain loss: 0.1242146303922866\n",
            "--------------------------------------------------\n",
            "iter: 2549 \ttrain loss: 0.1278981188065023\n",
            "--------------------------------------------------\n",
            "iter: 2550 \ttrain loss: 0.14082025606932277\n",
            "--------------------------------------------------\n",
            "iter: 2551 \ttrain loss: 0.11920039683831272\n",
            "--------------------------------------------------\n",
            "iter: 2552 \ttrain loss: 0.11675980520039551\n",
            "--------------------------------------------------\n",
            "iter: 2553 \ttrain loss: 0.12090432331537992\n",
            "--------------------------------------------------\n",
            "iter: 2554 \ttrain loss: 0.12293217295621885\n",
            "--------------------------------------------------\n",
            "iter: 2555 \ttrain loss: 0.1222797941269357\n",
            "--------------------------------------------------\n",
            "iter: 2556 \ttrain loss: 0.1228384454520067\n",
            "--------------------------------------------------\n",
            "iter: 2557 \ttrain loss: 0.1258834643103072\n",
            "--------------------------------------------------\n",
            "iter: 2558 \ttrain loss: 0.13182517627449009\n",
            "--------------------------------------------------\n",
            "iter: 2559 \ttrain loss: 0.11722595367399674\n",
            "--------------------------------------------------\n",
            "iter: 2560 \ttrain loss: 0.12946758834347424\n",
            "--------------------------------------------------\n",
            "iter: 2561 \ttrain loss: 0.11266367620938192\n",
            "--------------------------------------------------\n",
            "iter: 2562 \ttrain loss: 0.12086513401158902\n",
            "--------------------------------------------------\n",
            "iter: 2563 \ttrain loss: 0.13308698448316283\n",
            "--------------------------------------------------\n",
            "iter: 2564 \ttrain loss: 0.12924889735493245\n",
            "--------------------------------------------------\n",
            "iter: 2565 \ttrain loss: 0.1274693436055602\n",
            "--------------------------------------------------\n",
            "iter: 2566 \ttrain loss: 0.12055618736236683\n",
            "--------------------------------------------------\n",
            "iter: 2567 \ttrain loss: 0.10869802002358928\n",
            "--------------------------------------------------\n",
            "iter: 2568 \ttrain loss: 0.13071760500556356\n",
            "--------------------------------------------------\n",
            "iter: 2569 \ttrain loss: 0.1307302723090277\n",
            "--------------------------------------------------\n",
            "iter: 2570 \ttrain loss: 0.12218954870720329\n",
            "--------------------------------------------------\n",
            "iter: 2571 \ttrain loss: 0.12312352804348703\n",
            "--------------------------------------------------\n",
            "iter: 2572 \ttrain loss: 0.12000055227772194\n",
            "--------------------------------------------------\n",
            "iter: 2573 \ttrain loss: 0.12170922294338732\n",
            "--------------------------------------------------\n",
            "iter: 2574 \ttrain loss: 0.12447614783186838\n",
            "--------------------------------------------------\n",
            "iter: 2575 \ttrain loss: 0.11358527937623969\n",
            "--------------------------------------------------\n",
            "iter: 2576 \ttrain loss: 0.11819458373365829\n",
            "--------------------------------------------------\n",
            "iter: 2577 \ttrain loss: 0.12149351951521581\n",
            "--------------------------------------------------\n",
            "iter: 2578 \ttrain loss: 0.11818443830896018\n",
            "--------------------------------------------------\n",
            "iter: 2579 \ttrain loss: 0.13473317122008047\n",
            "--------------------------------------------------\n",
            "iter: 2580 \ttrain loss: 0.12533034822626885\n",
            "--------------------------------------------------\n",
            "iter: 2581 \ttrain loss: 0.12612218142196804\n",
            "--------------------------------------------------\n",
            "iter: 2582 \ttrain loss: 0.1291532060561469\n",
            "--------------------------------------------------\n",
            "iter: 2583 \ttrain loss: 0.11906001465666702\n",
            "--------------------------------------------------\n",
            "iter: 2584 \ttrain loss: 0.1273479288397292\n",
            "--------------------------------------------------\n",
            "iter: 2585 \ttrain loss: 0.12421810821511005\n",
            "--------------------------------------------------\n",
            "iter: 2586 \ttrain loss: 0.1195885451378022\n",
            "--------------------------------------------------\n",
            "iter: 2587 \ttrain loss: 0.1290567611200684\n",
            "--------------------------------------------------\n",
            "iter: 2588 \ttrain loss: 0.10627143642325713\n",
            "--------------------------------------------------\n",
            "iter: 2589 \ttrain loss: 0.13031455542587306\n",
            "--------------------------------------------------\n",
            "iter: 2590 \ttrain loss: 0.11855868887205694\n",
            "--------------------------------------------------\n",
            "iter: 2591 \ttrain loss: 0.13166565965454957\n",
            "--------------------------------------------------\n",
            "iter: 2592 \ttrain loss: 0.10909881685264665\n",
            "--------------------------------------------------\n",
            "iter: 2593 \ttrain loss: 0.12391282565034144\n",
            "--------------------------------------------------\n",
            "iter: 2594 \ttrain loss: 0.13910809184445483\n",
            "--------------------------------------------------\n",
            "iter: 2595 \ttrain loss: 0.1256428590581662\n",
            "--------------------------------------------------\n",
            "iter: 2596 \ttrain loss: 0.11365362386864676\n",
            "--------------------------------------------------\n",
            "iter: 2597 \ttrain loss: 0.12971326116981233\n",
            "--------------------------------------------------\n",
            "iter: 2598 \ttrain loss: 0.1351068732973402\n",
            "--------------------------------------------------\n",
            "iter: 2599 \ttrain loss: 0.11901807105155272\n",
            "--------------------------------------------------\n",
            "iter: 2600 \ttrain loss: 0.12148468866999304\n",
            "--------------------------------------------------\n",
            "iter: 2601 \ttrain loss: 0.12178467162410532\n",
            "--------------------------------------------------\n",
            "iter: 2602 \ttrain loss: 0.12175851419187951\n",
            "--------------------------------------------------\n",
            "iter: 2603 \ttrain loss: 0.11220271359478697\n",
            "--------------------------------------------------\n",
            "iter: 2604 \ttrain loss: 0.12462274861257065\n",
            "--------------------------------------------------\n",
            "iter: 2605 \ttrain loss: 0.12059832430670242\n",
            "--------------------------------------------------\n",
            "iter: 2606 \ttrain loss: 0.12353012767880035\n",
            "--------------------------------------------------\n",
            "iter: 2607 \ttrain loss: 0.12118493471047076\n",
            "--------------------------------------------------\n",
            "iter: 2608 \ttrain loss: 0.14073877745656377\n",
            "--------------------------------------------------\n",
            "iter: 2609 \ttrain loss: 0.1080826976988832\n",
            "--------------------------------------------------\n",
            "iter: 2610 \ttrain loss: 0.12782267413467377\n",
            "--------------------------------------------------\n",
            "iter: 2611 \ttrain loss: 0.11773146425840954\n",
            "--------------------------------------------------\n",
            "iter: 2612 \ttrain loss: 0.11963907893999108\n",
            "--------------------------------------------------\n",
            "iter: 2613 \ttrain loss: 0.11352985707674033\n",
            "--------------------------------------------------\n",
            "\n",
            "Pre-training SOM\n",
            "==================================================\n",
            "iter: 1 \ttrain loss: 0.03408295470325012\n",
            "--------------------------------------------------\n",
            "iter: 2 \ttrain loss: 0.03263094593874939\n",
            "--------------------------------------------------\n",
            "iter: 3 \ttrain loss: 0.03118665969257497\n",
            "--------------------------------------------------\n",
            "iter: 4 \ttrain loss: 0.030567798785983983\n",
            "--------------------------------------------------\n",
            "iter: 5 \ttrain loss: 0.033514143932008716\n",
            "--------------------------------------------------\n",
            "iter: 6 \ttrain loss: 0.031153386922246208\n",
            "--------------------------------------------------\n",
            "iter: 7 \ttrain loss: 0.030433077221962403\n",
            "--------------------------------------------------\n",
            "iter: 8 \ttrain loss: 0.030175919208245514\n",
            "--------------------------------------------------\n",
            "iter: 9 \ttrain loss: 0.030795974821763872\n",
            "--------------------------------------------------\n",
            "iter: 10 \ttrain loss: 0.028889787318802037\n",
            "--------------------------------------------------\n",
            "iter: 11 \ttrain loss: 0.030845109817054652\n",
            "--------------------------------------------------\n",
            "iter: 12 \ttrain loss: 0.0337124437831462\n",
            "--------------------------------------------------\n",
            "iter: 13 \ttrain loss: 0.0313286591202247\n",
            "--------------------------------------------------\n",
            "iter: 14 \ttrain loss: 0.030833627716973375\n",
            "--------------------------------------------------\n",
            "iter: 15 \ttrain loss: 0.027915980597769997\n",
            "--------------------------------------------------\n",
            "iter: 16 \ttrain loss: 0.02994040925041692\n",
            "--------------------------------------------------\n",
            "iter: 17 \ttrain loss: 0.0323954512483179\n",
            "--------------------------------------------------\n",
            "iter: 18 \ttrain loss: 0.030726889118606902\n",
            "--------------------------------------------------\n",
            "iter: 19 \ttrain loss: 0.028118834190670527\n",
            "--------------------------------------------------\n",
            "iter: 20 \ttrain loss: 0.03047062160479097\n",
            "--------------------------------------------------\n",
            "iter: 21 \ttrain loss: 0.029938444819702623\n",
            "--------------------------------------------------\n",
            "iter: 22 \ttrain loss: 0.030303863165885545\n",
            "--------------------------------------------------\n",
            "iter: 23 \ttrain loss: 0.02846349358262362\n",
            "--------------------------------------------------\n",
            "iter: 24 \ttrain loss: 0.029566604256554208\n",
            "--------------------------------------------------\n",
            "iter: 25 \ttrain loss: 0.029581873806466\n",
            "--------------------------------------------------\n",
            "iter: 26 \ttrain loss: 0.028074792811421667\n",
            "--------------------------------------------------\n",
            "iter: 27 \ttrain loss: 0.02959984140490182\n",
            "--------------------------------------------------\n",
            "iter: 28 \ttrain loss: 0.0266642823927466\n",
            "--------------------------------------------------\n",
            "iter: 29 \ttrain loss: 0.030708435895472206\n",
            "--------------------------------------------------\n",
            "iter: 30 \ttrain loss: 0.026744828637446826\n",
            "--------------------------------------------------\n",
            "iter: 31 \ttrain loss: 0.02956878106709392\n",
            "--------------------------------------------------\n",
            "iter: 32 \ttrain loss: 0.028070583112184334\n",
            "--------------------------------------------------\n",
            "iter: 33 \ttrain loss: 0.02856389805632319\n",
            "--------------------------------------------------\n",
            "iter: 34 \ttrain loss: 0.02986280758512575\n",
            "--------------------------------------------------\n",
            "iter: 35 \ttrain loss: 0.028131457001133706\n",
            "--------------------------------------------------\n",
            "iter: 36 \ttrain loss: 0.02765519046373326\n",
            "--------------------------------------------------\n",
            "iter: 37 \ttrain loss: 0.027986769506207196\n",
            "--------------------------------------------------\n",
            "iter: 38 \ttrain loss: 0.026838749606937537\n",
            "--------------------------------------------------\n",
            "iter: 39 \ttrain loss: 0.024850857794418202\n",
            "--------------------------------------------------\n",
            "iter: 40 \ttrain loss: 0.030516944658197254\n",
            "--------------------------------------------------\n",
            "iter: 41 \ttrain loss: 0.029159632789002563\n",
            "--------------------------------------------------\n",
            "iter: 42 \ttrain loss: 0.02978821967757248\n",
            "--------------------------------------------------\n",
            "iter: 43 \ttrain loss: 0.029038090967390205\n",
            "--------------------------------------------------\n",
            "iter: 44 \ttrain loss: 0.02531733625730862\n",
            "--------------------------------------------------\n",
            "iter: 45 \ttrain loss: 0.028908474733293343\n",
            "--------------------------------------------------\n",
            "iter: 46 \ttrain loss: 0.02659113070027471\n",
            "--------------------------------------------------\n",
            "iter: 47 \ttrain loss: 0.02834405338938562\n",
            "--------------------------------------------------\n",
            "iter: 48 \ttrain loss: 0.02692258216121697\n",
            "--------------------------------------------------\n",
            "iter: 49 \ttrain loss: 0.02567474409198215\n",
            "--------------------------------------------------\n",
            "iter: 50 \ttrain loss: 0.025133208207351657\n",
            "--------------------------------------------------\n",
            "iter: 51 \ttrain loss: 0.026289239642963057\n",
            "--------------------------------------------------\n",
            "iter: 52 \ttrain loss: 0.025739300357287345\n",
            "--------------------------------------------------\n",
            "iter: 53 \ttrain loss: 0.02522099923563944\n",
            "--------------------------------------------------\n",
            "iter: 54 \ttrain loss: 0.025732979330427022\n",
            "--------------------------------------------------\n",
            "iter: 55 \ttrain loss: 0.02637608555843293\n",
            "--------------------------------------------------\n",
            "iter: 56 \ttrain loss: 0.0249719752556231\n",
            "--------------------------------------------------\n",
            "iter: 57 \ttrain loss: 0.02677779665830703\n",
            "--------------------------------------------------\n",
            "iter: 58 \ttrain loss: 0.023090299346014975\n",
            "--------------------------------------------------\n",
            "iter: 59 \ttrain loss: 0.02756855440242254\n",
            "--------------------------------------------------\n",
            "iter: 60 \ttrain loss: 0.02440626465981716\n",
            "--------------------------------------------------\n",
            "iter: 61 \ttrain loss: 0.02534374527768524\n",
            "--------------------------------------------------\n",
            "iter: 62 \ttrain loss: 0.02664285679685411\n",
            "--------------------------------------------------\n",
            "iter: 63 \ttrain loss: 0.02262528853318341\n",
            "--------------------------------------------------\n",
            "iter: 64 \ttrain loss: 0.02591427993588877\n",
            "--------------------------------------------------\n",
            "iter: 65 \ttrain loss: 0.02373157067855608\n",
            "--------------------------------------------------\n",
            "iter: 66 \ttrain loss: 0.027320776885165866\n",
            "--------------------------------------------------\n",
            "iter: 67 \ttrain loss: 0.02523239583983161\n",
            "--------------------------------------------------\n",
            "iter: 68 \ttrain loss: 0.02333005607696738\n",
            "--------------------------------------------------\n",
            "iter: 69 \ttrain loss: 0.025000878508679856\n",
            "--------------------------------------------------\n",
            "iter: 70 \ttrain loss: 0.02557754673399423\n",
            "--------------------------------------------------\n",
            "iter: 71 \ttrain loss: 0.024463301549854436\n",
            "--------------------------------------------------\n",
            "iter: 72 \ttrain loss: 0.02492172230369317\n",
            "--------------------------------------------------\n",
            "iter: 73 \ttrain loss: 0.024381504275755885\n",
            "--------------------------------------------------\n",
            "iter: 74 \ttrain loss: 0.02564542562871881\n",
            "--------------------------------------------------\n",
            "iter: 75 \ttrain loss: 0.02389784748038506\n",
            "--------------------------------------------------\n",
            "iter: 76 \ttrain loss: 0.024432000605312258\n",
            "--------------------------------------------------\n",
            "iter: 77 \ttrain loss: 0.023952933231557904\n",
            "--------------------------------------------------\n",
            "iter: 78 \ttrain loss: 0.022077760502550597\n",
            "--------------------------------------------------\n",
            "iter: 79 \ttrain loss: 0.02668477165946331\n",
            "--------------------------------------------------\n",
            "iter: 80 \ttrain loss: 0.024983981402158986\n",
            "--------------------------------------------------\n",
            "iter: 81 \ttrain loss: 0.024199917691612984\n",
            "--------------------------------------------------\n",
            "iter: 82 \ttrain loss: 0.02409061329349461\n",
            "--------------------------------------------------\n",
            "iter: 83 \ttrain loss: 0.023608799213283713\n",
            "--------------------------------------------------\n",
            "iter: 84 \ttrain loss: 0.024889733034557382\n",
            "--------------------------------------------------\n",
            "iter: 85 \ttrain loss: 0.022777632961935353\n",
            "--------------------------------------------------\n",
            "iter: 86 \ttrain loss: 0.02458688336423145\n",
            "--------------------------------------------------\n",
            "iter: 87 \ttrain loss: 0.0241333022461965\n",
            "--------------------------------------------------\n",
            "iter: 88 \ttrain loss: 0.022643010015090184\n",
            "--------------------------------------------------\n",
            "iter: 89 \ttrain loss: 0.02303372997234827\n",
            "--------------------------------------------------\n",
            "iter: 90 \ttrain loss: 0.022276572050914874\n",
            "--------------------------------------------------\n",
            "iter: 91 \ttrain loss: 0.022680227094342933\n",
            "--------------------------------------------------\n",
            "iter: 92 \ttrain loss: 0.023405488309852892\n",
            "--------------------------------------------------\n",
            "iter: 93 \ttrain loss: 0.02300752229072236\n",
            "--------------------------------------------------\n",
            "iter: 94 \ttrain loss: 0.022775241487220533\n",
            "--------------------------------------------------\n",
            "iter: 95 \ttrain loss: 0.023386294771378248\n",
            "--------------------------------------------------\n",
            "iter: 96 \ttrain loss: 0.023323193478180067\n",
            "--------------------------------------------------\n",
            "iter: 97 \ttrain loss: 0.021968212110254935\n",
            "--------------------------------------------------\n",
            "iter: 98 \ttrain loss: 0.02059556033955905\n",
            "--------------------------------------------------\n",
            "iter: 99 \ttrain loss: 0.023413315995692595\n",
            "--------------------------------------------------\n",
            "iter: 100 \ttrain loss: 0.022827590151598742\n",
            "--------------------------------------------------\n",
            "iter: 101 \ttrain loss: 0.02015685769344651\n",
            "--------------------------------------------------\n",
            "iter: 102 \ttrain loss: 0.027102936966952592\n",
            "--------------------------------------------------\n",
            "iter: 103 \ttrain loss: 0.020911648358441788\n",
            "--------------------------------------------------\n",
            "iter: 104 \ttrain loss: 0.020851781645285165\n",
            "--------------------------------------------------\n",
            "iter: 105 \ttrain loss: 0.02078614885418592\n",
            "--------------------------------------------------\n",
            "iter: 106 \ttrain loss: 0.025849176713532183\n",
            "--------------------------------------------------\n",
            "iter: 107 \ttrain loss: 0.02081952659267943\n",
            "--------------------------------------------------\n",
            "iter: 108 \ttrain loss: 0.021249735217179842\n",
            "--------------------------------------------------\n",
            "iter: 109 \ttrain loss: 0.02064536912701491\n",
            "--------------------------------------------------\n",
            "iter: 110 \ttrain loss: 0.02268721759412616\n",
            "--------------------------------------------------\n",
            "iter: 111 \ttrain loss: 0.022673288891471223\n",
            "--------------------------------------------------\n",
            "iter: 112 \ttrain loss: 0.0207179963536368\n",
            "--------------------------------------------------\n",
            "iter: 113 \ttrain loss: 0.020933369523380988\n",
            "--------------------------------------------------\n",
            "iter: 114 \ttrain loss: 0.02203103576280491\n",
            "--------------------------------------------------\n",
            "iter: 115 \ttrain loss: 0.020356099271751627\n",
            "--------------------------------------------------\n",
            "iter: 116 \ttrain loss: 0.020927868905882056\n",
            "--------------------------------------------------\n",
            "iter: 117 \ttrain loss: 0.020410414109877473\n",
            "--------------------------------------------------\n",
            "iter: 118 \ttrain loss: 0.018690524525492417\n",
            "--------------------------------------------------\n",
            "iter: 119 \ttrain loss: 0.02488945464646772\n",
            "--------------------------------------------------\n",
            "iter: 120 \ttrain loss: 0.019666564506479245\n",
            "--------------------------------------------------\n",
            "iter: 121 \ttrain loss: 0.020676902116624814\n",
            "--------------------------------------------------\n",
            "iter: 122 \ttrain loss: 0.01929020300601916\n",
            "--------------------------------------------------\n",
            "iter: 123 \ttrain loss: 0.020396931133683124\n",
            "--------------------------------------------------\n",
            "iter: 124 \ttrain loss: 0.020029682180627787\n",
            "--------------------------------------------------\n",
            "iter: 125 \ttrain loss: 0.02054188576214934\n",
            "--------------------------------------------------\n",
            "iter: 126 \ttrain loss: 0.019596014949783344\n",
            "--------------------------------------------------\n",
            "iter: 127 \ttrain loss: 0.01879834918222603\n",
            "--------------------------------------------------\n",
            "iter: 128 \ttrain loss: 0.020135722386509354\n",
            "--------------------------------------------------\n",
            "iter: 129 \ttrain loss: 0.01889340610634407\n",
            "--------------------------------------------------\n",
            "iter: 130 \ttrain loss: 0.017006322828604585\n",
            "--------------------------------------------------\n",
            "iter: 131 \ttrain loss: 0.021136000536768198\n",
            "--------------------------------------------------\n",
            "iter: 132 \ttrain loss: 0.019566302941505284\n",
            "--------------------------------------------------\n",
            "iter: 133 \ttrain loss: 0.021116541638853757\n",
            "--------------------------------------------------\n",
            "iter: 134 \ttrain loss: 0.017341725884607515\n",
            "--------------------------------------------------\n",
            "iter: 135 \ttrain loss: 0.020886452923113925\n",
            "--------------------------------------------------\n",
            "iter: 136 \ttrain loss: 0.018460531085400182\n",
            "--------------------------------------------------\n",
            "iter: 137 \ttrain loss: 0.018049400494566593\n",
            "--------------------------------------------------\n",
            "iter: 138 \ttrain loss: 0.01869062850125915\n",
            "--------------------------------------------------\n",
            "iter: 139 \ttrain loss: 0.01924925450607815\n",
            "--------------------------------------------------\n",
            "iter: 140 \ttrain loss: 0.018698350814198086\n",
            "--------------------------------------------------\n",
            "iter: 141 \ttrain loss: 0.019657546870547844\n",
            "--------------------------------------------------\n",
            "iter: 142 \ttrain loss: 0.017286647852007703\n",
            "--------------------------------------------------\n",
            "iter: 143 \ttrain loss: 0.02259173579790632\n",
            "--------------------------------------------------\n",
            "iter: 144 \ttrain loss: 0.017658665791190426\n",
            "--------------------------------------------------\n",
            "iter: 145 \ttrain loss: 0.019458811846323507\n",
            "--------------------------------------------------\n",
            "iter: 146 \ttrain loss: 0.017617837708903367\n",
            "--------------------------------------------------\n",
            "iter: 147 \ttrain loss: 0.019148336060398417\n",
            "--------------------------------------------------\n",
            "iter: 148 \ttrain loss: 0.01824609420913795\n",
            "--------------------------------------------------\n",
            "iter: 149 \ttrain loss: 0.018598745805042474\n",
            "--------------------------------------------------\n",
            "iter: 150 \ttrain loss: 0.019031702269001107\n",
            "--------------------------------------------------\n",
            "iter: 151 \ttrain loss: 0.017553116235360336\n",
            "--------------------------------------------------\n",
            "iter: 152 \ttrain loss: 0.015771906880882605\n",
            "--------------------------------------------------\n",
            "iter: 153 \ttrain loss: 0.018014990369766896\n",
            "--------------------------------------------------\n",
            "iter: 154 \ttrain loss: 0.017743866333489653\n",
            "--------------------------------------------------\n",
            "iter: 155 \ttrain loss: 0.01762091673649074\n",
            "--------------------------------------------------\n",
            "iter: 156 \ttrain loss: 0.01722033732028386\n",
            "--------------------------------------------------\n",
            "iter: 157 \ttrain loss: 0.016725496427191355\n",
            "--------------------------------------------------\n",
            "iter: 158 \ttrain loss: 0.01840035496746751\n",
            "--------------------------------------------------\n",
            "iter: 159 \ttrain loss: 0.016874715636207047\n",
            "--------------------------------------------------\n",
            "iter: 160 \ttrain loss: 0.016700290548959305\n",
            "--------------------------------------------------\n",
            "iter: 161 \ttrain loss: 0.017990628918378992\n",
            "--------------------------------------------------\n",
            "iter: 162 \ttrain loss: 0.017109125016320496\n",
            "--------------------------------------------------\n",
            "iter: 163 \ttrain loss: 0.016914080846597405\n",
            "--------------------------------------------------\n",
            "iter: 164 \ttrain loss: 0.016738740033667123\n",
            "--------------------------------------------------\n",
            "iter: 165 \ttrain loss: 0.018358521685098328\n",
            "--------------------------------------------------\n",
            "iter: 166 \ttrain loss: 0.016433760618370433\n",
            "--------------------------------------------------\n",
            "iter: 167 \ttrain loss: 0.01650481197195592\n",
            "--------------------------------------------------\n",
            "iter: 168 \ttrain loss: 0.016674223402671082\n",
            "--------------------------------------------------\n",
            "iter: 169 \ttrain loss: 0.01669666729407092\n",
            "--------------------------------------------------\n",
            "iter: 170 \ttrain loss: 0.017565587313634796\n",
            "--------------------------------------------------\n",
            "iter: 171 \ttrain loss: 0.015137539232460105\n",
            "--------------------------------------------------\n",
            "iter: 172 \ttrain loss: 0.016959778177226045\n",
            "--------------------------------------------------\n",
            "iter: 173 \ttrain loss: 0.01552498272690408\n",
            "--------------------------------------------------\n",
            "iter: 174 \ttrain loss: 0.01798127922818857\n",
            "--------------------------------------------------\n",
            "iter: 175 \ttrain loss: 0.014980792544701265\n",
            "--------------------------------------------------\n",
            "iter: 176 \ttrain loss: 0.01667402606829073\n",
            "--------------------------------------------------\n",
            "iter: 177 \ttrain loss: 0.016830461687251698\n",
            "--------------------------------------------------\n",
            "iter: 178 \ttrain loss: 0.016390842411713544\n",
            "--------------------------------------------------\n",
            "iter: 179 \ttrain loss: 0.01601553583065297\n",
            "--------------------------------------------------\n",
            "iter: 180 \ttrain loss: 0.015250301273955458\n",
            "--------------------------------------------------\n",
            "iter: 181 \ttrain loss: 0.015498412099491743\n",
            "--------------------------------------------------\n",
            "iter: 182 \ttrain loss: 0.015441109732832898\n",
            "--------------------------------------------------\n",
            "iter: 183 \ttrain loss: 0.016683151443329727\n",
            "--------------------------------------------------\n",
            "iter: 184 \ttrain loss: 0.015199687507784894\n",
            "--------------------------------------------------\n",
            "iter: 185 \ttrain loss: 0.015466798640239017\n",
            "--------------------------------------------------\n",
            "iter: 186 \ttrain loss: 0.015543270760435442\n",
            "--------------------------------------------------\n",
            "iter: 187 \ttrain loss: 0.014403815405736554\n",
            "--------------------------------------------------\n",
            "iter: 188 \ttrain loss: 0.014826817138547516\n",
            "--------------------------------------------------\n",
            "iter: 189 \ttrain loss: 0.016265366855318375\n",
            "--------------------------------------------------\n",
            "iter: 190 \ttrain loss: 0.01456720246654001\n",
            "--------------------------------------------------\n",
            "iter: 191 \ttrain loss: 0.016494309680868655\n",
            "--------------------------------------------------\n",
            "iter: 192 \ttrain loss: 0.01324684784950561\n",
            "--------------------------------------------------\n",
            "iter: 193 \ttrain loss: 0.015784254384612253\n",
            "--------------------------------------------------\n",
            "iter: 194 \ttrain loss: 0.014193831504324746\n",
            "--------------------------------------------------\n",
            "iter: 195 \ttrain loss: 0.01688452588181257\n",
            "--------------------------------------------------\n",
            "iter: 196 \ttrain loss: 0.01400844085470216\n",
            "--------------------------------------------------\n",
            "iter: 197 \ttrain loss: 0.015414377338944059\n",
            "--------------------------------------------------\n",
            "iter: 198 \ttrain loss: 0.013927007540595775\n",
            "--------------------------------------------------\n",
            "iter: 199 \ttrain loss: 0.015491070863302574\n",
            "--------------------------------------------------\n",
            "iter: 200 \ttrain loss: 0.013879087441161208\n",
            "--------------------------------------------------\n",
            "iter: 201 \ttrain loss: 0.014252801532924379\n",
            "--------------------------------------------------\n",
            "iter: 202 \ttrain loss: 0.013975890402287255\n",
            "--------------------------------------------------\n",
            "iter: 203 \ttrain loss: 0.014098722529976434\n",
            "--------------------------------------------------\n",
            "iter: 204 \ttrain loss: 0.014167124310380424\n",
            "--------------------------------------------------\n",
            "iter: 205 \ttrain loss: 0.01458210549513969\n",
            "--------------------------------------------------\n",
            "iter: 206 \ttrain loss: 0.016919275966055337\n",
            "--------------------------------------------------\n",
            "iter: 207 \ttrain loss: 0.013351721883858974\n",
            "--------------------------------------------------\n",
            "iter: 208 \ttrain loss: 0.013982821832185287\n",
            "--------------------------------------------------\n",
            "iter: 209 \ttrain loss: 0.014018673391640424\n",
            "--------------------------------------------------\n",
            "iter: 210 \ttrain loss: 0.013294865213405888\n",
            "--------------------------------------------------\n",
            "iter: 211 \ttrain loss: 0.014957010929483589\n",
            "--------------------------------------------------\n",
            "iter: 212 \ttrain loss: 0.016228453834484277\n",
            "--------------------------------------------------\n",
            "iter: 213 \ttrain loss: 0.0158791030735867\n",
            "--------------------------------------------------\n",
            "iter: 214 \ttrain loss: 0.012549836691961297\n",
            "--------------------------------------------------\n",
            "iter: 215 \ttrain loss: 0.01412854039111542\n",
            "--------------------------------------------------\n",
            "iter: 216 \ttrain loss: 0.013919418228243597\n",
            "--------------------------------------------------\n",
            "iter: 217 \ttrain loss: 0.0111182128783751\n",
            "--------------------------------------------------\n",
            "iter: 218 \ttrain loss: 0.01565094519274539\n",
            "--------------------------------------------------\n",
            "iter: 219 \ttrain loss: 0.013260591947300705\n",
            "--------------------------------------------------\n",
            "iter: 220 \ttrain loss: 0.013248926433758391\n",
            "--------------------------------------------------\n",
            "iter: 221 \ttrain loss: 0.013480462888109225\n",
            "--------------------------------------------------\n",
            "iter: 222 \ttrain loss: 0.012192773503980053\n",
            "--------------------------------------------------\n",
            "iter: 223 \ttrain loss: 0.013555712958354534\n",
            "--------------------------------------------------\n",
            "iter: 224 \ttrain loss: 0.01289434036688038\n",
            "--------------------------------------------------\n",
            "iter: 225 \ttrain loss: 0.013316346052267157\n",
            "--------------------------------------------------\n",
            "iter: 226 \ttrain loss: 0.01222315340687277\n",
            "--------------------------------------------------\n",
            "iter: 227 \ttrain loss: 0.013197398983088228\n",
            "--------------------------------------------------\n",
            "iter: 228 \ttrain loss: 0.012126647304617499\n",
            "--------------------------------------------------\n",
            "iter: 229 \ttrain loss: 0.013475635002307642\n",
            "--------------------------------------------------\n",
            "iter: 230 \ttrain loss: 0.012796473470823662\n",
            "--------------------------------------------------\n",
            "iter: 231 \ttrain loss: 0.01303252097059951\n",
            "--------------------------------------------------\n",
            "iter: 232 \ttrain loss: 0.012280965169129\n",
            "--------------------------------------------------\n",
            "iter: 233 \ttrain loss: 0.01254394702291816\n",
            "--------------------------------------------------\n",
            "iter: 234 \ttrain loss: 0.01198952416527188\n",
            "--------------------------------------------------\n",
            "iter: 235 \ttrain loss: 0.012865451582371974\n",
            "--------------------------------------------------\n",
            "iter: 236 \ttrain loss: 0.010833684640421972\n",
            "--------------------------------------------------\n",
            "iter: 237 \ttrain loss: 0.013821508891968148\n",
            "--------------------------------------------------\n",
            "iter: 238 \ttrain loss: 0.011314643473866003\n",
            "--------------------------------------------------\n",
            "iter: 239 \ttrain loss: 0.01232594732453318\n",
            "--------------------------------------------------\n",
            "iter: 240 \ttrain loss: 0.012316603412750952\n",
            "--------------------------------------------------\n",
            "iter: 241 \ttrain loss: 0.011781195470406748\n",
            "--------------------------------------------------\n",
            "iter: 242 \ttrain loss: 0.012249817050948665\n",
            "--------------------------------------------------\n",
            "iter: 243 \ttrain loss: 0.01279229168478489\n",
            "--------------------------------------------------\n",
            "iter: 244 \ttrain loss: 0.009998716421933427\n",
            "--------------------------------------------------\n",
            "iter: 245 \ttrain loss: 0.012844389464030744\n",
            "--------------------------------------------------\n",
            "iter: 246 \ttrain loss: 0.01229497336779294\n",
            "--------------------------------------------------\n",
            "iter: 247 \ttrain loss: 0.011570904414079331\n",
            "--------------------------------------------------\n",
            "iter: 248 \ttrain loss: 0.010769003064170786\n",
            "--------------------------------------------------\n",
            "iter: 249 \ttrain loss: 0.01145199452197293\n",
            "--------------------------------------------------\n",
            "iter: 250 \ttrain loss: 0.011091663734134623\n",
            "--------------------------------------------------\n",
            "iter: 251 \ttrain loss: 0.010862664357829094\n",
            "--------------------------------------------------\n",
            "iter: 252 \ttrain loss: 0.012012546529965184\n",
            "--------------------------------------------------\n",
            "iter: 253 \ttrain loss: 0.010856136817560714\n",
            "--------------------------------------------------\n",
            "iter: 254 \ttrain loss: 0.011437719376254518\n",
            "--------------------------------------------------\n",
            "iter: 255 \ttrain loss: 0.011122406315411941\n",
            "--------------------------------------------------\n",
            "iter: 256 \ttrain loss: 0.01299119901476291\n",
            "--------------------------------------------------\n",
            "iter: 257 \ttrain loss: 0.011289829060686228\n",
            "--------------------------------------------------\n",
            "iter: 258 \ttrain loss: 0.010717417905063758\n",
            "--------------------------------------------------\n",
            "iter: 259 \ttrain loss: 0.010283459457188263\n",
            "--------------------------------------------------\n",
            "iter: 260 \ttrain loss: 0.01149250302714519\n",
            "--------------------------------------------------\n",
            "iter: 261 \ttrain loss: 0.010840111610823218\n",
            "--------------------------------------------------\n",
            "iter: 262 \ttrain loss: 0.010857289293038589\n",
            "--------------------------------------------------\n",
            "iter: 263 \ttrain loss: 0.011220516728530881\n",
            "--------------------------------------------------\n",
            "iter: 264 \ttrain loss: 0.009733140810963496\n",
            "--------------------------------------------------\n",
            "iter: 265 \ttrain loss: 0.010136928458037521\n",
            "--------------------------------------------------\n",
            "iter: 266 \ttrain loss: 0.011959612358144007\n",
            "--------------------------------------------------\n",
            "iter: 267 \ttrain loss: 0.009640104453071868\n",
            "--------------------------------------------------\n",
            "iter: 268 \ttrain loss: 0.01076429925913065\n",
            "--------------------------------------------------\n",
            "iter: 269 \ttrain loss: 0.010123837099923037\n",
            "--------------------------------------------------\n",
            "iter: 270 \ttrain loss: 0.01045982865668013\n",
            "--------------------------------------------------\n",
            "iter: 271 \ttrain loss: 0.011308095482525803\n",
            "--------------------------------------------------\n",
            "iter: 272 \ttrain loss: 0.009558492956528657\n",
            "--------------------------------------------------\n",
            "iter: 273 \ttrain loss: 0.00995329926784984\n",
            "--------------------------------------------------\n",
            "iter: 274 \ttrain loss: 0.011265524407117013\n",
            "--------------------------------------------------\n",
            "iter: 275 \ttrain loss: 0.00990707617767071\n",
            "--------------------------------------------------\n",
            "iter: 276 \ttrain loss: 0.010422774166471123\n",
            "--------------------------------------------------\n",
            "iter: 277 \ttrain loss: 0.010082636652112493\n",
            "--------------------------------------------------\n",
            "iter: 278 \ttrain loss: 0.010349959892747994\n",
            "--------------------------------------------------\n",
            "iter: 279 \ttrain loss: 0.009343299057376244\n",
            "--------------------------------------------------\n",
            "iter: 280 \ttrain loss: 0.010314653963982678\n",
            "--------------------------------------------------\n",
            "iter: 281 \ttrain loss: 0.009183725497020241\n",
            "--------------------------------------------------\n",
            "iter: 282 \ttrain loss: 0.010522483550995635\n",
            "--------------------------------------------------\n",
            "iter: 283 \ttrain loss: 0.010878286376384038\n",
            "--------------------------------------------------\n",
            "iter: 284 \ttrain loss: 0.008933168662288077\n",
            "--------------------------------------------------\n",
            "iter: 285 \ttrain loss: 0.009751778278477065\n",
            "--------------------------------------------------\n",
            "iter: 286 \ttrain loss: 0.010842469960697244\n",
            "--------------------------------------------------\n",
            "iter: 287 \ttrain loss: 0.009112083685101859\n",
            "--------------------------------------------------\n",
            "iter: 288 \ttrain loss: 0.00876848628945416\n",
            "--------------------------------------------------\n",
            "iter: 289 \ttrain loss: 0.009964301863671685\n",
            "--------------------------------------------------\n",
            "iter: 290 \ttrain loss: 0.009043268246943055\n",
            "--------------------------------------------------\n",
            "iter: 291 \ttrain loss: 0.011441738304923309\n",
            "--------------------------------------------------\n",
            "iter: 292 \ttrain loss: 0.009166421052604888\n",
            "--------------------------------------------------\n",
            "iter: 293 \ttrain loss: 0.007986244248212327\n",
            "--------------------------------------------------\n",
            "iter: 294 \ttrain loss: 0.0103563711718358\n",
            "--------------------------------------------------\n",
            "iter: 295 \ttrain loss: 0.008707068283773343\n",
            "--------------------------------------------------\n",
            "iter: 296 \ttrain loss: 0.009506987051134765\n",
            "--------------------------------------------------\n",
            "iter: 297 \ttrain loss: 0.009020284563771451\n",
            "--------------------------------------------------\n",
            "iter: 298 \ttrain loss: 0.009638859930770953\n",
            "--------------------------------------------------\n",
            "iter: 299 \ttrain loss: 0.008761702337422584\n",
            "--------------------------------------------------\n",
            "iter: 300 \ttrain loss: 0.008856853452833656\n",
            "--------------------------------------------------\n",
            "iter: 301 \ttrain loss: 0.00877565425711493\n",
            "--------------------------------------------------\n",
            "iter: 302 \ttrain loss: 0.009105596610152202\n",
            "--------------------------------------------------\n",
            "iter: 303 \ttrain loss: 0.01070979166322633\n",
            "--------------------------------------------------\n",
            "iter: 304 \ttrain loss: 0.008634343811561857\n",
            "--------------------------------------------------\n",
            "iter: 305 \ttrain loss: 0.008028188577907014\n",
            "--------------------------------------------------\n",
            "iter: 306 \ttrain loss: 0.008761519571735052\n",
            "--------------------------------------------------\n",
            "iter: 307 \ttrain loss: 0.00881255774356378\n",
            "--------------------------------------------------\n",
            "iter: 308 \ttrain loss: 0.008212340025943395\n",
            "--------------------------------------------------\n",
            "iter: 309 \ttrain loss: 0.007830218261728927\n",
            "--------------------------------------------------\n",
            "iter: 310 \ttrain loss: 0.009691415104790586\n",
            "--------------------------------------------------\n",
            "iter: 311 \ttrain loss: 0.009171258454394305\n",
            "--------------------------------------------------\n",
            "iter: 312 \ttrain loss: 0.00859540902181443\n",
            "--------------------------------------------------\n",
            "iter: 313 \ttrain loss: 0.008301932098065805\n",
            "--------------------------------------------------\n",
            "iter: 314 \ttrain loss: 0.008462676400456538\n",
            "--------------------------------------------------\n",
            "iter: 315 \ttrain loss: 0.008987697843902541\n",
            "--------------------------------------------------\n",
            "iter: 316 \ttrain loss: 0.007442780926182827\n",
            "--------------------------------------------------\n",
            "iter: 317 \ttrain loss: 0.008036388081109277\n",
            "--------------------------------------------------\n",
            "iter: 318 \ttrain loss: 0.008342768706369281\n",
            "--------------------------------------------------\n",
            "iter: 319 \ttrain loss: 0.008103973079464415\n",
            "--------------------------------------------------\n",
            "iter: 320 \ttrain loss: 0.00812796767778841\n",
            "--------------------------------------------------\n",
            "iter: 321 \ttrain loss: 0.007716258313803056\n",
            "--------------------------------------------------\n",
            "iter: 322 \ttrain loss: 0.008199126169105547\n",
            "--------------------------------------------------\n",
            "iter: 323 \ttrain loss: 0.008132023237455472\n",
            "--------------------------------------------------\n",
            "iter: 324 \ttrain loss: 0.008075080297883331\n",
            "--------------------------------------------------\n",
            "iter: 325 \ttrain loss: 0.007801991332849447\n",
            "--------------------------------------------------\n",
            "iter: 326 \ttrain loss: 0.008001574239021396\n",
            "--------------------------------------------------\n",
            "iter: 327 \ttrain loss: 0.00828934215718295\n",
            "--------------------------------------------------\n",
            "iter: 328 \ttrain loss: 0.008017226705583948\n",
            "--------------------------------------------------\n",
            "iter: 329 \ttrain loss: 0.007500987248214013\n",
            "--------------------------------------------------\n",
            "iter: 330 \ttrain loss: 0.00853869032176686\n",
            "--------------------------------------------------\n",
            "iter: 331 \ttrain loss: 0.007482895572762038\n",
            "--------------------------------------------------\n",
            "iter: 332 \ttrain loss: 0.009377533743091294\n",
            "--------------------------------------------------\n",
            "iter: 333 \ttrain loss: 0.006921001282985196\n",
            "--------------------------------------------------\n",
            "iter: 334 \ttrain loss: 0.008743752077757782\n",
            "--------------------------------------------------\n",
            "iter: 335 \ttrain loss: 0.007086769284461097\n",
            "--------------------------------------------------\n",
            "iter: 336 \ttrain loss: 0.006807188343732103\n",
            "--------------------------------------------------\n",
            "iter: 337 \ttrain loss: 0.007943362946204811\n",
            "--------------------------------------------------\n",
            "iter: 338 \ttrain loss: 0.007843094643561858\n",
            "--------------------------------------------------\n",
            "iter: 339 \ttrain loss: 0.0076591999898615835\n",
            "--------------------------------------------------\n",
            "iter: 340 \ttrain loss: 0.006132713554293654\n",
            "--------------------------------------------------\n",
            "iter: 341 \ttrain loss: 0.007687171142601567\n",
            "--------------------------------------------------\n",
            "iter: 342 \ttrain loss: 0.0073739510871059875\n",
            "--------------------------------------------------\n",
            "iter: 343 \ttrain loss: 0.007375696750069804\n",
            "--------------------------------------------------\n",
            "iter: 344 \ttrain loss: 0.007258198621853765\n",
            "--------------------------------------------------\n",
            "iter: 345 \ttrain loss: 0.006926959782316389\n",
            "--------------------------------------------------\n",
            "iter: 346 \ttrain loss: 0.007145374099983346\n",
            "--------------------------------------------------\n",
            "iter: 347 \ttrain loss: 0.007004950340135105\n",
            "--------------------------------------------------\n",
            "iter: 348 \ttrain loss: 0.007480905358382287\n",
            "--------------------------------------------------\n",
            "iter: 349 \ttrain loss: 0.0065965419265909325\n",
            "--------------------------------------------------\n",
            "iter: 350 \ttrain loss: 0.007229295829256549\n",
            "--------------------------------------------------\n",
            "iter: 351 \ttrain loss: 0.006926717009885405\n",
            "--------------------------------------------------\n",
            "iter: 352 \ttrain loss: 0.006170387038897982\n",
            "--------------------------------------------------\n",
            "iter: 353 \ttrain loss: 0.006823903097930501\n",
            "--------------------------------------------------\n",
            "iter: 354 \ttrain loss: 0.007636784991431631\n",
            "--------------------------------------------------\n",
            "iter: 355 \ttrain loss: 0.006277344658231923\n",
            "--------------------------------------------------\n",
            "iter: 356 \ttrain loss: 0.007108917007900636\n",
            "--------------------------------------------------\n",
            "iter: 357 \ttrain loss: 0.006863110388773783\n",
            "--------------------------------------------------\n",
            "iter: 358 \ttrain loss: 0.006461375501984865\n",
            "--------------------------------------------------\n",
            "iter: 359 \ttrain loss: 0.006571781691146205\n",
            "--------------------------------------------------\n",
            "iter: 360 \ttrain loss: 0.0066533292130355635\n",
            "--------------------------------------------------\n",
            "iter: 361 \ttrain loss: 0.006052616520890148\n",
            "--------------------------------------------------\n",
            "iter: 362 \ttrain loss: 0.0072589321995008916\n",
            "--------------------------------------------------\n",
            "iter: 363 \ttrain loss: 0.00654739404760368\n",
            "--------------------------------------------------\n",
            "iter: 364 \ttrain loss: 0.006072721380403993\n",
            "--------------------------------------------------\n",
            "iter: 365 \ttrain loss: 0.007228654696591396\n",
            "--------------------------------------------------\n",
            "iter: 366 \ttrain loss: 0.006293681581040628\n",
            "--------------------------------------------------\n",
            "iter: 367 \ttrain loss: 0.007684897687383865\n",
            "--------------------------------------------------\n",
            "iter: 368 \ttrain loss: 0.0064740407024543225\n",
            "--------------------------------------------------\n",
            "iter: 369 \ttrain loss: 0.006318305682370886\n",
            "--------------------------------------------------\n",
            "iter: 370 \ttrain loss: 0.006233666773262304\n",
            "--------------------------------------------------\n",
            "iter: 371 \ttrain loss: 0.005947079132546779\n",
            "--------------------------------------------------\n",
            "iter: 372 \ttrain loss: 0.00686423730256774\n",
            "--------------------------------------------------\n",
            "iter: 373 \ttrain loss: 0.00609837149114488\n",
            "--------------------------------------------------\n",
            "iter: 374 \ttrain loss: 0.006326740478994073\n",
            "--------------------------------------------------\n",
            "iter: 375 \ttrain loss: 0.005592926052046865\n",
            "--------------------------------------------------\n",
            "iter: 376 \ttrain loss: 0.0061653846436713795\n",
            "--------------------------------------------------\n",
            "iter: 377 \ttrain loss: 0.0065419879653937085\n",
            "--------------------------------------------------\n",
            "iter: 378 \ttrain loss: 0.0064606456794011535\n",
            "--------------------------------------------------\n",
            "iter: 379 \ttrain loss: 0.006025811312977506\n",
            "--------------------------------------------------\n",
            "iter: 380 \ttrain loss: 0.005973013250583554\n",
            "--------------------------------------------------\n",
            "iter: 381 \ttrain loss: 0.005297805368411491\n",
            "--------------------------------------------------\n",
            "iter: 382 \ttrain loss: 0.006614571432846158\n",
            "--------------------------------------------------\n",
            "iter: 383 \ttrain loss: 0.0061072133226484715\n",
            "--------------------------------------------------\n",
            "iter: 384 \ttrain loss: 0.005527688843632981\n",
            "--------------------------------------------------\n",
            "iter: 385 \ttrain loss: 0.0059031016408442015\n",
            "--------------------------------------------------\n",
            "iter: 386 \ttrain loss: 0.00554460013992502\n",
            "--------------------------------------------------\n",
            "iter: 387 \ttrain loss: 0.005801014124303785\n",
            "--------------------------------------------------\n",
            "iter: 388 \ttrain loss: 0.005567663195911975\n",
            "--------------------------------------------------\n",
            "iter: 389 \ttrain loss: 0.005847077949873019\n",
            "--------------------------------------------------\n",
            "iter: 390 \ttrain loss: 0.005841942553519566\n",
            "--------------------------------------------------\n",
            "iter: 391 \ttrain loss: 0.005520146244227205\n",
            "--------------------------------------------------\n",
            "iter: 392 \ttrain loss: 0.0056182189909974355\n",
            "--------------------------------------------------\n",
            "iter: 393 \ttrain loss: 0.005550700229551144\n",
            "--------------------------------------------------\n",
            "iter: 394 \ttrain loss: 0.005840954404047957\n",
            "--------------------------------------------------\n",
            "iter: 395 \ttrain loss: 0.005437971206729531\n",
            "--------------------------------------------------\n",
            "iter: 396 \ttrain loss: 0.005492177292384727\n",
            "--------------------------------------------------\n",
            "iter: 397 \ttrain loss: 0.0054683018950673935\n",
            "--------------------------------------------------\n",
            "iter: 398 \ttrain loss: 0.005770054556483434\n",
            "--------------------------------------------------\n",
            "iter: 399 \ttrain loss: 0.005256197096737137\n",
            "--------------------------------------------------\n",
            "iter: 400 \ttrain loss: 0.005349174600027601\n",
            "--------------------------------------------------\n",
            "iter: 401 \ttrain loss: 0.005304380865494991\n",
            "--------------------------------------------------\n",
            "iter: 402 \ttrain loss: 0.005481361256397115\n",
            "--------------------------------------------------\n",
            "iter: 403 \ttrain loss: 0.0051585882031164095\n",
            "--------------------------------------------------\n",
            "iter: 404 \ttrain loss: 0.004973028285555591\n",
            "--------------------------------------------------\n",
            "iter: 405 \ttrain loss: 0.005459375361334709\n",
            "--------------------------------------------------\n",
            "iter: 406 \ttrain loss: 0.005980973949789596\n",
            "--------------------------------------------------\n",
            "iter: 407 \ttrain loss: 0.0048974443153417015\n",
            "--------------------------------------------------\n",
            "iter: 408 \ttrain loss: 0.00506446085747617\n",
            "--------------------------------------------------\n",
            "iter: 409 \ttrain loss: 0.004958769720539026\n",
            "--------------------------------------------------\n",
            "iter: 410 \ttrain loss: 0.004925035385616856\n",
            "--------------------------------------------------\n",
            "iter: 411 \ttrain loss: 0.0052971796359536395\n",
            "--------------------------------------------------\n",
            "iter: 412 \ttrain loss: 0.005122959276384214\n",
            "--------------------------------------------------\n",
            "iter: 413 \ttrain loss: 0.004646295391394868\n",
            "--------------------------------------------------\n",
            "iter: 414 \ttrain loss: 0.005477505827584607\n",
            "--------------------------------------------------\n",
            "iter: 415 \ttrain loss: 0.005027175680128781\n",
            "--------------------------------------------------\n",
            "iter: 416 \ttrain loss: 0.00501726897624825\n",
            "--------------------------------------------------\n",
            "iter: 417 \ttrain loss: 0.004502256066173987\n",
            "--------------------------------------------------\n",
            "iter: 418 \ttrain loss: 0.005122622021298002\n",
            "--------------------------------------------------\n",
            "iter: 419 \ttrain loss: 0.004735963687673498\n",
            "--------------------------------------------------\n",
            "iter: 420 \ttrain loss: 0.005000194568434339\n",
            "--------------------------------------------------\n",
            "iter: 421 \ttrain loss: 0.004459319731668436\n",
            "--------------------------------------------------\n",
            "iter: 422 \ttrain loss: 0.004829830143590273\n",
            "--------------------------------------------------\n",
            "iter: 423 \ttrain loss: 0.004872160142302942\n",
            "--------------------------------------------------\n",
            "iter: 424 \ttrain loss: 0.004354485308372491\n",
            "--------------------------------------------------\n",
            "iter: 425 \ttrain loss: 0.0049044735522410345\n",
            "--------------------------------------------------\n",
            "iter: 426 \ttrain loss: 0.004642624969123565\n",
            "--------------------------------------------------\n",
            "iter: 427 \ttrain loss: 0.005176096498212441\n",
            "--------------------------------------------------\n",
            "iter: 428 \ttrain loss: 0.004251729589848248\n",
            "--------------------------------------------------\n",
            "iter: 429 \ttrain loss: 0.004638785643594854\n",
            "--------------------------------------------------\n",
            "iter: 430 \ttrain loss: 0.004611892497425072\n",
            "--------------------------------------------------\n",
            "iter: 431 \ttrain loss: 0.00465458747947387\n",
            "--------------------------------------------------\n",
            "iter: 432 \ttrain loss: 0.004743406722209173\n",
            "--------------------------------------------------\n",
            "iter: 433 \ttrain loss: 0.005111578659611773\n",
            "--------------------------------------------------\n",
            "iter: 434 \ttrain loss: 0.004900273027703184\n",
            "--------------------------------------------------\n",
            "iter: 435 \ttrain loss: 0.004275199398845068\n",
            "--------------------------------------------------\n",
            "iter: 436 \ttrain loss: 0.004547530242040416\n",
            "--------------------------------------------------\n",
            "iter: 437 \ttrain loss: 0.004310071655753533\n",
            "--------------------------------------------------\n",
            "iter: 438 \ttrain loss: 0.0045829428031385205\n",
            "--------------------------------------------------\n",
            "iter: 439 \ttrain loss: 0.004528399530973439\n",
            "--------------------------------------------------\n",
            "iter: 440 \ttrain loss: 0.004897475326771896\n",
            "--------------------------------------------------\n",
            "iter: 441 \ttrain loss: 0.004152302468808432\n",
            "--------------------------------------------------\n",
            "iter: 442 \ttrain loss: 0.00464396841145919\n",
            "--------------------------------------------------\n",
            "iter: 443 \ttrain loss: 0.0040223043202312\n",
            "--------------------------------------------------\n",
            "iter: 444 \ttrain loss: 0.004611685855134664\n",
            "--------------------------------------------------\n",
            "iter: 445 \ttrain loss: 0.004143454010097014\n",
            "--------------------------------------------------\n",
            "iter: 446 \ttrain loss: 0.003784752758717731\n",
            "--------------------------------------------------\n",
            "iter: 447 \ttrain loss: 0.004206686189984524\n",
            "--------------------------------------------------\n",
            "iter: 448 \ttrain loss: 0.004398169868831259\n",
            "--------------------------------------------------\n",
            "iter: 449 \ttrain loss: 0.003936176814927675\n",
            "--------------------------------------------------\n",
            "iter: 450 \ttrain loss: 0.004375044769758251\n",
            "--------------------------------------------------\n",
            "iter: 451 \ttrain loss: 0.0049190323143772224\n",
            "--------------------------------------------------\n",
            "iter: 452 \ttrain loss: 0.004024200051601373\n",
            "--------------------------------------------------\n",
            "iter: 453 \ttrain loss: 0.003932200020250554\n",
            "--------------------------------------------------\n",
            "iter: 454 \ttrain loss: 0.0038341146897540477\n",
            "--------------------------------------------------\n",
            "iter: 455 \ttrain loss: 0.003934663065601811\n",
            "--------------------------------------------------\n",
            "iter: 456 \ttrain loss: 0.004049753989617094\n",
            "--------------------------------------------------\n",
            "iter: 457 \ttrain loss: 0.003646422869512209\n",
            "--------------------------------------------------\n",
            "iter: 458 \ttrain loss: 0.003928763778928541\n",
            "--------------------------------------------------\n",
            "iter: 459 \ttrain loss: 0.003949803983264846\n",
            "--------------------------------------------------\n",
            "iter: 460 \ttrain loss: 0.0038458644423866677\n",
            "--------------------------------------------------\n",
            "iter: 461 \ttrain loss: 0.003960745310745368\n",
            "--------------------------------------------------\n",
            "iter: 462 \ttrain loss: 0.003990607509881635\n",
            "--------------------------------------------------\n",
            "iter: 463 \ttrain loss: 0.003516249397749704\n",
            "--------------------------------------------------\n",
            "iter: 464 \ttrain loss: 0.004243246493942168\n",
            "--------------------------------------------------\n",
            "iter: 465 \ttrain loss: 0.004026768855020722\n",
            "--------------------------------------------------\n",
            "iter: 466 \ttrain loss: 0.0035520200165487167\n",
            "--------------------------------------------------\n",
            "iter: 467 \ttrain loss: 0.004180277938950179\n",
            "--------------------------------------------------\n",
            "iter: 468 \ttrain loss: 0.0035039992773873638\n",
            "--------------------------------------------------\n",
            "iter: 469 \ttrain loss: 0.003403537335696436\n",
            "--------------------------------------------------\n",
            "iter: 470 \ttrain loss: 0.0036246451431735283\n",
            "--------------------------------------------------\n",
            "iter: 471 \ttrain loss: 0.0035110250399622493\n",
            "--------------------------------------------------\n",
            "iter: 472 \ttrain loss: 0.0042317179242394075\n",
            "--------------------------------------------------\n",
            "iter: 473 \ttrain loss: 0.0032274574195908135\n",
            "--------------------------------------------------\n",
            "iter: 474 \ttrain loss: 0.003999393634674628\n",
            "--------------------------------------------------\n",
            "iter: 475 \ttrain loss: 0.0033500492503700254\n",
            "--------------------------------------------------\n",
            "iter: 476 \ttrain loss: 0.003524425843644001\n",
            "--------------------------------------------------\n",
            "iter: 477 \ttrain loss: 0.003756239600029018\n",
            "--------------------------------------------------\n",
            "iter: 478 \ttrain loss: 0.003507922129915415\n",
            "--------------------------------------------------\n",
            "iter: 479 \ttrain loss: 0.003666346096900271\n",
            "--------------------------------------------------\n",
            "iter: 480 \ttrain loss: 0.0036955820777099632\n",
            "--------------------------------------------------\n",
            "iter: 481 \ttrain loss: 0.0031906426933170566\n",
            "--------------------------------------------------\n",
            "iter: 482 \ttrain loss: 0.0032868353189521183\n",
            "--------------------------------------------------\n",
            "iter: 483 \ttrain loss: 0.003360370799829669\n",
            "--------------------------------------------------\n",
            "iter: 484 \ttrain loss: 0.0034504275177207446\n",
            "--------------------------------------------------\n",
            "iter: 485 \ttrain loss: 0.0034219019189122126\n",
            "--------------------------------------------------\n",
            "iter: 486 \ttrain loss: 0.003302324670887144\n",
            "--------------------------------------------------\n",
            "iter: 487 \ttrain loss: 0.003550093945387699\n",
            "--------------------------------------------------\n",
            "iter: 488 \ttrain loss: 0.0033207931949547384\n",
            "--------------------------------------------------\n",
            "iter: 489 \ttrain loss: 0.003121503662025128\n",
            "--------------------------------------------------\n",
            "iter: 490 \ttrain loss: 0.003447637866822427\n",
            "--------------------------------------------------\n",
            "iter: 491 \ttrain loss: 0.0031837276512248335\n",
            "--------------------------------------------------\n",
            "iter: 492 \ttrain loss: 0.0032546055390645506\n",
            "--------------------------------------------------\n",
            "iter: 493 \ttrain loss: 0.0036049972436058256\n",
            "--------------------------------------------------\n",
            "iter: 494 \ttrain loss: 0.003154252072641117\n",
            "--------------------------------------------------\n",
            "iter: 495 \ttrain loss: 0.0029395455349734063\n",
            "--------------------------------------------------\n",
            "iter: 496 \ttrain loss: 0.0032728594658799097\n",
            "--------------------------------------------------\n",
            "iter: 497 \ttrain loss: 0.0030392680178652582\n",
            "--------------------------------------------------\n",
            "iter: 498 \ttrain loss: 0.0032252723123054284\n",
            "--------------------------------------------------\n",
            "iter: 499 \ttrain loss: 0.0035433968035545648\n",
            "--------------------------------------------------\n",
            "iter: 500 \ttrain loss: 0.0029326340546515475\n",
            "--------------------------------------------------\n",
            "iter: 501 \ttrain loss: 0.0030867219296479313\n",
            "--------------------------------------------------\n",
            "iter: 502 \ttrain loss: 0.0032061844620084098\n",
            "--------------------------------------------------\n",
            "iter: 503 \ttrain loss: 0.002858650480390769\n",
            "--------------------------------------------------\n",
            "iter: 504 \ttrain loss: 0.0031175546380142055\n",
            "--------------------------------------------------\n",
            "iter: 505 \ttrain loss: 0.003138415768009129\n",
            "--------------------------------------------------\n",
            "iter: 506 \ttrain loss: 0.0029586853527886247\n",
            "--------------------------------------------------\n",
            "iter: 507 \ttrain loss: 0.002977215256538798\n",
            "--------------------------------------------------\n",
            "iter: 508 \ttrain loss: 0.002917095401127674\n",
            "--------------------------------------------------\n",
            "iter: 509 \ttrain loss: 0.0030643901290874323\n",
            "--------------------------------------------------\n",
            "iter: 510 \ttrain loss: 0.003031475192493784\n",
            "--------------------------------------------------\n",
            "iter: 511 \ttrain loss: 0.003079683905133056\n",
            "--------------------------------------------------\n",
            "iter: 512 \ttrain loss: 0.002953629593022537\n",
            "--------------------------------------------------\n",
            "iter: 513 \ttrain loss: 0.0030628947939468024\n",
            "--------------------------------------------------\n",
            "iter: 514 \ttrain loss: 0.0028506786030657785\n",
            "--------------------------------------------------\n",
            "iter: 515 \ttrain loss: 0.00257581590928338\n",
            "--------------------------------------------------\n",
            "iter: 516 \ttrain loss: 0.0028248639782101526\n",
            "--------------------------------------------------\n",
            "iter: 517 \ttrain loss: 0.0025824816145170117\n",
            "--------------------------------------------------\n",
            "iter: 518 \ttrain loss: 0.0027973308156580847\n",
            "--------------------------------------------------\n",
            "iter: 519 \ttrain loss: 0.002971829337030556\n",
            "--------------------------------------------------\n",
            "iter: 520 \ttrain loss: 0.002566390973759608\n",
            "--------------------------------------------------\n",
            "iter: 521 \ttrain loss: 0.0028541215285339846\n",
            "--------------------------------------------------\n",
            "iter: 522 \ttrain loss: 0.0028511290206576197\n",
            "--------------------------------------------------\n",
            "iter: 523 \ttrain loss: 0.0026587665166879233\n",
            "--------------------------------------------------\n",
            "iter: 524 \ttrain loss: 0.002694400620952604\n",
            "--------------------------------------------------\n",
            "iter: 525 \ttrain loss: 0.0027818501473702924\n",
            "--------------------------------------------------\n",
            "iter: 526 \ttrain loss: 0.0027159934483422785\n",
            "--------------------------------------------------\n",
            "iter: 527 \ttrain loss: 0.0024405774277343095\n",
            "--------------------------------------------------\n",
            "iter: 528 \ttrain loss: 0.0027975993347673254\n",
            "--------------------------------------------------\n",
            "iter: 529 \ttrain loss: 0.0025815465476244416\n",
            "--------------------------------------------------\n",
            "iter: 530 \ttrain loss: 0.002603060843303948\n",
            "--------------------------------------------------\n",
            "iter: 531 \ttrain loss: 0.002562940400970987\n",
            "--------------------------------------------------\n",
            "iter: 532 \ttrain loss: 0.0025751684277283858\n",
            "--------------------------------------------------\n",
            "iter: 533 \ttrain loss: 0.0025646020691622983\n",
            "--------------------------------------------------\n",
            "iter: 534 \ttrain loss: 0.002701333156601403\n",
            "--------------------------------------------------\n",
            "iter: 535 \ttrain loss: 0.002319148452349803\n",
            "--------------------------------------------------\n",
            "iter: 536 \ttrain loss: 0.0025611726419563375\n",
            "--------------------------------------------------\n",
            "iter: 537 \ttrain loss: 0.0023261012516227874\n",
            "--------------------------------------------------\n",
            "iter: 538 \ttrain loss: 0.002846280672461253\n",
            "--------------------------------------------------\n",
            "iter: 539 \ttrain loss: 0.0027113371715059074\n",
            "--------------------------------------------------\n",
            "iter: 540 \ttrain loss: 0.0021515238474572153\n",
            "--------------------------------------------------\n",
            "iter: 541 \ttrain loss: 0.0026843425260813856\n",
            "--------------------------------------------------\n",
            "iter: 542 \ttrain loss: 0.0022359277530636106\n",
            "--------------------------------------------------\n",
            "iter: 543 \ttrain loss: 0.002476245666425661\n",
            "--------------------------------------------------\n",
            "iter: 544 \ttrain loss: 0.0024606756854702386\n",
            "--------------------------------------------------\n",
            "iter: 545 \ttrain loss: 0.002309572935297866\n",
            "--------------------------------------------------\n",
            "iter: 546 \ttrain loss: 0.002268203644856868\n",
            "--------------------------------------------------\n",
            "iter: 547 \ttrain loss: 0.002342467415528011\n",
            "--------------------------------------------------\n",
            "iter: 548 \ttrain loss: 0.0022507083509627586\n",
            "--------------------------------------------------\n",
            "iter: 549 \ttrain loss: 0.00223025767525083\n",
            "--------------------------------------------------\n",
            "iter: 550 \ttrain loss: 0.002393900496902525\n",
            "--------------------------------------------------\n",
            "iter: 551 \ttrain loss: 0.002381627856288006\n",
            "--------------------------------------------------\n",
            "iter: 552 \ttrain loss: 0.00222687486162304\n",
            "--------------------------------------------------\n",
            "iter: 553 \ttrain loss: 0.002253464260988824\n",
            "--------------------------------------------------\n",
            "iter: 554 \ttrain loss: 0.002254619570627545\n",
            "--------------------------------------------------\n",
            "iter: 555 \ttrain loss: 0.0021292960588327834\n",
            "--------------------------------------------------\n",
            "iter: 556 \ttrain loss: 0.0019254144552159132\n",
            "--------------------------------------------------\n",
            "iter: 557 \ttrain loss: 0.002370844536515841\n",
            "--------------------------------------------------\n",
            "iter: 558 \ttrain loss: 0.0021115328438171834\n",
            "--------------------------------------------------\n",
            "iter: 559 \ttrain loss: 0.002380350147006549\n",
            "--------------------------------------------------\n",
            "iter: 560 \ttrain loss: 0.002172690982334462\n",
            "--------------------------------------------------\n",
            "iter: 561 \ttrain loss: 0.0019570390443461798\n",
            "--------------------------------------------------\n",
            "iter: 562 \ttrain loss: 0.002051978422813837\n",
            "--------------------------------------------------\n",
            "iter: 563 \ttrain loss: 0.002194897571398383\n",
            "--------------------------------------------------\n",
            "iter: 564 \ttrain loss: 0.002027977535881249\n",
            "--------------------------------------------------\n",
            "iter: 565 \ttrain loss: 0.002225234874012595\n",
            "--------------------------------------------------\n",
            "iter: 566 \ttrain loss: 0.0021567217814545962\n",
            "--------------------------------------------------\n",
            "iter: 567 \ttrain loss: 0.00214926771897127\n",
            "--------------------------------------------------\n",
            "iter: 568 \ttrain loss: 0.001997600146598851\n",
            "--------------------------------------------------\n",
            "iter: 569 \ttrain loss: 0.0019819941587124654\n",
            "--------------------------------------------------\n",
            "iter: 570 \ttrain loss: 0.0018989386793286352\n",
            "--------------------------------------------------\n",
            "iter: 571 \ttrain loss: 0.0019679599893146116\n",
            "--------------------------------------------------\n",
            "iter: 572 \ttrain loss: 0.0019471129762514937\n",
            "--------------------------------------------------\n",
            "iter: 573 \ttrain loss: 0.00190580591940811\n",
            "--------------------------------------------------\n",
            "iter: 574 \ttrain loss: 0.001907019508490425\n",
            "--------------------------------------------------\n",
            "iter: 575 \ttrain loss: 0.00227725888727523\n",
            "--------------------------------------------------\n",
            "iter: 576 \ttrain loss: 0.0018943851938284552\n",
            "--------------------------------------------------\n",
            "iter: 577 \ttrain loss: 0.001849932107321283\n",
            "--------------------------------------------------\n",
            "iter: 578 \ttrain loss: 0.0020142025213802303\n",
            "--------------------------------------------------\n",
            "iter: 579 \ttrain loss: 0.0019182255552312579\n",
            "--------------------------------------------------\n",
            "iter: 580 \ttrain loss: 0.0018362868235347004\n",
            "--------------------------------------------------\n",
            "iter: 581 \ttrain loss: 0.0018141211170075652\n",
            "--------------------------------------------------\n",
            "iter: 582 \ttrain loss: 0.0017875658542831068\n",
            "--------------------------------------------------\n",
            "iter: 583 \ttrain loss: 0.0019483924011386336\n",
            "--------------------------------------------------\n",
            "iter: 584 \ttrain loss: 0.0018078872757032174\n",
            "--------------------------------------------------\n",
            "iter: 585 \ttrain loss: 0.0015996529833086518\n",
            "--------------------------------------------------\n",
            "iter: 586 \ttrain loss: 0.0019288681785769874\n",
            "--------------------------------------------------\n",
            "iter: 587 \ttrain loss: 0.0016711409021253926\n",
            "--------------------------------------------------\n",
            "iter: 588 \ttrain loss: 0.0019526928150476344\n",
            "--------------------------------------------------\n",
            "iter: 589 \ttrain loss: 0.0016974711219802658\n",
            "--------------------------------------------------\n",
            "iter: 590 \ttrain loss: 0.0016102636954944758\n",
            "--------------------------------------------------\n",
            "iter: 591 \ttrain loss: 0.0018614277537415602\n",
            "--------------------------------------------------\n",
            "iter: 592 \ttrain loss: 0.0018901523831780535\n",
            "--------------------------------------------------\n",
            "iter: 593 \ttrain loss: 0.0017923504105086488\n",
            "--------------------------------------------------\n",
            "iter: 594 \ttrain loss: 0.0015187231589612648\n",
            "--------------------------------------------------\n",
            "iter: 595 \ttrain loss: 0.0017355475305061096\n",
            "--------------------------------------------------\n",
            "iter: 596 \ttrain loss: 0.001733200314800476\n",
            "--------------------------------------------------\n",
            "iter: 597 \ttrain loss: 0.0019071662745826898\n",
            "--------------------------------------------------\n",
            "iter: 598 \ttrain loss: 0.0016489210045491432\n",
            "--------------------------------------------------\n",
            "iter: 599 \ttrain loss: 0.001789804995793362\n",
            "--------------------------------------------------\n",
            "iter: 600 \ttrain loss: 0.0018212956175327004\n",
            "--------------------------------------------------\n",
            "iter: 601 \ttrain loss: 0.001724672216248714\n",
            "--------------------------------------------------\n",
            "iter: 602 \ttrain loss: 0.0015182037476479749\n",
            "--------------------------------------------------\n",
            "iter: 603 \ttrain loss: 0.0016694612136366747\n",
            "--------------------------------------------------\n",
            "iter: 604 \ttrain loss: 0.001399494566076565\n",
            "--------------------------------------------------\n",
            "iter: 605 \ttrain loss: 0.00170163828991853\n",
            "--------------------------------------------------\n",
            "iter: 606 \ttrain loss: 0.001562666657817304\n",
            "--------------------------------------------------\n",
            "iter: 607 \ttrain loss: 0.0017521205758344988\n",
            "--------------------------------------------------\n",
            "iter: 608 \ttrain loss: 0.0016003383138103143\n",
            "--------------------------------------------------\n",
            "iter: 609 \ttrain loss: 0.0015419178036616445\n",
            "--------------------------------------------------\n",
            "iter: 610 \ttrain loss: 0.0017337875887345445\n",
            "--------------------------------------------------\n",
            "iter: 611 \ttrain loss: 0.001557231813807356\n",
            "--------------------------------------------------\n",
            "iter: 612 \ttrain loss: 0.0013766674995884826\n",
            "--------------------------------------------------\n",
            "iter: 613 \ttrain loss: 0.0014780073861666563\n",
            "--------------------------------------------------\n",
            "iter: 614 \ttrain loss: 0.0015785197810151289\n",
            "--------------------------------------------------\n",
            "iter: 615 \ttrain loss: 0.0014192950762916378\n",
            "--------------------------------------------------\n",
            "iter: 616 \ttrain loss: 0.001681151089534729\n",
            "--------------------------------------------------\n",
            "iter: 617 \ttrain loss: 0.0015328622092912814\n",
            "--------------------------------------------------\n",
            "iter: 618 \ttrain loss: 0.0014345813810679986\n",
            "--------------------------------------------------\n",
            "iter: 619 \ttrain loss: 0.0015045707915160268\n",
            "--------------------------------------------------\n",
            "iter: 620 \ttrain loss: 0.0014629378362053073\n",
            "--------------------------------------------------\n",
            "iter: 621 \ttrain loss: 0.0014811668210761928\n",
            "--------------------------------------------------\n",
            "iter: 622 \ttrain loss: 0.0014563151784130859\n",
            "--------------------------------------------------\n",
            "iter: 623 \ttrain loss: 0.0014323474784956776\n",
            "--------------------------------------------------\n",
            "iter: 624 \ttrain loss: 0.0015332641599301033\n",
            "--------------------------------------------------\n",
            "iter: 625 \ttrain loss: 0.0013958712881756063\n",
            "--------------------------------------------------\n",
            "iter: 626 \ttrain loss: 0.0015055986828023857\n",
            "--------------------------------------------------\n",
            "iter: 627 \ttrain loss: 0.0013210522544011903\n",
            "--------------------------------------------------\n",
            "iter: 628 \ttrain loss: 0.0013593175017988326\n",
            "--------------------------------------------------\n",
            "iter: 629 \ttrain loss: 0.0014268310279781258\n",
            "--------------------------------------------------\n",
            "iter: 630 \ttrain loss: 0.0014911730249679072\n",
            "--------------------------------------------------\n",
            "iter: 631 \ttrain loss: 0.0012616197222759299\n",
            "--------------------------------------------------\n",
            "iter: 632 \ttrain loss: 0.0018043869399700806\n",
            "--------------------------------------------------\n",
            "iter: 633 \ttrain loss: 0.001227003543959395\n",
            "--------------------------------------------------\n",
            "iter: 634 \ttrain loss: 0.0012826551052634578\n",
            "--------------------------------------------------\n",
            "iter: 635 \ttrain loss: 0.0015384798502863248\n",
            "--------------------------------------------------\n",
            "iter: 636 \ttrain loss: 0.0012710551522237342\n",
            "--------------------------------------------------\n",
            "iter: 637 \ttrain loss: 0.0013137488047937696\n",
            "--------------------------------------------------\n",
            "iter: 638 \ttrain loss: 0.0014613856654353491\n",
            "--------------------------------------------------\n",
            "iter: 639 \ttrain loss: 0.001315363849007555\n",
            "--------------------------------------------------\n",
            "iter: 640 \ttrain loss: 0.0016648433716758664\n",
            "--------------------------------------------------\n",
            "iter: 641 \ttrain loss: 0.0013275638290082164\n",
            "--------------------------------------------------\n",
            "iter: 642 \ttrain loss: 0.0013986727827154345\n",
            "--------------------------------------------------\n",
            "iter: 643 \ttrain loss: 0.0013177755682543457\n",
            "--------------------------------------------------\n",
            "iter: 644 \ttrain loss: 0.001337621519625133\n",
            "--------------------------------------------------\n",
            "iter: 645 \ttrain loss: 0.001361944295706907\n",
            "--------------------------------------------------\n",
            "iter: 646 \ttrain loss: 0.0013690357172939519\n",
            "--------------------------------------------------\n",
            "iter: 647 \ttrain loss: 0.001301442971308191\n",
            "--------------------------------------------------\n",
            "iter: 648 \ttrain loss: 0.0013035341061190307\n",
            "--------------------------------------------------\n",
            "iter: 649 \ttrain loss: 0.0014035694944318268\n",
            "--------------------------------------------------\n",
            "iter: 650 \ttrain loss: 0.0013054165253759916\n",
            "--------------------------------------------------\n",
            "iter: 651 \ttrain loss: 0.0012476532860823922\n",
            "--------------------------------------------------\n",
            "iter: 652 \ttrain loss: 0.0017064128623047077\n",
            "--------------------------------------------------\n",
            "iter: 653 \ttrain loss: 0.0013921854892716312\n",
            "--------------------------------------------------\n",
            "iter: 654 \ttrain loss: 0.0012675224502040941\n",
            "--------------------------------------------------\n",
            "iter: 655 \ttrain loss: 0.001210183743816736\n",
            "--------------------------------------------------\n",
            "iter: 656 \ttrain loss: 0.0013641065990857182\n",
            "--------------------------------------------------\n",
            "iter: 657 \ttrain loss: 0.0012074822669443097\n",
            "--------------------------------------------------\n",
            "iter: 658 \ttrain loss: 0.0011698963220806572\n",
            "--------------------------------------------------\n",
            "iter: 659 \ttrain loss: 0.0013937910314223267\n",
            "--------------------------------------------------\n",
            "iter: 660 \ttrain loss: 0.0012408776025257132\n",
            "--------------------------------------------------\n",
            "iter: 661 \ttrain loss: 0.0013761261810450718\n",
            "--------------------------------------------------\n",
            "iter: 662 \ttrain loss: 0.0012066971294138014\n",
            "--------------------------------------------------\n",
            "iter: 663 \ttrain loss: 0.0012002625339555708\n",
            "--------------------------------------------------\n",
            "iter: 664 \ttrain loss: 0.0012308113685810004\n",
            "--------------------------------------------------\n",
            "iter: 665 \ttrain loss: 0.001309849721740471\n",
            "--------------------------------------------------\n",
            "iter: 666 \ttrain loss: 0.0012715421901769298\n",
            "--------------------------------------------------\n",
            "iter: 667 \ttrain loss: 0.0012964309061148096\n",
            "--------------------------------------------------\n",
            "iter: 668 \ttrain loss: 0.0012736344783919466\n",
            "--------------------------------------------------\n",
            "iter: 669 \ttrain loss: 0.0012758950678877247\n",
            "--------------------------------------------------\n",
            "iter: 670 \ttrain loss: 0.0012609299332034913\n",
            "--------------------------------------------------\n",
            "iter: 671 \ttrain loss: 0.0011250162587705954\n",
            "--------------------------------------------------\n",
            "iter: 672 \ttrain loss: 0.0013819633720236354\n",
            "--------------------------------------------------\n",
            "iter: 673 \ttrain loss: 0.0012359670345161895\n",
            "--------------------------------------------------\n",
            "iter: 674 \ttrain loss: 0.001172774057318292\n",
            "--------------------------------------------------\n",
            "iter: 675 \ttrain loss: 0.0012365965754063042\n",
            "--------------------------------------------------\n",
            "iter: 676 \ttrain loss: 0.001276997394558979\n",
            "--------------------------------------------------\n",
            "iter: 677 \ttrain loss: 0.0011059483342040837\n",
            "--------------------------------------------------\n",
            "iter: 678 \ttrain loss: 0.0012966696765460495\n",
            "--------------------------------------------------\n",
            "iter: 679 \ttrain loss: 0.0014953106144398422\n",
            "--------------------------------------------------\n",
            "iter: 680 \ttrain loss: 0.0012460755011490731\n",
            "--------------------------------------------------\n",
            "iter: 681 \ttrain loss: 0.001273249769111329\n",
            "--------------------------------------------------\n",
            "iter: 682 \ttrain loss: 0.0013145179759659314\n",
            "--------------------------------------------------\n",
            "iter: 683 \ttrain loss: 0.00107839069685398\n",
            "--------------------------------------------------\n",
            "iter: 684 \ttrain loss: 0.0013999488784486168\n",
            "--------------------------------------------------\n",
            "iter: 685 \ttrain loss: 0.0011302805167800612\n",
            "--------------------------------------------------\n",
            "iter: 686 \ttrain loss: 0.0011940361464713536\n",
            "--------------------------------------------------\n",
            "iter: 687 \ttrain loss: 0.0014298405679499025\n",
            "--------------------------------------------------\n",
            "iter: 688 \ttrain loss: 0.00118569727934407\n",
            "--------------------------------------------------\n",
            "iter: 689 \ttrain loss: 0.0013931141094488922\n",
            "--------------------------------------------------\n",
            "iter: 690 \ttrain loss: 0.0012453993407503575\n",
            "--------------------------------------------------\n",
            "iter: 691 \ttrain loss: 0.0012096628511411819\n",
            "--------------------------------------------------\n",
            "iter: 692 \ttrain loss: 0.0012040327358286507\n",
            "--------------------------------------------------\n",
            "iter: 693 \ttrain loss: 0.0012110682012561917\n",
            "--------------------------------------------------\n",
            "iter: 694 \ttrain loss: 0.0011951387467403806\n",
            "--------------------------------------------------\n",
            "iter: 695 \ttrain loss: 0.0012408279106651899\n",
            "--------------------------------------------------\n",
            "iter: 696 \ttrain loss: 0.0011771429468215378\n",
            "--------------------------------------------------\n",
            "iter: 697 \ttrain loss: 0.0011787154910370733\n",
            "--------------------------------------------------\n",
            "iter: 698 \ttrain loss: 0.0012199520146459017\n",
            "--------------------------------------------------\n",
            "iter: 699 \ttrain loss: 0.001122351188515531\n",
            "--------------------------------------------------\n",
            "iter: 700 \ttrain loss: 0.0011754455636896676\n",
            "--------------------------------------------------\n",
            "iter: 701 \ttrain loss: 0.001304503342677762\n",
            "--------------------------------------------------\n",
            "iter: 702 \ttrain loss: 0.0012875535793519363\n",
            "--------------------------------------------------\n",
            "iter: 703 \ttrain loss: 0.0010606690865996517\n",
            "--------------------------------------------------\n",
            "iter: 704 \ttrain loss: 0.0012995231976712105\n",
            "--------------------------------------------------\n",
            "iter: 705 \ttrain loss: 0.0009712007938147125\n",
            "--------------------------------------------------\n",
            "iter: 706 \ttrain loss: 0.0012320672179945379\n",
            "--------------------------------------------------\n",
            "iter: 707 \ttrain loss: 0.0012215143873136314\n",
            "--------------------------------------------------\n",
            "iter: 708 \ttrain loss: 0.0011964022702199947\n",
            "--------------------------------------------------\n",
            "iter: 709 \ttrain loss: 0.0011618609048184528\n",
            "--------------------------------------------------\n",
            "iter: 710 \ttrain loss: 0.0011902525083603737\n",
            "--------------------------------------------------\n",
            "iter: 711 \ttrain loss: 0.0010732279434592315\n",
            "--------------------------------------------------\n",
            "iter: 712 \ttrain loss: 0.0012563311753601826\n",
            "--------------------------------------------------\n",
            "iter: 713 \ttrain loss: 0.001117489611646206\n",
            "--------------------------------------------------\n",
            "iter: 714 \ttrain loss: 0.001223157045007753\n",
            "--------------------------------------------------\n",
            "iter: 715 \ttrain loss: 0.0012463388282116176\n",
            "--------------------------------------------------\n",
            "iter: 716 \ttrain loss: 0.0011098972373275668\n",
            "--------------------------------------------------\n",
            "iter: 717 \ttrain loss: 0.0011609300176049187\n",
            "--------------------------------------------------\n",
            "iter: 718 \ttrain loss: 0.0012065422346887176\n",
            "--------------------------------------------------\n",
            "iter: 719 \ttrain loss: 0.0011779215425877882\n",
            "--------------------------------------------------\n",
            "iter: 720 \ttrain loss: 0.0010595405400198729\n",
            "--------------------------------------------------\n",
            "iter: 721 \ttrain loss: 0.0012275938443517683\n",
            "--------------------------------------------------\n",
            "iter: 722 \ttrain loss: 0.0011776216180923017\n",
            "--------------------------------------------------\n",
            "iter: 723 \ttrain loss: 0.0010980350736511499\n",
            "--------------------------------------------------\n",
            "iter: 724 \ttrain loss: 0.001075632962493515\n",
            "--------------------------------------------------\n",
            "iter: 725 \ttrain loss: 0.0010431099858231442\n",
            "--------------------------------------------------\n",
            "iter: 726 \ttrain loss: 0.0010919962450790972\n",
            "--------------------------------------------------\n",
            "iter: 727 \ttrain loss: 0.0011933082735898458\n",
            "--------------------------------------------------\n",
            "iter: 728 \ttrain loss: 0.001118270056067209\n",
            "--------------------------------------------------\n",
            "iter: 729 \ttrain loss: 0.0011983127036591012\n",
            "--------------------------------------------------\n",
            "iter: 730 \ttrain loss: 0.0010675758828797245\n",
            "--------------------------------------------------\n",
            "iter: 731 \ttrain loss: 0.001108053213211449\n",
            "--------------------------------------------------\n",
            "iter: 732 \ttrain loss: 0.0011348192877858377\n",
            "--------------------------------------------------\n",
            "iter: 733 \ttrain loss: 0.001132714697576532\n",
            "--------------------------------------------------\n",
            "iter: 734 \ttrain loss: 0.0011473000879253794\n",
            "--------------------------------------------------\n",
            "iter: 735 \ttrain loss: 0.001276995415737656\n",
            "--------------------------------------------------\n",
            "iter: 736 \ttrain loss: 0.0010625789153432326\n",
            "--------------------------------------------------\n",
            "iter: 737 \ttrain loss: 0.0011529197419174087\n",
            "--------------------------------------------------\n",
            "iter: 738 \ttrain loss: 0.0011462736344739943\n",
            "--------------------------------------------------\n",
            "iter: 739 \ttrain loss: 0.0011951215719892049\n",
            "--------------------------------------------------\n",
            "iter: 740 \ttrain loss: 0.001073942066679406\n",
            "--------------------------------------------------\n",
            "iter: 741 \ttrain loss: 0.0010646610861268075\n",
            "--------------------------------------------------\n",
            "iter: 742 \ttrain loss: 0.0011379393259040767\n",
            "--------------------------------------------------\n",
            "iter: 743 \ttrain loss: 0.0011970025447002276\n",
            "--------------------------------------------------\n",
            "iter: 744 \ttrain loss: 0.001159398967707092\n",
            "--------------------------------------------------\n",
            "iter: 745 \ttrain loss: 0.0011724326468037845\n",
            "--------------------------------------------------\n",
            "iter: 746 \ttrain loss: 0.0011850530951794526\n",
            "--------------------------------------------------\n",
            "iter: 747 \ttrain loss: 0.0010431744659638446\n",
            "--------------------------------------------------\n",
            "iter: 748 \ttrain loss: 0.0010462276508136636\n",
            "--------------------------------------------------\n",
            "iter: 749 \ttrain loss: 0.0011542789785516242\n",
            "--------------------------------------------------\n",
            "iter: 750 \ttrain loss: 0.00141002410894861\n",
            "--------------------------------------------------\n",
            "iter: 751 \ttrain loss: 0.0011212045388380978\n",
            "--------------------------------------------------\n",
            "iter: 752 \ttrain loss: 0.0011146299113382924\n",
            "--------------------------------------------------\n",
            "iter: 753 \ttrain loss: 0.0011295345972089543\n",
            "--------------------------------------------------\n",
            "iter: 754 \ttrain loss: 0.0011434738533195538\n",
            "--------------------------------------------------\n",
            "iter: 755 \ttrain loss: 0.00124421089840542\n",
            "--------------------------------------------------\n",
            "iter: 756 \ttrain loss: 0.0010717892856886707\n",
            "--------------------------------------------------\n",
            "iter: 757 \ttrain loss: 0.0010760843105853389\n",
            "--------------------------------------------------\n",
            "iter: 758 \ttrain loss: 0.0011549977552073004\n",
            "--------------------------------------------------\n",
            "iter: 759 \ttrain loss: 0.0011497370422650028\n",
            "--------------------------------------------------\n",
            "iter: 760 \ttrain loss: 0.0011418941507201506\n",
            "--------------------------------------------------\n",
            "iter: 761 \ttrain loss: 0.0011418037497637346\n",
            "--------------------------------------------------\n",
            "iter: 762 \ttrain loss: 0.0010965283780347776\n",
            "--------------------------------------------------\n",
            "iter: 763 \ttrain loss: 0.001416695718418571\n",
            "--------------------------------------------------\n",
            "iter: 764 \ttrain loss: 0.0009946516106389318\n",
            "--------------------------------------------------\n",
            "iter: 765 \ttrain loss: 0.0012009885442407931\n",
            "--------------------------------------------------\n",
            "iter: 766 \ttrain loss: 0.001132315591961205\n",
            "--------------------------------------------------\n",
            "iter: 767 \ttrain loss: 0.0010369757437654929\n",
            "--------------------------------------------------\n",
            "iter: 768 \ttrain loss: 0.001280775511041344\n",
            "--------------------------------------------------\n",
            "iter: 769 \ttrain loss: 0.0008798511659173479\n",
            "--------------------------------------------------\n",
            "iter: 770 \ttrain loss: 0.0011326103362715256\n",
            "--------------------------------------------------\n",
            "iter: 771 \ttrain loss: 0.0013178298542220047\n",
            "--------------------------------------------------\n",
            "iter: 772 \ttrain loss: 0.001018200754602561\n",
            "--------------------------------------------------\n",
            "iter: 773 \ttrain loss: 0.0010755063095357288\n",
            "--------------------------------------------------\n",
            "iter: 774 \ttrain loss: 0.0013055313445640053\n",
            "--------------------------------------------------\n",
            "iter: 775 \ttrain loss: 0.0011421552706188999\n",
            "--------------------------------------------------\n",
            "iter: 776 \ttrain loss: 0.001103694628492138\n",
            "--------------------------------------------------\n",
            "iter: 777 \ttrain loss: 0.001203245403357724\n",
            "--------------------------------------------------\n",
            "iter: 778 \ttrain loss: 0.0011610564667132153\n",
            "--------------------------------------------------\n",
            "iter: 779 \ttrain loss: 0.0012179316265140804\n",
            "--------------------------------------------------\n",
            "iter: 780 \ttrain loss: 0.0009727143649794034\n",
            "--------------------------------------------------\n",
            "iter: 781 \ttrain loss: 0.001163791228298874\n",
            "--------------------------------------------------\n",
            "iter: 782 \ttrain loss: 0.0011422446366868674\n",
            "--------------------------------------------------\n",
            "iter: 783 \ttrain loss: 0.0011323713702529772\n",
            "--------------------------------------------------\n",
            "iter: 784 \ttrain loss: 0.0011192542635699316\n",
            "--------------------------------------------------\n",
            "iter: 785 \ttrain loss: 0.0012284584499109624\n",
            "--------------------------------------------------\n",
            "iter: 786 \ttrain loss: 0.0010765787713027245\n",
            "--------------------------------------------------\n",
            "iter: 787 \ttrain loss: 0.0010906056293744052\n",
            "--------------------------------------------------\n",
            "iter: 788 \ttrain loss: 0.0011318821850649244\n",
            "--------------------------------------------------\n",
            "iter: 789 \ttrain loss: 0.0009030527102992886\n",
            "--------------------------------------------------\n",
            "iter: 790 \ttrain loss: 0.0016282961825433012\n",
            "--------------------------------------------------\n",
            "iter: 791 \ttrain loss: 0.0010993433757901258\n",
            "--------------------------------------------------\n",
            "iter: 792 \ttrain loss: 0.0011612960877638025\n",
            "--------------------------------------------------\n",
            "iter: 793 \ttrain loss: 0.0010999864926719025\n",
            "--------------------------------------------------\n",
            "iter: 794 \ttrain loss: 0.0011085798354453424\n",
            "--------------------------------------------------\n",
            "iter: 795 \ttrain loss: 0.0011759659763881701\n",
            "--------------------------------------------------\n",
            "iter: 796 \ttrain loss: 0.0010006173373213487\n",
            "--------------------------------------------------\n",
            "iter: 797 \ttrain loss: 0.0013076269803658505\n",
            "--------------------------------------------------\n",
            "iter: 798 \ttrain loss: 0.0012724977911316385\n",
            "--------------------------------------------------\n",
            "iter: 799 \ttrain loss: 0.001025734638321777\n",
            "--------------------------------------------------\n",
            "iter: 800 \ttrain loss: 0.0010750196626124217\n",
            "--------------------------------------------------\n",
            "iter: 801 \ttrain loss: 0.0010732701449365487\n",
            "--------------------------------------------------\n",
            "iter: 802 \ttrain loss: 0.0010550699268902617\n",
            "--------------------------------------------------\n",
            "iter: 803 \ttrain loss: 0.0012135810855351677\n",
            "--------------------------------------------------\n",
            "iter: 804 \ttrain loss: 0.0014000165050784914\n",
            "--------------------------------------------------\n",
            "iter: 805 \ttrain loss: 0.0011108156114446128\n",
            "--------------------------------------------------\n",
            "iter: 806 \ttrain loss: 0.0011521272086501507\n",
            "--------------------------------------------------\n",
            "iter: 807 \ttrain loss: 0.0011587417203569054\n",
            "--------------------------------------------------\n",
            "iter: 808 \ttrain loss: 0.0011462977563937995\n",
            "--------------------------------------------------\n",
            "iter: 809 \ttrain loss: 0.001147859238085608\n",
            "--------------------------------------------------\n",
            "iter: 810 \ttrain loss: 0.0010930721916630427\n",
            "--------------------------------------------------\n",
            "iter: 811 \ttrain loss: 0.0010845479107396802\n",
            "--------------------------------------------------\n",
            "iter: 812 \ttrain loss: 0.001166687490694245\n",
            "--------------------------------------------------\n",
            "iter: 813 \ttrain loss: 0.0010189569832937456\n",
            "--------------------------------------------------\n",
            "iter: 814 \ttrain loss: 0.001087237381914116\n",
            "--------------------------------------------------\n",
            "iter: 815 \ttrain loss: 0.001210617229398309\n",
            "--------------------------------------------------\n",
            "iter: 816 \ttrain loss: 0.0010932982046887503\n",
            "--------------------------------------------------\n",
            "iter: 817 \ttrain loss: 0.001102785506745886\n",
            "--------------------------------------------------\n",
            "iter: 818 \ttrain loss: 0.0010377031174683814\n",
            "--------------------------------------------------\n",
            "iter: 819 \ttrain loss: 0.0011884928876576598\n",
            "--------------------------------------------------\n",
            "iter: 820 \ttrain loss: 0.00114961603444619\n",
            "--------------------------------------------------\n",
            "iter: 821 \ttrain loss: 0.0012215198773703766\n",
            "--------------------------------------------------\n",
            "iter: 822 \ttrain loss: 0.0010724012881387682\n",
            "--------------------------------------------------\n",
            "iter: 823 \ttrain loss: 0.0014463591691939192\n",
            "--------------------------------------------------\n",
            "iter: 824 \ttrain loss: 0.0010328805947787537\n",
            "--------------------------------------------------\n",
            "iter: 825 \ttrain loss: 0.0010389153613796392\n",
            "--------------------------------------------------\n",
            "iter: 826 \ttrain loss: 0.0011094385425625163\n",
            "--------------------------------------------------\n",
            "iter: 827 \ttrain loss: 0.001185163279821986\n",
            "--------------------------------------------------\n",
            "iter: 828 \ttrain loss: 0.001025020842428648\n",
            "--------------------------------------------------\n",
            "iter: 829 \ttrain loss: 0.0011163024696732875\n",
            "--------------------------------------------------\n",
            "iter: 830 \ttrain loss: 0.0011189219526056592\n",
            "--------------------------------------------------\n",
            "iter: 831 \ttrain loss: 0.001083400093012948\n",
            "--------------------------------------------------\n",
            "iter: 832 \ttrain loss: 0.0011066694084078939\n",
            "--------------------------------------------------\n",
            "iter: 833 \ttrain loss: 0.0012531147567578906\n",
            "--------------------------------------------------\n",
            "iter: 834 \ttrain loss: 0.0010600501118635103\n",
            "--------------------------------------------------\n",
            "iter: 835 \ttrain loss: 0.001198005358197291\n",
            "--------------------------------------------------\n",
            "iter: 836 \ttrain loss: 0.0010405278748902929\n",
            "--------------------------------------------------\n",
            "iter: 837 \ttrain loss: 0.0011757784013624384\n",
            "--------------------------------------------------\n",
            "iter: 838 \ttrain loss: 0.0011971155827242555\n",
            "--------------------------------------------------\n",
            "iter: 839 \ttrain loss: 0.0009839207840212354\n",
            "--------------------------------------------------\n",
            "iter: 840 \ttrain loss: 0.0012560775139911868\n",
            "--------------------------------------------------\n",
            "iter: 841 \ttrain loss: 0.000883974995846937\n",
            "--------------------------------------------------\n",
            "iter: 842 \ttrain loss: 0.00109981218010983\n",
            "--------------------------------------------------\n",
            "iter: 843 \ttrain loss: 0.0011926005560282396\n",
            "--------------------------------------------------\n",
            "iter: 844 \ttrain loss: 0.0010980523755853295\n",
            "--------------------------------------------------\n",
            "iter: 845 \ttrain loss: 0.0009841522607371995\n",
            "--------------------------------------------------\n",
            "iter: 846 \ttrain loss: 0.0012163286581065988\n",
            "--------------------------------------------------\n",
            "iter: 847 \ttrain loss: 0.0010407561244567777\n",
            "--------------------------------------------------\n",
            "iter: 848 \ttrain loss: 0.001127414745658443\n",
            "--------------------------------------------------\n",
            "iter: 849 \ttrain loss: 0.0012250862826602732\n",
            "--------------------------------------------------\n",
            "iter: 850 \ttrain loss: 0.0011587928839675125\n",
            "--------------------------------------------------\n",
            "iter: 851 \ttrain loss: 0.0010034679674254384\n",
            "--------------------------------------------------\n",
            "iter: 852 \ttrain loss: 0.0010945340800139465\n",
            "--------------------------------------------------\n",
            "iter: 853 \ttrain loss: 0.0010792457815093832\n",
            "--------------------------------------------------\n",
            "iter: 854 \ttrain loss: 0.0011539482965475293\n",
            "--------------------------------------------------\n",
            "iter: 855 \ttrain loss: 0.0011871088452596024\n",
            "--------------------------------------------------\n",
            "iter: 856 \ttrain loss: 0.0010602661467252563\n",
            "--------------------------------------------------\n",
            "iter: 857 \ttrain loss: 0.0010680436027592011\n",
            "--------------------------------------------------\n",
            "iter: 858 \ttrain loss: 0.0011042747642927018\n",
            "--------------------------------------------------\n",
            "iter: 859 \ttrain loss: 0.0011240474555387415\n",
            "--------------------------------------------------\n",
            "iter: 860 \ttrain loss: 0.0010783793505673115\n",
            "--------------------------------------------------\n",
            "iter: 861 \ttrain loss: 0.00115101030718112\n",
            "--------------------------------------------------\n",
            "iter: 862 \ttrain loss: 0.0010069658854134432\n",
            "--------------------------------------------------\n",
            "iter: 863 \ttrain loss: 0.001139971711619061\n",
            "--------------------------------------------------\n",
            "iter: 864 \ttrain loss: 0.0010994732918675296\n",
            "--------------------------------------------------\n",
            "iter: 865 \ttrain loss: 0.0011602835210307947\n",
            "--------------------------------------------------\n",
            "iter: 866 \ttrain loss: 0.0010737263345629482\n",
            "--------------------------------------------------\n",
            "iter: 867 \ttrain loss: 0.0010310779457257918\n",
            "--------------------------------------------------\n",
            "iter: 868 \ttrain loss: 0.0010902840880931022\n",
            "--------------------------------------------------\n",
            "iter: 869 \ttrain loss: 0.0012513996233654014\n",
            "--------------------------------------------------\n",
            "iter: 870 \ttrain loss: 0.001052645414753561\n",
            "--------------------------------------------------\n",
            "iter: 871 \ttrain loss: 0.001020937956037177\n",
            "--------------------------------------------------\n",
            "iter: 872 \ttrain loss: 0.001120807820792262\n",
            "--------------------------------------------------\n",
            "iter: 873 \ttrain loss: 0.0010813257181911636\n",
            "--------------------------------------------------\n",
            "iter: 874 \ttrain loss: 0.0011055436123189389\n",
            "--------------------------------------------------\n",
            "iter: 875 \ttrain loss: 0.0010845886686370817\n",
            "--------------------------------------------------\n",
            "iter: 876 \ttrain loss: 0.0010754790177589352\n",
            "--------------------------------------------------\n",
            "iter: 877 \ttrain loss: 0.0011008599202937865\n",
            "--------------------------------------------------\n",
            "iter: 878 \ttrain loss: 0.0010530889162747152\n",
            "--------------------------------------------------\n",
            "iter: 879 \ttrain loss: 0.0012964402170738546\n",
            "--------------------------------------------------\n",
            "iter: 880 \ttrain loss: 0.0010365267648734734\n",
            "--------------------------------------------------\n",
            "iter: 881 \ttrain loss: 0.001208897284665176\n",
            "--------------------------------------------------\n",
            "iter: 882 \ttrain loss: 0.0010624046503953802\n",
            "--------------------------------------------------\n",
            "iter: 883 \ttrain loss: 0.0010190114967015343\n",
            "--------------------------------------------------\n",
            "iter: 884 \ttrain loss: 0.001166014235206945\n",
            "--------------------------------------------------\n",
            "iter: 885 \ttrain loss: 0.0010511275003203741\n",
            "--------------------------------------------------\n",
            "iter: 886 \ttrain loss: 0.0013954556772713316\n",
            "--------------------------------------------------\n",
            "iter: 887 \ttrain loss: 0.001075640121641903\n",
            "--------------------------------------------------\n",
            "iter: 888 \ttrain loss: 0.0011094370010469284\n",
            "--------------------------------------------------\n",
            "iter: 889 \ttrain loss: 0.0010270873602534576\n",
            "--------------------------------------------------\n",
            "iter: 890 \ttrain loss: 0.0010243587229902542\n",
            "--------------------------------------------------\n",
            "iter: 891 \ttrain loss: 0.0011502161935605718\n",
            "--------------------------------------------------\n",
            "iter: 892 \ttrain loss: 0.0011268617765663191\n",
            "--------------------------------------------------\n",
            "iter: 893 \ttrain loss: 0.001052812698368067\n",
            "--------------------------------------------------\n",
            "iter: 894 \ttrain loss: 0.001109535639851053\n",
            "--------------------------------------------------\n",
            "iter: 895 \ttrain loss: 0.0011758207612114545\n",
            "--------------------------------------------------\n",
            "iter: 896 \ttrain loss: 0.001059749609343877\n",
            "--------------------------------------------------\n",
            "iter: 897 \ttrain loss: 0.0011380347690513831\n",
            "--------------------------------------------------\n",
            "iter: 898 \ttrain loss: 0.0010955703803033955\n",
            "--------------------------------------------------\n",
            "iter: 899 \ttrain loss: 0.0010990204979157614\n",
            "--------------------------------------------------\n",
            "iter: 900 \ttrain loss: 0.001033603947211368\n",
            "--------------------------------------------------\n",
            "iter: 901 \ttrain loss: 0.001103668885610659\n",
            "--------------------------------------------------\n",
            "iter: 902 \ttrain loss: 0.0012906341080636756\n",
            "--------------------------------------------------\n",
            "iter: 903 \ttrain loss: 0.001103206047222429\n",
            "--------------------------------------------------\n",
            "iter: 904 \ttrain loss: 0.001083768256693408\n",
            "--------------------------------------------------\n",
            "iter: 905 \ttrain loss: 0.0011419743202591385\n",
            "--------------------------------------------------\n",
            "iter: 906 \ttrain loss: 0.0010668152317778858\n",
            "--------------------------------------------------\n",
            "iter: 907 \ttrain loss: 0.0013538460980702693\n",
            "--------------------------------------------------\n",
            "iter: 908 \ttrain loss: 0.0011301065824730113\n",
            "--------------------------------------------------\n",
            "iter: 909 \ttrain loss: 0.0010537452119378607\n",
            "--------------------------------------------------\n",
            "iter: 910 \ttrain loss: 0.0010445141189118624\n",
            "--------------------------------------------------\n",
            "iter: 911 \ttrain loss: 0.0011679901300516858\n",
            "--------------------------------------------------\n",
            "iter: 912 \ttrain loss: 0.0009519072190416391\n",
            "--------------------------------------------------\n",
            "iter: 913 \ttrain loss: 0.0012552262689971136\n",
            "--------------------------------------------------\n",
            "iter: 914 \ttrain loss: 0.0011014354517980905\n",
            "--------------------------------------------------\n",
            "iter: 915 \ttrain loss: 0.001508783968910233\n",
            "--------------------------------------------------\n",
            "iter: 916 \ttrain loss: 0.0009485915858092342\n",
            "--------------------------------------------------\n",
            "iter: 917 \ttrain loss: 0.0011781187814539917\n",
            "--------------------------------------------------\n",
            "iter: 918 \ttrain loss: 0.001198359553791461\n",
            "--------------------------------------------------\n",
            "iter: 919 \ttrain loss: 0.000991671558774778\n",
            "--------------------------------------------------\n",
            "iter: 920 \ttrain loss: 0.001173004997686635\n",
            "--------------------------------------------------\n",
            "iter: 921 \ttrain loss: 0.0010993726803068334\n",
            "--------------------------------------------------\n",
            "iter: 922 \ttrain loss: 0.0010436645646308925\n",
            "--------------------------------------------------\n",
            "iter: 923 \ttrain loss: 0.0013955695816524648\n",
            "--------------------------------------------------\n",
            "iter: 924 \ttrain loss: 0.001046024771460069\n",
            "--------------------------------------------------\n",
            "iter: 925 \ttrain loss: 0.0010363338937854052\n",
            "--------------------------------------------------\n",
            "iter: 926 \ttrain loss: 0.001536177635583026\n",
            "--------------------------------------------------\n",
            "iter: 927 \ttrain loss: 0.0010618295272383166\n",
            "--------------------------------------------------\n",
            "iter: 928 \ttrain loss: 0.0011285256708967286\n",
            "--------------------------------------------------\n",
            "iter: 929 \ttrain loss: 0.0009712358215637984\n",
            "--------------------------------------------------\n",
            "iter: 930 \ttrain loss: 0.001274885842412942\n",
            "--------------------------------------------------\n",
            "iter: 931 \ttrain loss: 0.0011697026953718917\n",
            "--------------------------------------------------\n",
            "iter: 932 \ttrain loss: 0.0011042373016490674\n",
            "--------------------------------------------------\n",
            "iter: 933 \ttrain loss: 0.001095563942960018\n",
            "--------------------------------------------------\n",
            "iter: 934 \ttrain loss: 0.0011310159041459727\n",
            "--------------------------------------------------\n",
            "iter: 935 \ttrain loss: 0.0009634659572782052\n",
            "--------------------------------------------------\n",
            "iter: 936 \ttrain loss: 0.0010185326311660406\n",
            "--------------------------------------------------\n",
            "iter: 937 \ttrain loss: 0.0010894222328634085\n",
            "--------------------------------------------------\n",
            "iter: 938 \ttrain loss: 0.001184607043415713\n",
            "--------------------------------------------------\n",
            "iter: 939 \ttrain loss: 0.0011250786475948162\n",
            "--------------------------------------------------\n",
            "iter: 940 \ttrain loss: 0.001102997708100579\n",
            "--------------------------------------------------\n",
            "iter: 941 \ttrain loss: 0.0011070329185711284\n",
            "--------------------------------------------------\n",
            "iter: 942 \ttrain loss: 0.000991789934298335\n",
            "--------------------------------------------------\n",
            "iter: 943 \ttrain loss: 0.0010597574202147807\n",
            "--------------------------------------------------\n",
            "iter: 944 \ttrain loss: 0.001083776796978453\n",
            "--------------------------------------------------\n",
            "iter: 945 \ttrain loss: 0.0010681436922624773\n",
            "--------------------------------------------------\n",
            "iter: 946 \ttrain loss: 0.0011152225579352528\n",
            "--------------------------------------------------\n",
            "iter: 947 \ttrain loss: 0.0011734421444274555\n",
            "--------------------------------------------------\n",
            "iter: 948 \ttrain loss: 0.0010271368523713683\n",
            "--------------------------------------------------\n",
            "iter: 949 \ttrain loss: 0.0011564923508196828\n",
            "--------------------------------------------------\n",
            "iter: 950 \ttrain loss: 0.0011420972798405421\n",
            "--------------------------------------------------\n",
            "iter: 951 \ttrain loss: 0.0011509730608592348\n",
            "--------------------------------------------------\n",
            "iter: 952 \ttrain loss: 0.0008950255743469211\n",
            "--------------------------------------------------\n",
            "iter: 953 \ttrain loss: 0.001100596392797672\n",
            "--------------------------------------------------\n",
            "iter: 954 \ttrain loss: 0.001274784365978613\n",
            "--------------------------------------------------\n",
            "iter: 955 \ttrain loss: 0.0009686279084334704\n",
            "--------------------------------------------------\n",
            "iter: 956 \ttrain loss: 0.0011501178520956182\n",
            "--------------------------------------------------\n",
            "iter: 957 \ttrain loss: 0.0010454247975136193\n",
            "--------------------------------------------------\n",
            "iter: 958 \ttrain loss: 0.0011462172596132228\n",
            "--------------------------------------------------\n",
            "iter: 959 \ttrain loss: 0.0011897548506411605\n",
            "--------------------------------------------------\n",
            "iter: 960 \ttrain loss: 0.0010702312804358842\n",
            "--------------------------------------------------\n",
            "iter: 961 \ttrain loss: 0.001185083119924334\n",
            "--------------------------------------------------\n",
            "iter: 962 \ttrain loss: 0.000979109104569592\n",
            "--------------------------------------------------\n",
            "iter: 963 \ttrain loss: 0.0012465426455218738\n",
            "--------------------------------------------------\n",
            "iter: 964 \ttrain loss: 0.0009962718400402632\n",
            "--------------------------------------------------\n",
            "iter: 965 \ttrain loss: 0.0011369128414515433\n",
            "--------------------------------------------------\n",
            "iter: 966 \ttrain loss: 0.0010432340751379712\n",
            "--------------------------------------------------\n",
            "iter: 967 \ttrain loss: 0.0010962630778880726\n",
            "--------------------------------------------------\n",
            "iter: 968 \ttrain loss: 0.0010962748712578635\n",
            "--------------------------------------------------\n",
            "iter: 969 \ttrain loss: 0.001101646728840337\n",
            "--------------------------------------------------\n",
            "iter: 970 \ttrain loss: 0.0010555364243037474\n",
            "--------------------------------------------------\n",
            "iter: 971 \ttrain loss: 0.0010847264204063406\n",
            "--------------------------------------------------\n",
            "iter: 972 \ttrain loss: 0.0009643796286929054\n",
            "--------------------------------------------------\n",
            "iter: 973 \ttrain loss: 0.0011377194944383274\n",
            "--------------------------------------------------\n",
            "iter: 974 \ttrain loss: 0.0010592026799962912\n",
            "--------------------------------------------------\n",
            "iter: 975 \ttrain loss: 0.001430735141945494\n",
            "--------------------------------------------------\n",
            "iter: 976 \ttrain loss: 0.000987927538899308\n",
            "--------------------------------------------------\n",
            "iter: 977 \ttrain loss: 0.0013061655756010542\n",
            "--------------------------------------------------\n",
            "iter: 978 \ttrain loss: 0.0010436249382256213\n",
            "--------------------------------------------------\n",
            "iter: 979 \ttrain loss: 0.0010450210489257033\n",
            "--------------------------------------------------\n",
            "iter: 980 \ttrain loss: 0.0011498105794409024\n",
            "--------------------------------------------------\n",
            "iter: 981 \ttrain loss: 0.001107911295578964\n",
            "--------------------------------------------------\n",
            "iter: 982 \ttrain loss: 0.0009636667408121708\n",
            "--------------------------------------------------\n",
            "iter: 983 \ttrain loss: 0.0012550189121539122\n",
            "--------------------------------------------------\n",
            "iter: 984 \ttrain loss: 0.0009471011712406994\n",
            "--------------------------------------------------\n",
            "iter: 985 \ttrain loss: 0.0011272567885116137\n",
            "--------------------------------------------------\n",
            "iter: 986 \ttrain loss: 0.001142703018396492\n",
            "--------------------------------------------------\n",
            "iter: 987 \ttrain loss: 0.0009889720770637973\n",
            "--------------------------------------------------\n",
            "iter: 988 \ttrain loss: 0.001249652618805293\n",
            "--------------------------------------------------\n",
            "iter: 989 \ttrain loss: 0.0010804568320087972\n",
            "--------------------------------------------------\n",
            "iter: 990 \ttrain loss: 0.0011124248357432915\n",
            "--------------------------------------------------\n",
            "iter: 991 \ttrain loss: 0.0011242628704737122\n",
            "--------------------------------------------------\n",
            "iter: 992 \ttrain loss: 0.0010474957627948282\n",
            "--------------------------------------------------\n",
            "iter: 993 \ttrain loss: 0.0010632062750106932\n",
            "--------------------------------------------------\n",
            "iter: 994 \ttrain loss: 0.0010675950816455768\n",
            "--------------------------------------------------\n",
            "iter: 995 \ttrain loss: 0.0010477056126328994\n",
            "--------------------------------------------------\n",
            "iter: 996 \ttrain loss: 0.001480093901036224\n",
            "--------------------------------------------------\n",
            "iter: 997 \ttrain loss: 0.0011335433372465314\n",
            "--------------------------------------------------\n",
            "iter: 998 \ttrain loss: 0.0010844171707588676\n",
            "--------------------------------------------------\n",
            "iter: 999 \ttrain loss: 0.0011238985725471181\n",
            "--------------------------------------------------\n",
            "iter: 1000 \ttrain loss: 0.001333459375369725\n",
            "--------------------------------------------------\n",
            "iter: 1001 \ttrain loss: 0.0011510923750810524\n",
            "--------------------------------------------------\n",
            "iter: 1002 \ttrain loss: 0.0010610795875088353\n",
            "--------------------------------------------------\n",
            "iter: 1003 \ttrain loss: 0.001415081790447948\n",
            "--------------------------------------------------\n",
            "iter: 1004 \ttrain loss: 0.0009844059253503702\n",
            "--------------------------------------------------\n",
            "iter: 1005 \ttrain loss: 0.001341663162367146\n",
            "--------------------------------------------------\n",
            "iter: 1006 \ttrain loss: 0.0010895919310102112\n",
            "--------------------------------------------------\n",
            "iter: 1007 \ttrain loss: 0.0011237887108844881\n",
            "--------------------------------------------------\n",
            "iter: 1008 \ttrain loss: 0.0009643895537983113\n",
            "--------------------------------------------------\n",
            "iter: 1009 \ttrain loss: 0.001179326207121025\n",
            "--------------------------------------------------\n",
            "iter: 1010 \ttrain loss: 0.0011113809390480831\n",
            "--------------------------------------------------\n",
            "iter: 1011 \ttrain loss: 0.0011536915068450602\n",
            "--------------------------------------------------\n",
            "iter: 1012 \ttrain loss: 0.00104336873670143\n",
            "--------------------------------------------------\n",
            "iter: 1013 \ttrain loss: 0.0011067169177500446\n",
            "--------------------------------------------------\n",
            "iter: 1014 \ttrain loss: 0.0011080665940307332\n",
            "--------------------------------------------------\n",
            "iter: 1015 \ttrain loss: 0.0011286807822170196\n",
            "--------------------------------------------------\n",
            "iter: 1016 \ttrain loss: 0.0011315553212011959\n",
            "--------------------------------------------------\n",
            "iter: 1017 \ttrain loss: 0.001092613827203794\n",
            "--------------------------------------------------\n",
            "iter: 1018 \ttrain loss: 0.0010398740810037203\n",
            "--------------------------------------------------\n",
            "iter: 1019 \ttrain loss: 0.001152914034963075\n",
            "--------------------------------------------------\n",
            "iter: 1020 \ttrain loss: 0.0010009877188198574\n",
            "--------------------------------------------------\n",
            "iter: 1021 \ttrain loss: 0.0011444585937533962\n",
            "--------------------------------------------------\n",
            "iter: 1022 \ttrain loss: 0.0011081535255676033\n",
            "--------------------------------------------------\n",
            "iter: 1023 \ttrain loss: 0.0010297289321800922\n",
            "--------------------------------------------------\n",
            "iter: 1024 \ttrain loss: 0.0009797104748557632\n",
            "--------------------------------------------------\n",
            "iter: 1025 \ttrain loss: 0.0011652011081578044\n",
            "--------------------------------------------------\n",
            "iter: 1026 \ttrain loss: 0.0010188775898665795\n",
            "--------------------------------------------------\n",
            "iter: 1027 \ttrain loss: 0.0010086272968932474\n",
            "--------------------------------------------------\n",
            "iter: 1028 \ttrain loss: 0.0014933571442871572\n",
            "--------------------------------------------------\n",
            "iter: 1029 \ttrain loss: 0.0010259358588019865\n",
            "--------------------------------------------------\n",
            "iter: 1030 \ttrain loss: 0.0011252758126177936\n",
            "--------------------------------------------------\n",
            "iter: 1031 \ttrain loss: 0.0012386625256702128\n",
            "--------------------------------------------------\n",
            "iter: 1032 \ttrain loss: 0.0011116854342492282\n",
            "--------------------------------------------------\n",
            "iter: 1033 \ttrain loss: 0.0010251114374280985\n",
            "--------------------------------------------------\n",
            "iter: 1034 \ttrain loss: 0.0010844279466142722\n",
            "--------------------------------------------------\n",
            "iter: 1035 \ttrain loss: 0.0010585510159406369\n",
            "--------------------------------------------------\n",
            "iter: 1036 \ttrain loss: 0.0010978223393754\n",
            "--------------------------------------------------\n",
            "iter: 1037 \ttrain loss: 0.0011016796142148378\n",
            "--------------------------------------------------\n",
            "iter: 1038 \ttrain loss: 0.00106286699199356\n",
            "--------------------------------------------------\n",
            "iter: 1039 \ttrain loss: 0.0010282177989768251\n",
            "--------------------------------------------------\n",
            "iter: 1040 \ttrain loss: 0.0011655680093453304\n",
            "--------------------------------------------------\n",
            "iter: 1041 \ttrain loss: 0.0010429569639643216\n",
            "--------------------------------------------------\n",
            "iter: 1042 \ttrain loss: 0.0010267995604273847\n",
            "--------------------------------------------------\n",
            "iter: 1043 \ttrain loss: 0.0011154700023827304\n",
            "--------------------------------------------------\n",
            "iter: 1044 \ttrain loss: 0.001035234326608483\n",
            "--------------------------------------------------\n",
            "iter: 1045 \ttrain loss: 0.001247132751944677\n",
            "--------------------------------------------------\n",
            "iter: 1046 \ttrain loss: 0.001007499324250998\n",
            "--------------------------------------------------\n",
            "iter: 1047 \ttrain loss: 0.001180917899390411\n",
            "--------------------------------------------------\n",
            "iter: 1048 \ttrain loss: 0.0011057516512553002\n",
            "--------------------------------------------------\n",
            "iter: 1049 \ttrain loss: 0.0010506509924302974\n",
            "--------------------------------------------------\n",
            "iter: 1050 \ttrain loss: 0.0009346109123484539\n",
            "--------------------------------------------------\n",
            "iter: 1051 \ttrain loss: 0.0011512774200097377\n",
            "--------------------------------------------------\n",
            "iter: 1052 \ttrain loss: 0.0010756285346561114\n",
            "--------------------------------------------------\n",
            "iter: 1053 \ttrain loss: 0.0010126218621820291\n",
            "--------------------------------------------------\n",
            "iter: 1054 \ttrain loss: 0.0015569246184916898\n",
            "--------------------------------------------------\n",
            "iter: 1055 \ttrain loss: 0.000983814607220522\n",
            "--------------------------------------------------\n",
            "iter: 1056 \ttrain loss: 0.0011229622406995527\n",
            "--------------------------------------------------\n",
            "iter: 1057 \ttrain loss: 0.0010815327495064643\n",
            "--------------------------------------------------\n",
            "iter: 1058 \ttrain loss: 0.0011788209591947164\n",
            "--------------------------------------------------\n",
            "iter: 1059 \ttrain loss: 0.001074344079468301\n",
            "--------------------------------------------------\n",
            "iter: 1060 \ttrain loss: 0.0010385396900154196\n",
            "--------------------------------------------------\n",
            "iter: 1061 \ttrain loss: 0.0012758717821092832\n",
            "--------------------------------------------------\n",
            "iter: 1062 \ttrain loss: 0.001021113055215926\n",
            "--------------------------------------------------\n",
            "iter: 1063 \ttrain loss: 0.0012586494663583958\n",
            "--------------------------------------------------\n",
            "iter: 1064 \ttrain loss: 0.0010230756734096581\n",
            "--------------------------------------------------\n",
            "iter: 1065 \ttrain loss: 0.001024611758357793\n",
            "--------------------------------------------------\n",
            "iter: 1066 \ttrain loss: 0.000997207320961311\n",
            "--------------------------------------------------\n",
            "iter: 1067 \ttrain loss: 0.0010714703013363846\n",
            "--------------------------------------------------\n",
            "iter: 1068 \ttrain loss: 0.0010237992425679821\n",
            "--------------------------------------------------\n",
            "iter: 1069 \ttrain loss: 0.0010735932067321959\n",
            "--------------------------------------------------\n",
            "iter: 1070 \ttrain loss: 0.0011807619397072402\n",
            "--------------------------------------------------\n",
            "iter: 1071 \ttrain loss: 0.0009387810929823614\n",
            "--------------------------------------------------\n",
            "iter: 1072 \ttrain loss: 0.0012012095164118353\n",
            "--------------------------------------------------\n",
            "iter: 1073 \ttrain loss: 0.0010533941999074754\n",
            "--------------------------------------------------\n",
            "iter: 1074 \ttrain loss: 0.001146348512016842\n",
            "--------------------------------------------------\n",
            "iter: 1075 \ttrain loss: 0.0011165467196143741\n",
            "--------------------------------------------------\n",
            "iter: 1076 \ttrain loss: 0.0010821876371423624\n",
            "--------------------------------------------------\n",
            "iter: 1077 \ttrain loss: 0.0011312921888202944\n",
            "--------------------------------------------------\n",
            "iter: 1078 \ttrain loss: 0.000982275580961575\n",
            "--------------------------------------------------\n",
            "iter: 1079 \ttrain loss: 0.0013773686262697012\n",
            "--------------------------------------------------\n",
            "iter: 1080 \ttrain loss: 0.0011552828370822114\n",
            "--------------------------------------------------\n",
            "iter: 1081 \ttrain loss: 0.0009928833048490412\n",
            "--------------------------------------------------\n",
            "iter: 1082 \ttrain loss: 0.0010719777901112828\n",
            "--------------------------------------------------\n",
            "iter: 1083 \ttrain loss: 0.0010624658410901815\n",
            "--------------------------------------------------\n",
            "iter: 1084 \ttrain loss: 0.0010970740495795683\n",
            "--------------------------------------------------\n",
            "iter: 1085 \ttrain loss: 0.0010505502156192064\n",
            "--------------------------------------------------\n",
            "iter: 1086 \ttrain loss: 0.001064815157918249\n",
            "--------------------------------------------------\n",
            "iter: 1087 \ttrain loss: 0.0011134911388987684\n",
            "--------------------------------------------------\n",
            "iter: 1088 \ttrain loss: 0.001032199644817258\n",
            "--------------------------------------------------\n",
            "iter: 1089 \ttrain loss: 0.0011463982346251985\n",
            "--------------------------------------------------\n",
            "iter: 1090 \ttrain loss: 0.000985411914765383\n",
            "--------------------------------------------------\n",
            "iter: 1091 \ttrain loss: 0.0011161585164452773\n",
            "--------------------------------------------------\n",
            "iter: 1092 \ttrain loss: 0.001011245330908035\n",
            "--------------------------------------------------\n",
            "iter: 1093 \ttrain loss: 0.0012870227734693358\n",
            "--------------------------------------------------\n",
            "iter: 1094 \ttrain loss: 0.0009177977437850694\n",
            "--------------------------------------------------\n",
            "iter: 1095 \ttrain loss: 0.001095807791704354\n",
            "--------------------------------------------------\n",
            "iter: 1096 \ttrain loss: 0.0011862758137043783\n",
            "--------------------------------------------------\n",
            "iter: 1097 \ttrain loss: 0.001073225285286258\n",
            "--------------------------------------------------\n",
            "iter: 1098 \ttrain loss: 0.001093147400333192\n",
            "--------------------------------------------------\n",
            "iter: 1099 \ttrain loss: 0.0011577549208371392\n",
            "--------------------------------------------------\n",
            "iter: 1100 \ttrain loss: 0.0009828697629800517\n",
            "--------------------------------------------------\n",
            "iter: 1101 \ttrain loss: 0.0010583537153196028\n",
            "--------------------------------------------------\n",
            "iter: 1102 \ttrain loss: 0.00125264183047537\n",
            "--------------------------------------------------\n",
            "iter: 1103 \ttrain loss: 0.0010468570797182426\n",
            "--------------------------------------------------\n",
            "iter: 1104 \ttrain loss: 0.0011488074048553332\n",
            "--------------------------------------------------\n",
            "iter: 1105 \ttrain loss: 0.0011020672008727886\n",
            "--------------------------------------------------\n",
            "iter: 1106 \ttrain loss: 0.0011045402638079347\n",
            "--------------------------------------------------\n",
            "iter: 1107 \ttrain loss: 0.0011297312548622534\n",
            "--------------------------------------------------\n",
            "iter: 1108 \ttrain loss: 0.001013196478086524\n",
            "--------------------------------------------------\n",
            "iter: 1109 \ttrain loss: 0.0010714130663935693\n",
            "--------------------------------------------------\n",
            "iter: 1110 \ttrain loss: 0.00111223175466319\n",
            "--------------------------------------------------\n",
            "iter: 1111 \ttrain loss: 0.0010738069418357763\n",
            "--------------------------------------------------\n",
            "iter: 1112 \ttrain loss: 0.0009502727918389345\n",
            "--------------------------------------------------\n",
            "iter: 1113 \ttrain loss: 0.0012719755372439362\n",
            "--------------------------------------------------\n",
            "iter: 1114 \ttrain loss: 0.001127810291813644\n",
            "--------------------------------------------------\n",
            "iter: 1115 \ttrain loss: 0.0011082174851749654\n",
            "--------------------------------------------------\n",
            "iter: 1116 \ttrain loss: 0.001059534370427463\n",
            "--------------------------------------------------\n",
            "iter: 1117 \ttrain loss: 0.0010710935550590805\n",
            "--------------------------------------------------\n",
            "iter: 1118 \ttrain loss: 0.0010841788300389628\n",
            "--------------------------------------------------\n",
            "iter: 1119 \ttrain loss: 0.001149815092930339\n",
            "--------------------------------------------------\n",
            "iter: 1120 \ttrain loss: 0.001034787569416855\n",
            "--------------------------------------------------\n",
            "iter: 1121 \ttrain loss: 0.0011180603607553322\n",
            "--------------------------------------------------\n",
            "iter: 1122 \ttrain loss: 0.0010159876144232953\n",
            "--------------------------------------------------\n",
            "iter: 1123 \ttrain loss: 0.0012242076436989099\n",
            "--------------------------------------------------\n",
            "iter: 1124 \ttrain loss: 0.0009653288366169645\n",
            "--------------------------------------------------\n",
            "iter: 1125 \ttrain loss: 0.0010305418636260937\n",
            "--------------------------------------------------\n",
            "iter: 1126 \ttrain loss: 0.0011014897132923384\n",
            "--------------------------------------------------\n",
            "iter: 1127 \ttrain loss: 0.001079561818878236\n",
            "--------------------------------------------------\n",
            "iter: 1128 \ttrain loss: 0.0010833187906049793\n",
            "--------------------------------------------------\n",
            "iter: 1129 \ttrain loss: 0.0010844757639305069\n",
            "--------------------------------------------------\n",
            "iter: 1130 \ttrain loss: 0.0010775759712955255\n",
            "--------------------------------------------------\n",
            "iter: 1131 \ttrain loss: 0.0010320099558257649\n",
            "--------------------------------------------------\n",
            "iter: 1132 \ttrain loss: 0.0010304272251016123\n",
            "--------------------------------------------------\n",
            "iter: 1133 \ttrain loss: 0.0012123590347980374\n",
            "--------------------------------------------------\n",
            "iter: 1134 \ttrain loss: 0.001030547791295809\n",
            "--------------------------------------------------\n",
            "iter: 1135 \ttrain loss: 0.001070145621640592\n",
            "--------------------------------------------------\n",
            "iter: 1136 \ttrain loss: 0.0011766586835785563\n",
            "--------------------------------------------------\n",
            "iter: 1137 \ttrain loss: 0.001116273065776579\n",
            "--------------------------------------------------\n",
            "iter: 1138 \ttrain loss: 0.0009444915344578448\n",
            "--------------------------------------------------\n",
            "iter: 1139 \ttrain loss: 0.0011929898099739206\n",
            "--------------------------------------------------\n",
            "iter: 1140 \ttrain loss: 0.0011600098380181055\n",
            "--------------------------------------------------\n",
            "iter: 1141 \ttrain loss: 0.0011922480325510477\n",
            "--------------------------------------------------\n",
            "iter: 1142 \ttrain loss: 0.0009769417317130773\n",
            "--------------------------------------------------\n",
            "iter: 1143 \ttrain loss: 0.0012990612221615676\n",
            "--------------------------------------------------\n",
            "iter: 1144 \ttrain loss: 0.001053563213888975\n",
            "--------------------------------------------------\n",
            "iter: 1145 \ttrain loss: 0.001451158253677487\n",
            "--------------------------------------------------\n",
            "iter: 1146 \ttrain loss: 0.0011983537947072498\n",
            "--------------------------------------------------\n",
            "iter: 1147 \ttrain loss: 0.0009461500480223786\n",
            "--------------------------------------------------\n",
            "iter: 1148 \ttrain loss: 0.0010890707687456604\n",
            "--------------------------------------------------\n",
            "iter: 1149 \ttrain loss: 0.0011288189340074506\n",
            "--------------------------------------------------\n",
            "iter: 1150 \ttrain loss: 0.0010782233168656968\n",
            "--------------------------------------------------\n",
            "iter: 1151 \ttrain loss: 0.0011025267832303585\n",
            "--------------------------------------------------\n",
            "iter: 1152 \ttrain loss: 0.001111590927702786\n",
            "--------------------------------------------------\n",
            "iter: 1153 \ttrain loss: 0.001071680142818262\n",
            "--------------------------------------------------\n",
            "iter: 1154 \ttrain loss: 0.0010474305660538444\n",
            "--------------------------------------------------\n",
            "iter: 1155 \ttrain loss: 0.001059635159575561\n",
            "--------------------------------------------------\n",
            "iter: 1156 \ttrain loss: 0.0012800431331332956\n",
            "--------------------------------------------------\n",
            "iter: 1157 \ttrain loss: 0.0010083155161380662\n",
            "--------------------------------------------------\n",
            "iter: 1158 \ttrain loss: 0.0011751218653679447\n",
            "--------------------------------------------------\n",
            "iter: 1159 \ttrain loss: 0.0011129982496136752\n",
            "--------------------------------------------------\n",
            "iter: 1160 \ttrain loss: 0.0010766130853650615\n",
            "--------------------------------------------------\n",
            "iter: 1161 \ttrain loss: 0.001126218074777479\n",
            "--------------------------------------------------\n",
            "iter: 1162 \ttrain loss: 0.0010808860835109146\n",
            "--------------------------------------------------\n",
            "iter: 1163 \ttrain loss: 0.001201635655765738\n",
            "--------------------------------------------------\n",
            "iter: 1164 \ttrain loss: 0.001205300772354732\n",
            "--------------------------------------------------\n",
            "iter: 1165 \ttrain loss: 0.0011641043131340807\n",
            "--------------------------------------------------\n",
            "iter: 1166 \ttrain loss: 0.0010777568407477432\n",
            "--------------------------------------------------\n",
            "iter: 1167 \ttrain loss: 0.001033921464149842\n",
            "--------------------------------------------------\n",
            "iter: 1168 \ttrain loss: 0.0010107663349532311\n",
            "--------------------------------------------------\n",
            "iter: 1169 \ttrain loss: 0.0011399480794476483\n",
            "--------------------------------------------------\n",
            "iter: 1170 \ttrain loss: 0.0009792654116905616\n",
            "--------------------------------------------------\n",
            "iter: 1171 \ttrain loss: 0.0011102285651737264\n",
            "--------------------------------------------------\n",
            "iter: 1172 \ttrain loss: 0.001146309596039868\n",
            "--------------------------------------------------\n",
            "iter: 1173 \ttrain loss: 0.0011302933795679085\n",
            "--------------------------------------------------\n",
            "iter: 1174 \ttrain loss: 0.0010825407151497748\n",
            "--------------------------------------------------\n",
            "iter: 1175 \ttrain loss: 0.0010756561791404912\n",
            "--------------------------------------------------\n",
            "iter: 1176 \ttrain loss: 0.001103211957896832\n",
            "--------------------------------------------------\n",
            "iter: 1177 \ttrain loss: 0.0010903024490882306\n",
            "--------------------------------------------------\n",
            "iter: 1178 \ttrain loss: 0.0010740506810733002\n",
            "--------------------------------------------------\n",
            "iter: 1179 \ttrain loss: 0.0011441344887662861\n",
            "--------------------------------------------------\n",
            "iter: 1180 \ttrain loss: 0.0011471263607928204\n",
            "--------------------------------------------------\n",
            "iter: 1181 \ttrain loss: 0.0010349459087524082\n",
            "--------------------------------------------------\n",
            "iter: 1182 \ttrain loss: 0.0010407668482850096\n",
            "--------------------------------------------------\n",
            "iter: 1183 \ttrain loss: 0.0010507486236758584\n",
            "--------------------------------------------------\n",
            "iter: 1184 \ttrain loss: 0.0011475198315147204\n",
            "--------------------------------------------------\n",
            "iter: 1185 \ttrain loss: 0.0010203303963408883\n",
            "--------------------------------------------------\n",
            "iter: 1186 \ttrain loss: 0.0009109052423153945\n",
            "--------------------------------------------------\n",
            "iter: 1187 \ttrain loss: 0.0010257760091575338\n",
            "--------------------------------------------------\n",
            "iter: 1188 \ttrain loss: 0.001206214259310401\n",
            "--------------------------------------------------\n",
            "iter: 1189 \ttrain loss: 0.001205023271966809\n",
            "--------------------------------------------------\n",
            "iter: 1190 \ttrain loss: 0.0010076135150868285\n",
            "--------------------------------------------------\n",
            "iter: 1191 \ttrain loss: 0.0011206573179568715\n",
            "--------------------------------------------------\n",
            "iter: 1192 \ttrain loss: 0.0012637319801375518\n",
            "--------------------------------------------------\n",
            "iter: 1193 \ttrain loss: 0.0011104038221138033\n",
            "--------------------------------------------------\n",
            "iter: 1194 \ttrain loss: 0.0010320794596418838\n",
            "--------------------------------------------------\n",
            "iter: 1195 \ttrain loss: 0.001010853049311939\n",
            "--------------------------------------------------\n",
            "iter: 1196 \ttrain loss: 0.0010640337851342122\n",
            "--------------------------------------------------\n",
            "iter: 1197 \ttrain loss: 0.0010769066299710984\n",
            "--------------------------------------------------\n",
            "iter: 1198 \ttrain loss: 0.0010710283858356572\n",
            "--------------------------------------------------\n",
            "iter: 1199 \ttrain loss: 0.0010931532941246631\n",
            "--------------------------------------------------\n",
            "iter: 1200 \ttrain loss: 0.0010361217387743855\n",
            "--------------------------------------------------\n",
            "iter: 1201 \ttrain loss: 0.0011176822803362805\n",
            "--------------------------------------------------\n",
            "iter: 1202 \ttrain loss: 0.000981131414955563\n",
            "--------------------------------------------------\n",
            "iter: 1203 \ttrain loss: 0.0010417809542309503\n",
            "--------------------------------------------------\n",
            "iter: 1204 \ttrain loss: 0.0011246148831862352\n",
            "--------------------------------------------------\n",
            "iter: 1205 \ttrain loss: 0.0010599634724240174\n",
            "--------------------------------------------------\n",
            "iter: 1206 \ttrain loss: 0.0009690407556263364\n",
            "--------------------------------------------------\n",
            "iter: 1207 \ttrain loss: 0.0012228243403984128\n",
            "--------------------------------------------------\n",
            "iter: 1208 \ttrain loss: 0.001111784653707656\n",
            "--------------------------------------------------\n",
            "iter: 1209 \ttrain loss: 0.0009037984540724782\n",
            "--------------------------------------------------\n",
            "iter: 1210 \ttrain loss: 0.001190080198725674\n",
            "--------------------------------------------------\n",
            "iter: 1211 \ttrain loss: 0.0011326861896438893\n",
            "--------------------------------------------------\n",
            "iter: 1212 \ttrain loss: 0.0010532432105650425\n",
            "--------------------------------------------------\n",
            "iter: 1213 \ttrain loss: 0.0010642164818104108\n",
            "--------------------------------------------------\n",
            "iter: 1214 \ttrain loss: 0.0010638922617838135\n",
            "--------------------------------------------------\n",
            "iter: 1215 \ttrain loss: 0.0010812091347912227\n",
            "--------------------------------------------------\n",
            "iter: 1216 \ttrain loss: 0.0010791388063869122\n",
            "--------------------------------------------------\n",
            "iter: 1217 \ttrain loss: 0.0011130862289457047\n",
            "--------------------------------------------------\n",
            "iter: 1218 \ttrain loss: 0.0014878027984845143\n",
            "--------------------------------------------------\n",
            "iter: 1219 \ttrain loss: 0.0010729753151751539\n",
            "--------------------------------------------------\n",
            "iter: 1220 \ttrain loss: 0.0011414994786828779\n",
            "--------------------------------------------------\n",
            "iter: 1221 \ttrain loss: 0.001153181175760855\n",
            "--------------------------------------------------\n",
            "iter: 1222 \ttrain loss: 0.0011685621028061754\n",
            "--------------------------------------------------\n",
            "iter: 1223 \ttrain loss: 0.001016806848887195\n",
            "--------------------------------------------------\n",
            "iter: 1224 \ttrain loss: 0.001256949283675904\n",
            "--------------------------------------------------\n",
            "iter: 1225 \ttrain loss: 0.001130167187224847\n",
            "--------------------------------------------------\n",
            "iter: 1226 \ttrain loss: 0.001120414464593056\n",
            "--------------------------------------------------\n",
            "iter: 1227 \ttrain loss: 0.0010596143687677583\n",
            "--------------------------------------------------\n",
            "iter: 1228 \ttrain loss: 0.0011790818279370876\n",
            "--------------------------------------------------\n",
            "iter: 1229 \ttrain loss: 0.0009701800707411865\n",
            "--------------------------------------------------\n",
            "iter: 1230 \ttrain loss: 0.001073360513414891\n",
            "--------------------------------------------------\n",
            "iter: 1231 \ttrain loss: 0.0011596040658790477\n",
            "--------------------------------------------------\n",
            "iter: 1232 \ttrain loss: 0.0011100157117262587\n",
            "--------------------------------------------------\n",
            "iter: 1233 \ttrain loss: 0.0013508804881046668\n",
            "--------------------------------------------------\n",
            "iter: 1234 \ttrain loss: 0.0009797557695138978\n",
            "--------------------------------------------------\n",
            "iter: 1235 \ttrain loss: 0.0011205051181003179\n",
            "--------------------------------------------------\n",
            "iter: 1236 \ttrain loss: 0.0010432488226639017\n",
            "--------------------------------------------------\n",
            "iter: 1237 \ttrain loss: 0.0010653561142808159\n",
            "--------------------------------------------------\n",
            "iter: 1238 \ttrain loss: 0.0011138746653824672\n",
            "--------------------------------------------------\n",
            "iter: 1239 \ttrain loss: 0.0011712744280507588\n",
            "--------------------------------------------------\n",
            "iter: 1240 \ttrain loss: 0.0012007589159954636\n",
            "--------------------------------------------------\n",
            "iter: 1241 \ttrain loss: 0.0011159086060425086\n",
            "--------------------------------------------------\n",
            "iter: 1242 \ttrain loss: 0.000996235097661574\n",
            "--------------------------------------------------\n",
            "iter: 1243 \ttrain loss: 0.0010960161844629493\n",
            "--------------------------------------------------\n",
            "iter: 1244 \ttrain loss: 0.0010914927101584265\n",
            "--------------------------------------------------\n",
            "iter: 1245 \ttrain loss: 0.0011489197463340466\n",
            "--------------------------------------------------\n",
            "iter: 1246 \ttrain loss: 0.0009825421784324378\n",
            "--------------------------------------------------\n",
            "iter: 1247 \ttrain loss: 0.0010711860300349164\n",
            "--------------------------------------------------\n",
            "iter: 1248 \ttrain loss: 0.0009807078122107602\n",
            "--------------------------------------------------\n",
            "iter: 1249 \ttrain loss: 0.0015811705258429259\n",
            "--------------------------------------------------\n",
            "iter: 1250 \ttrain loss: 0.0009860272779780301\n",
            "--------------------------------------------------\n",
            "iter: 1251 \ttrain loss: 0.0012057902264650045\n",
            "--------------------------------------------------\n",
            "iter: 1252 \ttrain loss: 0.0009741413932940837\n",
            "--------------------------------------------------\n",
            "iter: 1253 \ttrain loss: 0.001102547075999896\n",
            "--------------------------------------------------\n",
            "iter: 1254 \ttrain loss: 0.0011106903933900962\n",
            "--------------------------------------------------\n",
            "iter: 1255 \ttrain loss: 0.0010355338675922788\n",
            "--------------------------------------------------\n",
            "iter: 1256 \ttrain loss: 0.0011455187719151816\n",
            "--------------------------------------------------\n",
            "iter: 1257 \ttrain loss: 0.0012518132189791138\n",
            "--------------------------------------------------\n",
            "iter: 1258 \ttrain loss: 0.0011385267577902678\n",
            "--------------------------------------------------\n",
            "iter: 1259 \ttrain loss: 0.0009689881260005119\n",
            "--------------------------------------------------\n",
            "iter: 1260 \ttrain loss: 0.0011223555660168833\n",
            "--------------------------------------------------\n",
            "iter: 1261 \ttrain loss: 0.0010242624185122467\n",
            "--------------------------------------------------\n",
            "iter: 1262 \ttrain loss: 0.0011643692306076304\n",
            "--------------------------------------------------\n",
            "iter: 1263 \ttrain loss: 0.0010232027795932122\n",
            "--------------------------------------------------\n",
            "iter: 1264 \ttrain loss: 0.0011357029836857636\n",
            "--------------------------------------------------\n",
            "iter: 1265 \ttrain loss: 0.0010310608863494917\n",
            "--------------------------------------------------\n",
            "iter: 1266 \ttrain loss: 0.0010834923287186137\n",
            "--------------------------------------------------\n",
            "iter: 1267 \ttrain loss: 0.0010870516404015407\n",
            "--------------------------------------------------\n",
            "iter: 1268 \ttrain loss: 0.0010050974355410656\n",
            "--------------------------------------------------\n",
            "iter: 1269 \ttrain loss: 0.0011111931815583878\n",
            "--------------------------------------------------\n",
            "iter: 1270 \ttrain loss: 0.0010850613999389606\n",
            "--------------------------------------------------\n",
            "iter: 1271 \ttrain loss: 0.0010985748621680632\n",
            "--------------------------------------------------\n",
            "iter: 1272 \ttrain loss: 0.0012147344883098122\n",
            "--------------------------------------------------\n",
            "iter: 1273 \ttrain loss: 0.0011663274526060089\n",
            "--------------------------------------------------\n",
            "iter: 1274 \ttrain loss: 0.0009375927976872797\n",
            "--------------------------------------------------\n",
            "iter: 1275 \ttrain loss: 0.0011755746676457924\n",
            "--------------------------------------------------\n",
            "iter: 1276 \ttrain loss: 0.0012612843553776792\n",
            "--------------------------------------------------\n",
            "iter: 1277 \ttrain loss: 0.0011329859274870164\n",
            "--------------------------------------------------\n",
            "iter: 1278 \ttrain loss: 0.0010692215686609501\n",
            "--------------------------------------------------\n",
            "iter: 1279 \ttrain loss: 0.0011407156215351576\n",
            "--------------------------------------------------\n",
            "iter: 1280 \ttrain loss: 0.0011202836729310264\n",
            "--------------------------------------------------\n",
            "iter: 1281 \ttrain loss: 0.0011777300148284506\n",
            "--------------------------------------------------\n",
            "iter: 1282 \ttrain loss: 0.0011180360062402518\n",
            "--------------------------------------------------\n",
            "iter: 1283 \ttrain loss: 0.0011648470227265137\n",
            "--------------------------------------------------\n",
            "iter: 1284 \ttrain loss: 0.0010774778740814677\n",
            "--------------------------------------------------\n",
            "iter: 1285 \ttrain loss: 0.0010910628437144328\n",
            "--------------------------------------------------\n",
            "iter: 1286 \ttrain loss: 0.001099438946858128\n",
            "--------------------------------------------------\n",
            "iter: 1287 \ttrain loss: 0.0010271538126386684\n",
            "--------------------------------------------------\n",
            "iter: 1288 \ttrain loss: 0.0011937153981906768\n",
            "--------------------------------------------------\n",
            "iter: 1289 \ttrain loss: 0.001081221969543464\n",
            "--------------------------------------------------\n",
            "iter: 1290 \ttrain loss: 0.001085900668724135\n",
            "--------------------------------------------------\n",
            "iter: 1291 \ttrain loss: 0.0010420588721659124\n",
            "--------------------------------------------------\n",
            "iter: 1292 \ttrain loss: 0.0009718292626604547\n",
            "--------------------------------------------------\n",
            "iter: 1293 \ttrain loss: 0.001195097722377782\n",
            "--------------------------------------------------\n",
            "iter: 1294 \ttrain loss: 0.0010872818329640888\n",
            "--------------------------------------------------\n",
            "iter: 1295 \ttrain loss: 0.0011070031371069192\n",
            "--------------------------------------------------\n",
            "iter: 1296 \ttrain loss: 0.001189959293212921\n",
            "--------------------------------------------------\n",
            "iter: 1297 \ttrain loss: 0.0009744684458026661\n",
            "--------------------------------------------------\n",
            "iter: 1298 \ttrain loss: 0.0010785834100010912\n",
            "--------------------------------------------------\n",
            "iter: 1299 \ttrain loss: 0.0011211428030521687\n",
            "--------------------------------------------------\n",
            "iter: 1300 \ttrain loss: 0.0012013207001921296\n",
            "--------------------------------------------------\n",
            "iter: 1301 \ttrain loss: 0.0009784548426416877\n",
            "--------------------------------------------------\n",
            "iter: 1302 \ttrain loss: 0.0011949402453733245\n",
            "--------------------------------------------------\n",
            "iter: 1303 \ttrain loss: 0.0010297442036449366\n",
            "--------------------------------------------------\n",
            "iter: 1304 \ttrain loss: 0.0011086521833037104\n",
            "--------------------------------------------------\n",
            "iter: 1305 \ttrain loss: 0.0009432956001675336\n",
            "--------------------------------------------------\n",
            "iter: 1306 \ttrain loss: 0.0011017886453115772\n",
            "--------------------------------------------------\n",
            "iter: 1307 \ttrain loss: 0.0010840318206537813\n",
            "--------------------------------------------------\n",
            "iter: 1308 \ttrain loss: 0.001056151200074806\n",
            "--------------------------------------------------\n",
            "iter: 1309 \ttrain loss: 0.0011012134304539403\n",
            "--------------------------------------------------\n",
            "iter: 1310 \ttrain loss: 0.0010602672356348343\n",
            "--------------------------------------------------\n",
            "iter: 1311 \ttrain loss: 0.0010744773925262492\n",
            "--------------------------------------------------\n",
            "iter: 1312 \ttrain loss: 0.0010648897735253597\n",
            "--------------------------------------------------\n",
            "iter: 1313 \ttrain loss: 0.0010039553946052358\n",
            "--------------------------------------------------\n",
            "iter: 1314 \ttrain loss: 0.001371203382773919\n",
            "--------------------------------------------------\n",
            "iter: 1315 \ttrain loss: 0.0011597038861143058\n",
            "--------------------------------------------------\n",
            "iter: 1316 \ttrain loss: 0.0009679795287672855\n",
            "--------------------------------------------------\n",
            "iter: 1317 \ttrain loss: 0.0011015630644493916\n",
            "--------------------------------------------------\n",
            "iter: 1318 \ttrain loss: 0.0010997299195989116\n",
            "--------------------------------------------------\n",
            "iter: 1319 \ttrain loss: 0.0011247283085547642\n",
            "--------------------------------------------------\n",
            "iter: 1320 \ttrain loss: 0.0009873403717287803\n",
            "--------------------------------------------------\n",
            "iter: 1321 \ttrain loss: 0.0011772275393722878\n",
            "--------------------------------------------------\n",
            "iter: 1322 \ttrain loss: 0.0011085822901425665\n",
            "--------------------------------------------------\n",
            "iter: 1323 \ttrain loss: 0.0009701963127814638\n",
            "--------------------------------------------------\n",
            "iter: 1324 \ttrain loss: 0.0011646286974197435\n",
            "--------------------------------------------------\n",
            "iter: 1325 \ttrain loss: 0.00105865851341187\n",
            "--------------------------------------------------\n",
            "iter: 1326 \ttrain loss: 0.0010549808826574238\n",
            "--------------------------------------------------\n",
            "iter: 1327 \ttrain loss: 0.001094010415884976\n",
            "--------------------------------------------------\n",
            "iter: 1328 \ttrain loss: 0.0010858360148568846\n",
            "--------------------------------------------------\n",
            "iter: 1329 \ttrain loss: 0.0010739323539072367\n",
            "--------------------------------------------------\n",
            "iter: 1330 \ttrain loss: 0.0011263504303371146\n",
            "--------------------------------------------------\n",
            "iter: 1331 \ttrain loss: 0.0010266580146458012\n",
            "--------------------------------------------------\n",
            "iter: 1332 \ttrain loss: 0.001067056420276902\n",
            "--------------------------------------------------\n",
            "iter: 1333 \ttrain loss: 0.0009916404467855374\n",
            "--------------------------------------------------\n",
            "iter: 1334 \ttrain loss: 0.0010178887028960148\n",
            "--------------------------------------------------\n",
            "iter: 1335 \ttrain loss: 0.0010949371123180759\n",
            "--------------------------------------------------\n",
            "iter: 1336 \ttrain loss: 0.0010923660370324455\n",
            "--------------------------------------------------\n",
            "iter: 1337 \ttrain loss: 0.0011436054313079794\n",
            "--------------------------------------------------\n",
            "iter: 1338 \ttrain loss: 0.0010180865398032807\n",
            "--------------------------------------------------\n",
            "iter: 1339 \ttrain loss: 0.0010614004187731415\n",
            "--------------------------------------------------\n",
            "iter: 1340 \ttrain loss: 0.0010323891902828933\n",
            "--------------------------------------------------\n",
            "iter: 1341 \ttrain loss: 0.0010768254311780642\n",
            "--------------------------------------------------\n",
            "iter: 1342 \ttrain loss: 0.001342934621212769\n",
            "--------------------------------------------------\n",
            "iter: 1343 \ttrain loss: 0.001015068239797201\n",
            "--------------------------------------------------\n",
            "iter: 1344 \ttrain loss: 0.0011818326436258157\n",
            "--------------------------------------------------\n",
            "iter: 1345 \ttrain loss: 0.001001403623643418\n",
            "--------------------------------------------------\n",
            "iter: 1346 \ttrain loss: 0.0010459606467612729\n",
            "--------------------------------------------------\n",
            "iter: 1347 \ttrain loss: 0.001307787314563618\n",
            "--------------------------------------------------\n",
            "iter: 1348 \ttrain loss: 0.0010607909253687994\n",
            "--------------------------------------------------\n",
            "iter: 1349 \ttrain loss: 0.0011649888870748288\n",
            "--------------------------------------------------\n",
            "iter: 1350 \ttrain loss: 0.0010219337890799525\n",
            "--------------------------------------------------\n",
            "iter: 1351 \ttrain loss: 0.001089800172735319\n",
            "--------------------------------------------------\n",
            "iter: 1352 \ttrain loss: 0.0012318174082002814\n",
            "--------------------------------------------------\n",
            "iter: 1353 \ttrain loss: 0.0009235530850663111\n",
            "--------------------------------------------------\n",
            "iter: 1354 \ttrain loss: 0.001231529467491406\n",
            "--------------------------------------------------\n",
            "iter: 1355 \ttrain loss: 0.0010314213765632153\n",
            "--------------------------------------------------\n",
            "iter: 1356 \ttrain loss: 0.0011714392024690578\n",
            "--------------------------------------------------\n",
            "iter: 1357 \ttrain loss: 0.0010467319140345615\n",
            "--------------------------------------------------\n",
            "iter: 1358 \ttrain loss: 0.001074440170085183\n",
            "--------------------------------------------------\n",
            "iter: 1359 \ttrain loss: 0.0010765181682622757\n",
            "--------------------------------------------------\n",
            "iter: 1360 \ttrain loss: 0.001010538029978863\n",
            "--------------------------------------------------\n",
            "iter: 1361 \ttrain loss: 0.00112756190295713\n",
            "--------------------------------------------------\n",
            "iter: 1362 \ttrain loss: 0.0010039737882836808\n",
            "--------------------------------------------------\n",
            "iter: 1363 \ttrain loss: 0.0011236489004723762\n",
            "--------------------------------------------------\n",
            "iter: 1364 \ttrain loss: 0.0010535044405439915\n",
            "--------------------------------------------------\n",
            "iter: 1365 \ttrain loss: 0.0010041982552206256\n",
            "--------------------------------------------------\n",
            "iter: 1366 \ttrain loss: 0.0009054053957805393\n",
            "--------------------------------------------------\n",
            "iter: 1367 \ttrain loss: 0.0013129619557121991\n",
            "--------------------------------------------------\n",
            "iter: 1368 \ttrain loss: 0.0010223406111909614\n",
            "--------------------------------------------------\n",
            "iter: 1369 \ttrain loss: 0.0010530269915287741\n",
            "--------------------------------------------------\n",
            "iter: 1370 \ttrain loss: 0.0010141473826427493\n",
            "--------------------------------------------------\n",
            "iter: 1371 \ttrain loss: 0.0011010289316329728\n",
            "--------------------------------------------------\n",
            "iter: 1372 \ttrain loss: 0.0010888733610484745\n",
            "--------------------------------------------------\n",
            "iter: 1373 \ttrain loss: 0.0011324465048993704\n",
            "--------------------------------------------------\n",
            "iter: 1374 \ttrain loss: 0.001293245499118637\n",
            "--------------------------------------------------\n",
            "iter: 1375 \ttrain loss: 0.001093757962379689\n",
            "--------------------------------------------------\n",
            "iter: 1376 \ttrain loss: 0.001101273636531003\n",
            "--------------------------------------------------\n",
            "iter: 1377 \ttrain loss: 0.0011576835598989933\n",
            "--------------------------------------------------\n",
            "iter: 1378 \ttrain loss: 0.0011011583117053814\n",
            "--------------------------------------------------\n",
            "iter: 1379 \ttrain loss: 0.0011047844454566127\n",
            "--------------------------------------------------\n",
            "iter: 1380 \ttrain loss: 0.0010525203807746453\n",
            "--------------------------------------------------\n",
            "iter: 1381 \ttrain loss: 0.0011388075174427277\n",
            "--------------------------------------------------\n",
            "iter: 1382 \ttrain loss: 0.0011601798791939346\n",
            "--------------------------------------------------\n",
            "iter: 1383 \ttrain loss: 0.0010744488953138885\n",
            "--------------------------------------------------\n",
            "iter: 1384 \ttrain loss: 0.0011871420166855286\n",
            "--------------------------------------------------\n",
            "iter: 1385 \ttrain loss: 0.0010699441984552797\n",
            "--------------------------------------------------\n",
            "iter: 1386 \ttrain loss: 0.0010140417565129513\n",
            "--------------------------------------------------\n",
            "iter: 1387 \ttrain loss: 0.0011161055113222218\n",
            "--------------------------------------------------\n",
            "iter: 1388 \ttrain loss: 0.0010517014527365047\n",
            "--------------------------------------------------\n",
            "iter: 1389 \ttrain loss: 0.0011094696036744944\n",
            "--------------------------------------------------\n",
            "iter: 1390 \ttrain loss: 0.0014545491338363956\n",
            "--------------------------------------------------\n",
            "iter: 1391 \ttrain loss: 0.0010351699668766329\n",
            "--------------------------------------------------\n",
            "iter: 1392 \ttrain loss: 0.0011792297146615617\n",
            "--------------------------------------------------\n",
            "iter: 1393 \ttrain loss: 0.0011217830139401198\n",
            "--------------------------------------------------\n",
            "iter: 1394 \ttrain loss: 0.0010867317214658051\n",
            "--------------------------------------------------\n",
            "iter: 1395 \ttrain loss: 0.0010621421352250064\n",
            "--------------------------------------------------\n",
            "iter: 1396 \ttrain loss: 0.0010813336260320652\n",
            "--------------------------------------------------\n",
            "iter: 1397 \ttrain loss: 0.0011284300185163765\n",
            "--------------------------------------------------\n",
            "iter: 1398 \ttrain loss: 0.0010608979510169148\n",
            "--------------------------------------------------\n",
            "iter: 1399 \ttrain loss: 0.0009665137382755707\n",
            "--------------------------------------------------\n",
            "iter: 1400 \ttrain loss: 0.0012004489468760606\n",
            "--------------------------------------------------\n",
            "iter: 1401 \ttrain loss: 0.0010908583625324326\n",
            "--------------------------------------------------\n",
            "iter: 1402 \ttrain loss: 0.0011186850873677859\n",
            "--------------------------------------------------\n",
            "iter: 1403 \ttrain loss: 0.0012132033637846994\n",
            "--------------------------------------------------\n",
            "iter: 1404 \ttrain loss: 0.0010803429309457846\n",
            "--------------------------------------------------\n",
            "iter: 1405 \ttrain loss: 0.0010738971178741578\n",
            "--------------------------------------------------\n",
            "iter: 1406 \ttrain loss: 0.0011024898533510989\n",
            "--------------------------------------------------\n",
            "iter: 1407 \ttrain loss: 0.0010634814797218252\n",
            "--------------------------------------------------\n",
            "iter: 1408 \ttrain loss: 0.0011017799551759065\n",
            "--------------------------------------------------\n",
            "iter: 1409 \ttrain loss: 0.0010675467155961472\n",
            "--------------------------------------------------\n",
            "iter: 1410 \ttrain loss: 0.0010696871060855795\n",
            "--------------------------------------------------\n",
            "iter: 1411 \ttrain loss: 0.001154502128440764\n",
            "--------------------------------------------------\n",
            "iter: 1412 \ttrain loss: 0.0009932061211666869\n",
            "--------------------------------------------------\n",
            "iter: 1413 \ttrain loss: 0.0011980826208352563\n",
            "--------------------------------------------------\n",
            "iter: 1414 \ttrain loss: 0.0009910404244039463\n",
            "--------------------------------------------------\n",
            "iter: 1415 \ttrain loss: 0.001093137845787021\n",
            "--------------------------------------------------\n",
            "iter: 1416 \ttrain loss: 0.0010410783670519476\n",
            "--------------------------------------------------\n",
            "iter: 1417 \ttrain loss: 0.0012045652883416274\n",
            "--------------------------------------------------\n",
            "iter: 1418 \ttrain loss: 0.001003460307983358\n",
            "--------------------------------------------------\n",
            "iter: 1419 \ttrain loss: 0.0011396173555405517\n",
            "--------------------------------------------------\n",
            "iter: 1420 \ttrain loss: 0.0010117408088701606\n",
            "--------------------------------------------------\n",
            "iter: 1421 \ttrain loss: 0.0009838850980842641\n",
            "--------------------------------------------------\n",
            "iter: 1422 \ttrain loss: 0.0012171430936445992\n",
            "--------------------------------------------------\n",
            "iter: 1423 \ttrain loss: 0.0009840631082148293\n",
            "--------------------------------------------------\n",
            "iter: 1424 \ttrain loss: 0.0010503101846034849\n",
            "--------------------------------------------------\n",
            "iter: 1425 \ttrain loss: 0.0010660874363677426\n",
            "--------------------------------------------------\n",
            "iter: 1426 \ttrain loss: 0.0012574072158926307\n",
            "--------------------------------------------------\n",
            "iter: 1427 \ttrain loss: 0.001088891530448602\n",
            "--------------------------------------------------\n",
            "iter: 1428 \ttrain loss: 0.00108880464502307\n",
            "--------------------------------------------------\n",
            "iter: 1429 \ttrain loss: 0.0010765590089209592\n",
            "--------------------------------------------------\n",
            "iter: 1430 \ttrain loss: 0.0011215096191024652\n",
            "--------------------------------------------------\n",
            "iter: 1431 \ttrain loss: 0.0009468084422071828\n",
            "--------------------------------------------------\n",
            "iter: 1432 \ttrain loss: 0.0010599774717688743\n",
            "--------------------------------------------------\n",
            "iter: 1433 \ttrain loss: 0.0010973067772224497\n",
            "--------------------------------------------------\n",
            "iter: 1434 \ttrain loss: 0.001057067470365004\n",
            "--------------------------------------------------\n",
            "iter: 1435 \ttrain loss: 0.0011725841569017578\n",
            "--------------------------------------------------\n",
            "iter: 1436 \ttrain loss: 0.0011655099473372437\n",
            "--------------------------------------------------\n",
            "iter: 1437 \ttrain loss: 0.0010118634253009908\n",
            "--------------------------------------------------\n",
            "iter: 1438 \ttrain loss: 0.0010973619170502721\n",
            "--------------------------------------------------\n",
            "iter: 1439 \ttrain loss: 0.0011603585514789539\n",
            "--------------------------------------------------\n",
            "iter: 1440 \ttrain loss: 0.0010317082548722807\n",
            "--------------------------------------------------\n",
            "iter: 1441 \ttrain loss: 0.001138137754527195\n",
            "--------------------------------------------------\n",
            "iter: 1442 \ttrain loss: 0.0009684857414708519\n",
            "--------------------------------------------------\n",
            "iter: 1443 \ttrain loss: 0.0011915753210919229\n",
            "--------------------------------------------------\n",
            "iter: 1444 \ttrain loss: 0.0011011925666799965\n",
            "--------------------------------------------------\n",
            "iter: 1445 \ttrain loss: 0.0010720266855960994\n",
            "--------------------------------------------------\n",
            "iter: 1446 \ttrain loss: 0.001134794478134037\n",
            "--------------------------------------------------\n",
            "iter: 1447 \ttrain loss: 0.0009599455963288491\n",
            "--------------------------------------------------\n",
            "iter: 1448 \ttrain loss: 0.0011271748138670554\n",
            "--------------------------------------------------\n",
            "iter: 1449 \ttrain loss: 0.0010475850543322192\n",
            "--------------------------------------------------\n",
            "iter: 1450 \ttrain loss: 0.0011485626020676602\n",
            "--------------------------------------------------\n",
            "iter: 1451 \ttrain loss: 0.0011485995750117497\n",
            "--------------------------------------------------\n",
            "iter: 1452 \ttrain loss: 0.0012547784936903757\n",
            "--------------------------------------------------\n",
            "iter: 1453 \ttrain loss: 0.0010984648776599332\n",
            "--------------------------------------------------\n",
            "iter: 1454 \ttrain loss: 0.0010631436006579363\n",
            "--------------------------------------------------\n",
            "iter: 1455 \ttrain loss: 0.0010875906600250865\n",
            "--------------------------------------------------\n",
            "iter: 1456 \ttrain loss: 0.0011296493338191133\n",
            "--------------------------------------------------\n",
            "iter: 1457 \ttrain loss: 0.001079595239304951\n",
            "--------------------------------------------------\n",
            "iter: 1458 \ttrain loss: 0.0010046547487861204\n",
            "--------------------------------------------------\n",
            "iter: 1459 \ttrain loss: 0.0008882519831798569\n",
            "--------------------------------------------------\n",
            "iter: 1460 \ttrain loss: 0.0013847826261511693\n",
            "--------------------------------------------------\n",
            "iter: 1461 \ttrain loss: 0.0011293949829551805\n",
            "--------------------------------------------------\n",
            "iter: 1462 \ttrain loss: 0.0009866546811537471\n",
            "--------------------------------------------------\n",
            "iter: 1463 \ttrain loss: 0.0015302409063762694\n",
            "--------------------------------------------------\n",
            "iter: 1464 \ttrain loss: 0.0011788936554038498\n",
            "--------------------------------------------------\n",
            "iter: 1465 \ttrain loss: 0.0010551494362641652\n",
            "--------------------------------------------------\n",
            "iter: 1466 \ttrain loss: 0.0011742591570361304\n",
            "--------------------------------------------------\n",
            "iter: 1467 \ttrain loss: 0.0010451658825130416\n",
            "--------------------------------------------------\n",
            "iter: 1468 \ttrain loss: 0.0010462842694792215\n",
            "--------------------------------------------------\n",
            "iter: 1469 \ttrain loss: 0.001056341204588929\n",
            "--------------------------------------------------\n",
            "iter: 1470 \ttrain loss: 0.0011148829885027996\n",
            "--------------------------------------------------\n",
            "iter: 1471 \ttrain loss: 0.0011459190925666336\n",
            "--------------------------------------------------\n",
            "iter: 1472 \ttrain loss: 0.001021401059324394\n",
            "--------------------------------------------------\n",
            "iter: 1473 \ttrain loss: 0.0010751555366123548\n",
            "--------------------------------------------------\n",
            "iter: 1474 \ttrain loss: 0.0010581749547705426\n",
            "--------------------------------------------------\n",
            "iter: 1475 \ttrain loss: 0.0011630983330724622\n",
            "--------------------------------------------------\n",
            "iter: 1476 \ttrain loss: 0.0010937595043895613\n",
            "--------------------------------------------------\n",
            "iter: 1477 \ttrain loss: 0.00104703007628657\n",
            "--------------------------------------------------\n",
            "iter: 1478 \ttrain loss: 0.0010872982332642263\n",
            "--------------------------------------------------\n",
            "iter: 1479 \ttrain loss: 0.001059464787300429\n",
            "--------------------------------------------------\n",
            "iter: 1480 \ttrain loss: 0.0009859592196954448\n",
            "--------------------------------------------------\n",
            "iter: 1481 \ttrain loss: 0.0013562640138982658\n",
            "--------------------------------------------------\n",
            "iter: 1482 \ttrain loss: 0.0010153691954609108\n",
            "--------------------------------------------------\n",
            "iter: 1483 \ttrain loss: 0.001091499718330306\n",
            "--------------------------------------------------\n",
            "iter: 1484 \ttrain loss: 0.0010005679548850397\n",
            "--------------------------------------------------\n",
            "iter: 1485 \ttrain loss: 0.0009436325450510788\n",
            "--------------------------------------------------\n",
            "iter: 1486 \ttrain loss: 0.0012262808490812642\n",
            "--------------------------------------------------\n",
            "iter: 1487 \ttrain loss: 0.0011429681840477782\n",
            "--------------------------------------------------\n",
            "iter: 1488 \ttrain loss: 0.001020357191393939\n",
            "--------------------------------------------------\n",
            "iter: 1489 \ttrain loss: 0.0009167032090986138\n",
            "--------------------------------------------------\n",
            "iter: 1490 \ttrain loss: 0.0011488665675864359\n",
            "--------------------------------------------------\n",
            "iter: 1491 \ttrain loss: 0.0012320179186963126\n",
            "--------------------------------------------------\n",
            "iter: 1492 \ttrain loss: 0.0010793776368959124\n",
            "--------------------------------------------------\n",
            "iter: 1493 \ttrain loss: 0.0009879170211787968\n",
            "--------------------------------------------------\n",
            "iter: 1494 \ttrain loss: 0.0010817720882662835\n",
            "--------------------------------------------------\n",
            "iter: 1495 \ttrain loss: 0.0011560230140701653\n",
            "--------------------------------------------------\n",
            "iter: 1496 \ttrain loss: 0.0011310443153057068\n",
            "--------------------------------------------------\n",
            "iter: 1497 \ttrain loss: 0.0011488515105448555\n",
            "--------------------------------------------------\n",
            "iter: 1498 \ttrain loss: 0.00109022301478006\n",
            "--------------------------------------------------\n",
            "iter: 1499 \ttrain loss: 0.0010749529657882726\n",
            "--------------------------------------------------\n",
            "iter: 1500 \ttrain loss: 0.0011022049455097311\n",
            "--------------------------------------------------\n",
            "iter: 1501 \ttrain loss: 0.0010513941496836923\n",
            "--------------------------------------------------\n",
            "iter: 1502 \ttrain loss: 0.0011384243179170571\n",
            "--------------------------------------------------\n",
            "iter: 1503 \ttrain loss: 0.000943564541596669\n",
            "--------------------------------------------------\n",
            "iter: 1504 \ttrain loss: 0.0011781462578806434\n",
            "--------------------------------------------------\n",
            "iter: 1505 \ttrain loss: 0.0010220227550658699\n",
            "--------------------------------------------------\n",
            "iter: 1506 \ttrain loss: 0.00119762767857831\n",
            "--------------------------------------------------\n",
            "iter: 1507 \ttrain loss: 0.0010456413099731731\n",
            "--------------------------------------------------\n",
            "iter: 1508 \ttrain loss: 0.0012046308125846143\n",
            "--------------------------------------------------\n",
            "iter: 1509 \ttrain loss: 0.0009312385508496653\n",
            "--------------------------------------------------\n",
            "iter: 1510 \ttrain loss: 0.0010650556309231168\n",
            "--------------------------------------------------\n",
            "iter: 1511 \ttrain loss: 0.0011328952689302165\n",
            "--------------------------------------------------\n",
            "iter: 1512 \ttrain loss: 0.0009819269996353354\n",
            "--------------------------------------------------\n",
            "iter: 1513 \ttrain loss: 0.0013435335200213594\n",
            "--------------------------------------------------\n",
            "iter: 1514 \ttrain loss: 0.0011364211289385288\n",
            "--------------------------------------------------\n",
            "iter: 1515 \ttrain loss: 0.0010480558946138413\n",
            "--------------------------------------------------\n",
            "iter: 1516 \ttrain loss: 0.001049995765166234\n",
            "--------------------------------------------------\n",
            "iter: 1517 \ttrain loss: 0.0010836145603493185\n",
            "--------------------------------------------------\n",
            "iter: 1518 \ttrain loss: 0.0011243306090596014\n",
            "--------------------------------------------------\n",
            "iter: 1519 \ttrain loss: 0.001097607943888164\n",
            "--------------------------------------------------\n",
            "iter: 1520 \ttrain loss: 0.0010867361572067871\n",
            "--------------------------------------------------\n",
            "iter: 1521 \ttrain loss: 0.0011947479244668164\n",
            "--------------------------------------------------\n",
            "iter: 1522 \ttrain loss: 0.0010794602167605789\n",
            "--------------------------------------------------\n",
            "iter: 1523 \ttrain loss: 0.0009878395930375211\n",
            "--------------------------------------------------\n",
            "iter: 1524 \ttrain loss: 0.0011165741134331966\n",
            "--------------------------------------------------\n",
            "iter: 1525 \ttrain loss: 0.001078247924182065\n",
            "--------------------------------------------------\n",
            "iter: 1526 \ttrain loss: 0.00108535916173609\n",
            "--------------------------------------------------\n",
            "iter: 1527 \ttrain loss: 0.0009279071729489901\n",
            "--------------------------------------------------\n",
            "iter: 1528 \ttrain loss: 0.0011224557990285125\n",
            "--------------------------------------------------\n",
            "iter: 1529 \ttrain loss: 0.001242166296425049\n",
            "--------------------------------------------------\n",
            "iter: 1530 \ttrain loss: 0.001058713427170225\n",
            "--------------------------------------------------\n",
            "iter: 1531 \ttrain loss: 0.001084392152254181\n",
            "--------------------------------------------------\n",
            "iter: 1532 \ttrain loss: 0.0010048477339191448\n",
            "--------------------------------------------------\n",
            "iter: 1533 \ttrain loss: 0.0015363143663810767\n",
            "--------------------------------------------------\n",
            "iter: 1534 \ttrain loss: 0.0011065955163384713\n",
            "--------------------------------------------------\n",
            "iter: 1535 \ttrain loss: 0.001021852407959412\n",
            "--------------------------------------------------\n",
            "iter: 1536 \ttrain loss: 0.0009452779788670763\n",
            "--------------------------------------------------\n",
            "iter: 1537 \ttrain loss: 0.0011272806798696068\n",
            "--------------------------------------------------\n",
            "iter: 1538 \ttrain loss: 0.001163624686968051\n",
            "--------------------------------------------------\n",
            "iter: 1539 \ttrain loss: 0.0011410753596363113\n",
            "--------------------------------------------------\n",
            "iter: 1540 \ttrain loss: 0.0010193590056505248\n",
            "--------------------------------------------------\n",
            "iter: 1541 \ttrain loss: 0.0011309539598789301\n",
            "--------------------------------------------------\n",
            "iter: 1542 \ttrain loss: 0.0010701319172496141\n",
            "--------------------------------------------------\n",
            "iter: 1543 \ttrain loss: 0.0010496773958188687\n",
            "--------------------------------------------------\n",
            "iter: 1544 \ttrain loss: 0.0011504186898864271\n",
            "--------------------------------------------------\n",
            "iter: 1545 \ttrain loss: 0.0010581167325696887\n",
            "--------------------------------------------------\n",
            "iter: 1546 \ttrain loss: 0.0010534631662871216\n",
            "--------------------------------------------------\n",
            "iter: 1547 \ttrain loss: 0.0011994237420882743\n",
            "--------------------------------------------------\n",
            "iter: 1548 \ttrain loss: 0.0010954190738837173\n",
            "--------------------------------------------------\n",
            "iter: 1549 \ttrain loss: 0.0010456231858669784\n",
            "--------------------------------------------------\n",
            "iter: 1550 \ttrain loss: 0.0010452905761261253\n",
            "--------------------------------------------------\n",
            "iter: 1551 \ttrain loss: 0.0011353785801700895\n",
            "--------------------------------------------------\n",
            "iter: 1552 \ttrain loss: 0.0010522704830216469\n",
            "--------------------------------------------------\n",
            "iter: 1553 \ttrain loss: 0.000902962559355473\n",
            "--------------------------------------------------\n",
            "iter: 1554 \ttrain loss: 0.001339011520740607\n",
            "--------------------------------------------------\n",
            "iter: 1555 \ttrain loss: 0.0008381116817731072\n",
            "--------------------------------------------------\n",
            "iter: 1556 \ttrain loss: 0.001149001026487751\n",
            "--------------------------------------------------\n",
            "iter: 1557 \ttrain loss: 0.0009715378256081528\n",
            "--------------------------------------------------\n",
            "iter: 1558 \ttrain loss: 0.0012790726849849804\n",
            "--------------------------------------------------\n",
            "iter: 1559 \ttrain loss: 0.0010849607817902188\n",
            "--------------------------------------------------\n",
            "iter: 1560 \ttrain loss: 0.0011142416507896186\n",
            "--------------------------------------------------\n",
            "iter: 1561 \ttrain loss: 0.0010947035224954575\n",
            "--------------------------------------------------\n",
            "iter: 1562 \ttrain loss: 0.0011173389563737363\n",
            "--------------------------------------------------\n",
            "iter: 1563 \ttrain loss: 0.0011436292105791955\n",
            "--------------------------------------------------\n",
            "iter: 1564 \ttrain loss: 0.0010434344167549105\n",
            "--------------------------------------------------\n",
            "iter: 1565 \ttrain loss: 0.0010901141072058803\n",
            "--------------------------------------------------\n",
            "iter: 1566 \ttrain loss: 0.001098849336032585\n",
            "--------------------------------------------------\n",
            "iter: 1567 \ttrain loss: 0.0010250610780451158\n",
            "--------------------------------------------------\n",
            "iter: 1568 \ttrain loss: 0.0010999605778988645\n",
            "--------------------------------------------------\n",
            "\n",
            "Training DNM\n",
            "==================================================\n",
            "iter: 1 \tloss: 0.12242915727383917, dist: 0.022751857723417892\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 2 \tloss: 0.12174353646023484, dist: 0.02280275870690279\n",
            "--------------------------------------------------\n",
            "iter: 3 \tloss: 0.11914424218403132, dist: 0.023712811648223\n",
            "--------------------------------------------------\n",
            "iter: 4 \tloss: 0.11759761084812811, dist: 0.02237029331169187\n",
            "--------------------------------------------------\n",
            "iter: 5 \tloss: 0.11870310491821584, dist: 0.022567582328918655\n",
            "--------------------------------------------------\n",
            "iter: 6 \tloss: 0.11255958929055962, dist: 0.02089904179555908\n",
            "--------------------------------------------------\n",
            "iter: 7 \tloss: 0.1283899597559424, dist: 0.02205811327127258\n",
            "--------------------------------------------------\n",
            "iter: 8 \tloss: 0.1366804031881687, dist: 0.022523608943616207\n",
            "--------------------------------------------------\n",
            "iter: 9 \tloss: 0.11449168112378788, dist: 0.020708195551087914\n",
            "--------------------------------------------------\n",
            "iter: 10 \tloss: 0.1384849696950976, dist: 0.022584861924736143\n",
            "--------------------------------------------------\n",
            "iter: 11 \tloss: 0.13371331783989382, dist: 0.020382116348997573\n",
            "--------------------------------------------------\n",
            "iter: 12 \tloss: 0.11686676848392798, dist: 0.021050029820224723\n",
            "--------------------------------------------------\n",
            "iter: 13 \tloss: 0.12211343486673432, dist: 0.021347080565898787\n",
            "--------------------------------------------------\n",
            "iter: 14 \tloss: 0.12311921270336833, dist: 0.02104015913901848\n",
            "--------------------------------------------------\n",
            "iter: 15 \tloss: 0.1135208292126237, dist: 0.01888360765158846\n",
            "--------------------------------------------------\n",
            "iter: 16 \tloss: 0.1366105952903095, dist: 0.02333901210545329\n",
            "--------------------------------------------------\n",
            "iter: 17 \tloss: 0.12002522871992923, dist: 0.020212354075313695\n",
            "--------------------------------------------------\n",
            "iter: 18 \tloss: 0.119575373591027, dist: 0.02243263708707584\n",
            "--------------------------------------------------\n",
            "iter: 19 \tloss: 0.13219237560312141, dist: 0.019533090524878262\n",
            "--------------------------------------------------\n",
            "iter: 20 \tloss: 0.13271026463373084, dist: 0.020688534408109905\n",
            "--------------------------------------------------\n",
            "iter: 21 \tloss: 0.11752933498443095, dist: 0.020972299562059454\n",
            "--------------------------------------------------\n",
            "iter: 22 \tloss: 0.1179009093261144, dist: 0.020467589994788003\n",
            "--------------------------------------------------\n",
            "iter: 23 \tloss: 0.12003691327396461, dist: 0.019630288126151214\n",
            "--------------------------------------------------\n",
            "iter: 24 \tloss: 0.1244935725093921, dist: 0.02010285793824111\n",
            "--------------------------------------------------\n",
            "iter: 25 \tloss: 0.12641999710746138, dist: 0.019713128655737638\n",
            "--------------------------------------------------\n",
            "iter: 26 \tloss: 0.11294409784795739, dist: 0.01989257364441979\n",
            "--------------------------------------------------\n",
            "iter: 27 \tloss: 0.12502714045113372, dist: 0.01990293223563152\n",
            "--------------------------------------------------\n",
            "iter: 28 \tloss: 0.1174386229224852, dist: 0.01867194145621334\n",
            "--------------------------------------------------\n",
            "iter: 29 \tloss: 0.12850890968209971, dist: 0.020358126116598006\n",
            "--------------------------------------------------\n",
            "iter: 30 \tloss: 0.11698585302957203, dist: 0.0182837233800284\n",
            "--------------------------------------------------\n",
            "iter: 31 \tloss: 0.11875930043573972, dist: 0.01991911582333355\n",
            "--------------------------------------------------\n",
            "iter: 32 \tloss: 0.11626400699013334, dist: 0.01969057952649904\n",
            "--------------------------------------------------\n",
            "iter: 33 \tloss: 0.13947940777168513, dist: 0.02134714699846276\n",
            "--------------------------------------------------\n",
            "iter: 34 \tloss: 0.11070326572655588, dist: 0.01782301855819521\n",
            "--------------------------------------------------\n",
            "iter: 35 \tloss: 0.1204253327793067, dist: 0.020123003781924994\n",
            "--------------------------------------------------\n",
            "iter: 36 \tloss: 0.11927307716623775, dist: 0.019375800516435067\n",
            "--------------------------------------------------\n",
            "iter: 37 \tloss: 0.11457567464017568, dist: 0.018482353165868017\n",
            "--------------------------------------------------\n",
            "iter: 38 \tloss: 0.12413762654493933, dist: 0.01969730896589304\n",
            "--------------------------------------------------\n",
            "iter: 39 \tloss: 0.11389858551785483, dist: 0.019508246996070046\n",
            "--------------------------------------------------\n",
            "iter: 40 \tloss: 0.1164977882817222, dist: 0.01889368236321099\n",
            "--------------------------------------------------\n",
            "iter: 41 \tloss: 0.1316376583076802, dist: 0.020802890576319167\n",
            "--------------------------------------------------\n",
            "iter: 42 \tloss: 0.11854923519794741, dist: 0.019119201242465685\n",
            "--------------------------------------------------\n",
            "iter: 43 \tloss: 0.118197580776599, dist: 0.018480656852443232\n",
            "--------------------------------------------------\n",
            "iter: 44 \tloss: 0.1305068856551056, dist: 0.01956151213125307\n",
            "--------------------------------------------------\n",
            "iter: 45 \tloss: 0.12872206750914097, dist: 0.01875865907243913\n",
            "--------------------------------------------------\n",
            "iter: 46 \tloss: 0.12520703743036707, dist: 0.018150620406791426\n",
            "--------------------------------------------------\n",
            "iter: 47 \tloss: 0.12285273382857165, dist: 0.019682236568150997\n",
            "--------------------------------------------------\n",
            "iter: 48 \tloss: 0.12365414725529998, dist: 0.01812393632588796\n",
            "--------------------------------------------------\n",
            "iter: 49 \tloss: 0.11920050256460368, dist: 0.02038958071609778\n",
            "--------------------------------------------------\n",
            "iter: 50 \tloss: 0.11853036208839068, dist: 0.018018630018955668\n",
            "--------------------------------------------------\n",
            "iter: 51 \tloss: 0.11909265445592666, dist: 0.018231956832199393\n",
            "--------------------------------------------------\n",
            "iter: 52 \tloss: 0.1273882260855914, dist: 0.020119620668496884\n",
            "--------------------------------------------------\n",
            "iter: 53 \tloss: 0.13104295094649518, dist: 0.020469663846434193\n",
            "--------------------------------------------------\n",
            "iter: 54 \tloss: 0.12396030010188165, dist: 0.01856654416900849\n",
            "--------------------------------------------------\n",
            "iter: 55 \tloss: 0.12678604477393163, dist: 0.01924376820596308\n",
            "--------------------------------------------------\n",
            "iter: 56 \tloss: 0.12440664082482554, dist: 0.01898859748106981\n",
            "--------------------------------------------------\n",
            "iter: 57 \tloss: 0.11327724284943187, dist: 0.016388362085100385\n",
            "--------------------------------------------------\n",
            "iter: 58 \tloss: 0.13215937979586725, dist: 0.019735522396853604\n",
            "--------------------------------------------------\n",
            "iter: 59 \tloss: 0.12026350340806903, dist: 0.017188156407727796\n",
            "--------------------------------------------------\n",
            "iter: 60 \tloss: 0.120986318908423, dist: 0.018264856088601764\n",
            "--------------------------------------------------\n",
            "iter: 61 \tloss: 0.12746077710708212, dist: 0.017070243106637937\n",
            "--------------------------------------------------\n",
            "iter: 62 \tloss: 0.11531435449737436, dist: 0.017556476077272218\n",
            "--------------------------------------------------\n",
            "iter: 63 \tloss: 0.13300808128361463, dist: 0.017799096703985988\n",
            "--------------------------------------------------\n",
            "iter: 64 \tloss: 0.12374392345237388, dist: 0.01741661409320998\n",
            "--------------------------------------------------\n",
            "iter: 65 \tloss: 0.1270318399929962, dist: 0.019152377306956306\n",
            "--------------------------------------------------\n",
            "iter: 66 \tloss: 0.11526589267956068, dist: 0.017420661659166274\n",
            "--------------------------------------------------\n",
            "iter: 67 \tloss: 0.11571224265146117, dist: 0.017660793588775245\n",
            "--------------------------------------------------\n",
            "iter: 68 \tloss: 0.12078510975830552, dist: 0.018960265928120272\n",
            "--------------------------------------------------\n",
            "iter: 69 \tloss: 0.1206286541792521, dist: 0.016764012411292203\n",
            "--------------------------------------------------\n",
            "iter: 70 \tloss: 0.12102937783758452, dist: 0.01695571463361696\n",
            "--------------------------------------------------\n",
            "iter: 71 \tloss: 0.12145195484111325, dist: 0.016899593568815135\n",
            "--------------------------------------------------\n",
            "iter: 72 \tloss: 0.11740498386801056, dist: 0.016616747779426524\n",
            "--------------------------------------------------\n",
            "iter: 73 \tloss: 0.11458318372572998, dist: 0.015900719601462013\n",
            "--------------------------------------------------\n",
            "iter: 74 \tloss: 0.12079492313303967, dist: 0.017570696498657945\n",
            "--------------------------------------------------\n",
            "iter: 75 \tloss: 0.11727864259484305, dist: 0.016498499321378517\n",
            "--------------------------------------------------\n",
            "iter: 76 \tloss: 0.12244612730211638, dist: 0.01704866480039289\n",
            "--------------------------------------------------\n",
            "iter: 77 \tloss: 0.1242170625343911, dist: 0.015573603796252599\n",
            "--------------------------------------------------\n",
            "iter: 78 \tloss: 0.12641848556892749, dist: 0.01778230062585183\n",
            "--------------------------------------------------\n",
            "iter: 79 \tloss: 0.11246911417660789, dist: 0.016214897114137164\n",
            "--------------------------------------------------\n",
            "iter: 80 \tloss: 0.12227073100457844, dist: 0.01754861148944112\n",
            "--------------------------------------------------\n",
            "iter: 81 \tloss: 0.11623178732824258, dist: 0.01520881834615988\n",
            "--------------------------------------------------\n",
            "iter: 82 \tloss: 0.11746911219885874, dist: 0.016408083859849133\n",
            "--------------------------------------------------\n",
            "iter: 83 \tloss: 0.11410633328157017, dist: 0.015871569964754157\n",
            "--------------------------------------------------\n",
            "iter: 84 \tloss: 0.12310661738497676, dist: 0.016000340362363988\n",
            "--------------------------------------------------\n",
            "iter: 85 \tloss: 0.12543027383177444, dist: 0.017687791573904075\n",
            "--------------------------------------------------\n",
            "iter: 86 \tloss: 0.13711981454314875, dist: 0.01452307747509038\n",
            "--------------------------------------------------\n",
            "iter: 87 \tloss: 0.12251718737112695, dist: 0.017929706889080084\n",
            "--------------------------------------------------\n",
            "iter: 88 \tloss: 0.12088311937640116, dist: 0.016402408085649408\n",
            "--------------------------------------------------\n",
            "iter: 89 \tloss: 0.11283924205156598, dist: 0.01575669130382877\n",
            "--------------------------------------------------\n",
            "iter: 90 \tloss: 0.11998861556506991, dist: 0.01589413282302177\n",
            "--------------------------------------------------\n",
            "iter: 91 \tloss: 0.12776635737459963, dist: 0.016131492910095073\n",
            "--------------------------------------------------\n",
            "iter: 92 \tloss: 0.11826926754867556, dist: 0.01661105248133524\n",
            "--------------------------------------------------\n",
            "iter: 93 \tloss: 0.1235657067954496, dist: 0.015742162062042875\n",
            "--------------------------------------------------\n",
            "iter: 94 \tloss: 0.12458701830025452, dist: 0.01626467121243647\n",
            "--------------------------------------------------\n",
            "iter: 95 \tloss: 0.11873954561484186, dist: 0.01578421144958211\n",
            "--------------------------------------------------\n",
            "iter: 96 \tloss: 0.11682365600611816, dist: 0.015041204142332891\n",
            "--------------------------------------------------\n",
            "iter: 97 \tloss: 0.12147444322953525, dist: 0.015209454370067514\n",
            "--------------------------------------------------\n",
            "iter: 98 \tloss: 0.13413612143388146, dist: 0.01627583683099601\n",
            "--------------------------------------------------\n",
            "iter: 99 \tloss: 0.1106653917836396, dist: 0.014844687321251228\n",
            "--------------------------------------------------\n",
            "iter: 100 \tloss: 0.1270358489747242, dist: 0.015174848483013802\n",
            "--------------------------------------------------\n",
            "iter: 101 \tloss: 0.11339691019426726, dist: 0.01454263524908777\n",
            "--------------------------------------------------\n",
            "iter: 102 \tloss: 0.1259897044515862, dist: 0.015408790246211139\n",
            "--------------------------------------------------\n",
            "iter: 103 \tloss: 0.1169631726112124, dist: 0.015815065623488583\n",
            "--------------------------------------------------\n",
            "iter: 104 \tloss: 0.11761477182254176, dist: 0.014120248470288895\n",
            "--------------------------------------------------\n",
            "iter: 105 \tloss: 0.11621646726435367, dist: 0.014373029567726382\n",
            "--------------------------------------------------\n",
            "iter: 106 \tloss: 0.12336064855878334, dist: 0.015021256256718275\n",
            "--------------------------------------------------\n",
            "iter: 107 \tloss: 0.11731190038721245, dist: 0.014947264986318664\n",
            "--------------------------------------------------\n",
            "iter: 108 \tloss: 0.1262675124142395, dist: 0.015073859474697549\n",
            "--------------------------------------------------\n",
            "iter: 109 \tloss: 0.10923816127678898, dist: 0.013677537822953695\n",
            "--------------------------------------------------\n",
            "iter: 110 \tloss: 0.13891638413447482, dist: 0.016658621938159632\n",
            "--------------------------------------------------\n",
            "iter: 111 \tloss: 0.11596197973411716, dist: 0.013377342846148282\n",
            "--------------------------------------------------\n",
            "iter: 112 \tloss: 0.11582553703493968, dist: 0.013868198861805549\n",
            "--------------------------------------------------\n",
            "iter: 113 \tloss: 0.12319837761146105, dist: 0.01599756525280356\n",
            "--------------------------------------------------\n",
            "iter: 114 \tloss: 0.12314279040054829, dist: 0.013743769948375438\n",
            "--------------------------------------------------\n",
            "iter: 115 \tloss: 0.11644735860838668, dist: 0.014727775413977167\n",
            "--------------------------------------------------\n",
            "iter: 116 \tloss: 0.12202537414722428, dist: 0.014047139009864519\n",
            "--------------------------------------------------\n",
            "iter: 117 \tloss: 0.12329340521647882, dist: 0.014510329632074338\n",
            "--------------------------------------------------\n",
            "iter: 118 \tloss: 0.11379827190490621, dist: 0.013794047053647635\n",
            "--------------------------------------------------\n",
            "iter: 119 \tloss: 0.1283796719799967, dist: 0.013724239733096544\n",
            "--------------------------------------------------\n",
            "iter: 120 \tloss: 0.11927806828176447, dist: 0.013199307010106028\n",
            "--------------------------------------------------\n",
            "iter: 121 \tloss: 0.11511973020104126, dist: 0.014730646014181246\n",
            "--------------------------------------------------\n",
            "iter: 122 \tloss: 0.136894403796758, dist: 0.015512641025665075\n",
            "--------------------------------------------------\n",
            "iter: 123 \tloss: 0.12027168977913526, dist: 0.014453057442603728\n",
            "--------------------------------------------------\n",
            "iter: 124 \tloss: 0.12799770133471117, dist: 0.01492285148003904\n",
            "--------------------------------------------------\n",
            "iter: 125 \tloss: 0.11704298643872763, dist: 0.014050369829365132\n",
            "--------------------------------------------------\n",
            "iter: 126 \tloss: 0.12822045589895345, dist: 0.013434133845338193\n",
            "--------------------------------------------------\n",
            "iter: 127 \tloss: 0.10699355321256034, dist: 0.012911236992848584\n",
            "--------------------------------------------------\n",
            "iter: 128 \tloss: 0.13058248399661612, dist: 0.014621864430990457\n",
            "--------------------------------------------------\n",
            "iter: 129 \tloss: 0.13638568575179863, dist: 0.012675500153825769\n",
            "--------------------------------------------------\n",
            "iter: 130 \tloss: 0.12050437128273804, dist: 0.014747941614009559\n",
            "--------------------------------------------------\n",
            "iter: 131 \tloss: 0.12675140010106134, dist: 0.013001034599591151\n",
            "--------------------------------------------------\n",
            "iter: 132 \tloss: 0.12199069360222142, dist: 0.014273117168643072\n",
            "--------------------------------------------------\n",
            "iter: 133 \tloss: 0.11171823851804348, dist: 0.011762019569302834\n",
            "--------------------------------------------------\n",
            "iter: 134 \tloss: 0.1160670986091385, dist: 0.012948091036763282\n",
            "--------------------------------------------------\n",
            "iter: 135 \tloss: 0.13224012705983013, dist: 0.01392854831675317\n",
            "--------------------------------------------------\n",
            "iter: 136 \tloss: 0.1275350726195812, dist: 0.01488623312528817\n",
            "--------------------------------------------------\n",
            "iter: 137 \tloss: 0.1186955295841054, dist: 0.01310451710088119\n",
            "--------------------------------------------------\n",
            "iter: 138 \tloss: 0.12008736121162168, dist: 0.013491279186176292\n",
            "--------------------------------------------------\n",
            "iter: 139 \tloss: 0.12076146914267673, dist: 0.013130815367371757\n",
            "--------------------------------------------------\n",
            "iter: 140 \tloss: 0.1188462689238159, dist: 0.012884100949250942\n",
            "--------------------------------------------------\n",
            "iter: 141 \tloss: 0.12331532652617695, dist: 0.013815121234444132\n",
            "--------------------------------------------------\n",
            "iter: 142 \tloss: 0.11990456563190462, dist: 0.013935937432344036\n",
            "--------------------------------------------------\n",
            "iter: 143 \tloss: 0.12059674488008755, dist: 0.012363037768678616\n",
            "--------------------------------------------------\n",
            "iter: 144 \tloss: 0.12922134788365963, dist: 0.014163469750061731\n",
            "--------------------------------------------------\n",
            "iter: 145 \tloss: 0.12516096906918778, dist: 0.011878194986890876\n",
            "--------------------------------------------------\n",
            "iter: 146 \tloss: 0.12906585342876176, dist: 0.012974142370425257\n",
            "--------------------------------------------------\n",
            "iter: 147 \tloss: 0.12233632581128763, dist: 0.011455524622805269\n",
            "--------------------------------------------------\n",
            "iter: 148 \tloss: 0.11947036469610874, dist: 0.015460063766748827\n",
            "--------------------------------------------------\n",
            "iter: 149 \tloss: 0.12221649562329784, dist: 0.012065456703232148\n",
            "--------------------------------------------------\n",
            "iter: 150 \tloss: 0.11367555293496022, dist: 0.011665551143446922\n",
            "--------------------------------------------------\n",
            "iter: 151 \tloss: 0.12757555713699495, dist: 0.013595796316960675\n",
            "--------------------------------------------------\n",
            "iter: 152 \tloss: 0.11739161660964335, dist: 0.012009898304692202\n",
            "--------------------------------------------------\n",
            "iter: 153 \tloss: 0.13003971344955578, dist: 0.012410940898572097\n",
            "--------------------------------------------------\n",
            "iter: 154 \tloss: 0.11213313274032007, dist: 0.012843937348860811\n",
            "--------------------------------------------------\n",
            "iter: 155 \tloss: 0.12319170238746747, dist: 0.013507380270197357\n",
            "--------------------------------------------------\n",
            "iter: 156 \tloss: 0.11228773607118576, dist: 0.011753582198856694\n",
            "--------------------------------------------------\n",
            "iter: 157 \tloss: 0.12403416412059856, dist: 0.012629776912838224\n",
            "--------------------------------------------------\n",
            "iter: 158 \tloss: 0.12243263031339256, dist: 0.012372068806886643\n",
            "--------------------------------------------------\n",
            "iter: 159 \tloss: 0.12114838640035774, dist: 0.012039594423115939\n",
            "--------------------------------------------------\n",
            "iter: 160 \tloss: 0.1171465223529959, dist: 0.011747824196618857\n",
            "--------------------------------------------------\n",
            "iter: 161 \tloss: 0.12181087515593511, dist: 0.012308365879771348\n",
            "--------------------------------------------------\n",
            "iter: 162 \tloss: 0.1204276016852246, dist: 0.01306789458969313\n",
            "--------------------------------------------------\n",
            "iter: 163 \tloss: 0.12024865057580257, dist: 0.012862689575670622\n",
            "--------------------------------------------------\n",
            "iter: 164 \tloss: 0.117506295861326, dist: 0.011453703676056836\n",
            "--------------------------------------------------\n",
            "iter: 165 \tloss: 0.12234510517980907, dist: 0.012122669865848727\n",
            "--------------------------------------------------\n",
            "iter: 166 \tloss: 0.12960782304275112, dist: 0.011519895127814676\n",
            "--------------------------------------------------\n",
            "iter: 167 \tloss: 0.11377027031171187, dist: 0.012134222292679863\n",
            "--------------------------------------------------\n",
            "iter: 168 \tloss: 0.1520611927127871, dist: 0.01585722824954297\n",
            "--------------------------------------------------\n",
            "iter: 169 \tloss: 0.11819090037086538, dist: 0.012077903424254556\n",
            "--------------------------------------------------\n",
            "iter: 170 \tloss: 0.1455049115879443, dist: 0.010940992848326291\n",
            "--------------------------------------------------\n",
            "iter: 171 \tloss: 0.11866064264296684, dist: 0.011697914195667397\n",
            "--------------------------------------------------\n",
            "iter: 172 \tloss: 0.12436570825985017, dist: 0.01241509249365889\n",
            "--------------------------------------------------\n",
            "iter: 173 \tloss: 0.12352934970423636, dist: 0.010738236967362903\n",
            "--------------------------------------------------\n",
            "iter: 174 \tloss: 0.11679638238467431, dist: 0.012604187200405643\n",
            "--------------------------------------------------\n",
            "iter: 175 \tloss: 0.11442207218579449, dist: 0.01028893340978308\n",
            "--------------------------------------------------\n",
            "iter: 176 \tloss: 0.11537373582456777, dist: 0.011994756124520379\n",
            "--------------------------------------------------\n",
            "iter: 177 \tloss: 0.12141518072975539, dist: 0.01126992914977707\n",
            "--------------------------------------------------\n",
            "iter: 178 \tloss: 0.13026545133565512, dist: 0.01229853908912787\n",
            "--------------------------------------------------\n",
            "iter: 179 \tloss: 0.1207181104520218, dist: 0.011353198979522531\n",
            "--------------------------------------------------\n",
            "iter: 180 \tloss: 0.1170767135287932, dist: 0.011615852118964804\n",
            "--------------------------------------------------\n",
            "iter: 181 \tloss: 0.1268144597680232, dist: 0.01273028378739478\n",
            "--------------------------------------------------\n",
            "iter: 182 \tloss: 0.11312025147397455, dist: 0.011230393473806603\n",
            "--------------------------------------------------\n",
            "iter: 183 \tloss: 0.11962920194208979, dist: 0.012196446165782478\n",
            "--------------------------------------------------\n",
            "iter: 184 \tloss: 0.12094591514445709, dist: 0.01223550888112761\n",
            "--------------------------------------------------\n",
            "iter: 185 \tloss: 0.12571797708210422, dist: 0.011959290423116607\n",
            "--------------------------------------------------\n",
            "iter: 186 \tloss: 0.12880671699779023, dist: 0.011774999589230407\n",
            "--------------------------------------------------\n",
            "iter: 187 \tloss: 0.11614321983166799, dist: 0.010840631403577926\n",
            "--------------------------------------------------\n",
            "iter: 188 \tloss: 0.13822467391184634, dist: 0.01086588873172367\n",
            "--------------------------------------------------\n",
            "iter: 189 \tloss: 0.11297604658171792, dist: 0.011310632828698444\n",
            "--------------------------------------------------\n",
            "iter: 190 \tloss: 0.1285083395109109, dist: 0.010759168248447106\n",
            "--------------------------------------------------\n",
            "iter: 191 \tloss: 0.127785362723464, dist: 0.011456226448630891\n",
            "--------------------------------------------------\n",
            "iter: 192 \tloss: 0.11462182968346152, dist: 0.010642649902320687\n",
            "--------------------------------------------------\n",
            "iter: 193 \tloss: 0.12094244416894653, dist: 0.010425990463416248\n",
            "--------------------------------------------------\n",
            "iter: 194 \tloss: 0.11460321919207207, dist: 0.011445889987691792\n",
            "--------------------------------------------------\n",
            "iter: 195 \tloss: 0.14713720272733075, dist: 0.011140187336509831\n",
            "--------------------------------------------------\n",
            "iter: 196 \tloss: 0.10561378339687981, dist: 0.009307413547185539\n",
            "--------------------------------------------------\n",
            "iter: 197 \tloss: 0.12699381113911742, dist: 0.011050498332479355\n",
            "--------------------------------------------------\n",
            "iter: 198 \tloss: 0.11363955102995658, dist: 0.00996044618164779\n",
            "--------------------------------------------------\n",
            "iter: 199 \tloss: 0.12025499542755023, dist: 0.010827414715541793\n",
            "--------------------------------------------------\n",
            "iter: 200 \tloss: 0.12249399901026198, dist: 0.011035118887745436\n",
            "--------------------------------------------------\n",
            "iter: 201 \tloss: 0.11587440266668106, dist: 0.0104308792473211\n",
            "--------------------------------------------------\n",
            "iter: 202 \tloss: 0.11639262822197059, dist: 0.009940746485281933\n",
            "--------------------------------------------------\n",
            "iter: 203 \tloss: 0.13018024494334265, dist: 0.01103789043305792\n",
            "--------------------------------------------------\n",
            "iter: 204 \tloss: 0.11719748226704235, dist: 0.010094525958511737\n",
            "--------------------------------------------------\n",
            "iter: 205 \tloss: 0.13190226942069583, dist: 0.01046602666822182\n",
            "--------------------------------------------------\n",
            "iter: 206 \tloss: 0.12563608102192744, dist: 0.010178811995782938\n",
            "--------------------------------------------------\n",
            "iter: 207 \tloss: 0.1201310529708966, dist: 0.010455530946176878\n",
            "--------------------------------------------------\n",
            "iter: 208 \tloss: 0.12108805440781865, dist: 0.0102377417000019\n",
            "--------------------------------------------------\n",
            "iter: 209 \tloss: 0.12073921252173213, dist: 0.010637787610837517\n",
            "--------------------------------------------------\n",
            "iter: 210 \tloss: 0.12133173129859719, dist: 0.010058600180828044\n",
            "--------------------------------------------------\n",
            "iter: 211 \tloss: 0.11562562439410896, dist: 0.010045818634577203\n",
            "--------------------------------------------------\n",
            "iter: 212 \tloss: 0.12239552445330722, dist: 0.010237945461867522\n",
            "--------------------------------------------------\n",
            "iter: 213 \tloss: 0.11769096037271792, dist: 0.0100167982793029\n",
            "--------------------------------------------------\n",
            "iter: 214 \tloss: 0.11238178064216563, dist: 0.0086225438462315\n",
            "--------------------------------------------------\n",
            "iter: 215 \tloss: 0.12519376050601808, dist: 0.010933275560934277\n",
            "--------------------------------------------------\n",
            "iter: 216 \tloss: 0.12146354292162642, dist: 0.009775409480178797\n",
            "--------------------------------------------------\n",
            "iter: 217 \tloss: 0.1219455150799942, dist: 0.009840272902061601\n",
            "--------------------------------------------------\n",
            "iter: 218 \tloss: 0.10949128665688454, dist: 0.0095896112107251\n",
            "--------------------------------------------------\n",
            "iter: 219 \tloss: 0.12213888983062554, dist: 0.009619471178976406\n",
            "--------------------------------------------------\n",
            "iter: 220 \tloss: 0.11965948932884024, dist: 0.009205889076776948\n",
            "--------------------------------------------------\n",
            "iter: 221 \tloss: 0.11850660681148238, dist: 0.00960262319820425\n",
            "--------------------------------------------------\n",
            "iter: 222 \tloss: 0.13563300696348696, dist: 0.009555586884700153\n",
            "--------------------------------------------------\n",
            "iter: 223 \tloss: 0.11642101320780478, dist: 0.010159568899540799\n",
            "--------------------------------------------------\n",
            "iter: 224 \tloss: 0.12143563027647394, dist: 0.008965248251999838\n",
            "--------------------------------------------------\n",
            "iter: 225 \tloss: 0.11794495804344773, dist: 0.009395489580469261\n",
            "--------------------------------------------------\n",
            "iter: 226 \tloss: 0.12260212053690492, dist: 0.010041205117635921\n",
            "--------------------------------------------------\n",
            "iter: 227 \tloss: 0.11391502092157456, dist: 0.0088861427575061\n",
            "--------------------------------------------------\n",
            "iter: 228 \tloss: 0.13566359306962625, dist: 0.012150963857720805\n",
            "--------------------------------------------------\n",
            "iter: 229 \tloss: 0.11987533072793016, dist: 0.009248279109667362\n",
            "--------------------------------------------------\n",
            "iter: 230 \tloss: 0.1182722790988675, dist: 0.00917070559088614\n",
            "--------------------------------------------------\n",
            "iter: 231 \tloss: 0.12227858755920568, dist: 0.0092462671600912\n",
            "--------------------------------------------------\n",
            "iter: 232 \tloss: 0.12374600211096932, dist: 0.009122599725644092\n",
            "--------------------------------------------------\n",
            "iter: 233 \tloss: 0.12265902858794991, dist: 0.00906605499507529\n",
            "--------------------------------------------------\n",
            "iter: 234 \tloss: 0.11991916270272746, dist: 0.010005571674609677\n",
            "--------------------------------------------------\n",
            "iter: 235 \tloss: 0.12064753831449893, dist: 0.00930652666300385\n",
            "--------------------------------------------------\n",
            "iter: 236 \tloss: 0.11630183594377591, dist: 0.009398566729736279\n",
            "--------------------------------------------------\n",
            "iter: 237 \tloss: 0.12454737120800351, dist: 0.00896025923616958\n",
            "--------------------------------------------------\n",
            "iter: 238 \tloss: 0.12168596923216578, dist: 0.009403573183325561\n",
            "--------------------------------------------------\n",
            "iter: 239 \tloss: 0.12046683447042544, dist: 0.008034448241018614\n",
            "--------------------------------------------------\n",
            "iter: 240 \tloss: 0.12318155724142005, dist: 0.009306007575652238\n",
            "--------------------------------------------------\n",
            "iter: 241 \tloss: 0.1272863815414821, dist: 0.00899777908418812\n",
            "--------------------------------------------------\n",
            "iter: 242 \tloss: 0.11303087202358451, dist: 0.008259884105138992\n",
            "--------------------------------------------------\n",
            "iter: 243 \tloss: 0.12656903095837205, dist: 0.009083330902239336\n",
            "--------------------------------------------------\n",
            "iter: 244 \tloss: 0.13473262260052585, dist: 0.008622418700495129\n",
            "--------------------------------------------------\n",
            "iter: 245 \tloss: 0.1182924419367517, dist: 0.008073790546907765\n",
            "--------------------------------------------------\n",
            "iter: 246 \tloss: 0.13632851518330813, dist: 0.009916264027593663\n",
            "--------------------------------------------------\n",
            "iter: 247 \tloss: 0.12402005714755115, dist: 0.00947685473449807\n",
            "--------------------------------------------------\n",
            "iter: 248 \tloss: 0.13726200094047256, dist: 0.00834175155613488\n",
            "--------------------------------------------------\n",
            "iter: 249 \tloss: 0.12177613451592516, dist: 0.009240964648576934\n",
            "--------------------------------------------------\n",
            "iter: 250 \tloss: 0.12266654939041671, dist: 0.008102045360822329\n",
            "--------------------------------------------------\n",
            "iter: 251 \tloss: 0.11420057007277777, dist: 0.008612077013765337\n",
            "--------------------------------------------------\n",
            "iter: 252 \tloss: 0.12750097900386184, dist: 0.008756286345777443\n",
            "--------------------------------------------------\n",
            "iter: 253 \tloss: 0.11897433795154665, dist: 0.008680878058399526\n",
            "--------------------------------------------------\n",
            "iter: 254 \tloss: 0.11957573592882897, dist: 0.008376351365079663\n",
            "--------------------------------------------------\n",
            "iter: 255 \tloss: 0.11847619913961022, dist: 0.008453216303261074\n",
            "--------------------------------------------------\n",
            "iter: 256 \tloss: 0.12447723016492991, dist: 0.008970992380328379\n",
            "--------------------------------------------------\n",
            "iter: 257 \tloss: 0.12369027770382836, dist: 0.007917687592252209\n",
            "--------------------------------------------------\n",
            "iter: 258 \tloss: 0.12438040097036898, dist: 0.008818104044446428\n",
            "--------------------------------------------------\n",
            "iter: 259 \tloss: 0.14621736798744578, dist: 0.0113159730903421\n",
            "--------------------------------------------------\n",
            "iter: 260 \tloss: 0.10997217994609625, dist: 0.007985180249103186\n",
            "--------------------------------------------------\n",
            "iter: 261 \tloss: 0.11334608125748837, dist: 0.0077104735937865345\n",
            "--------------------------------------------------\n",
            "iter: 262 \tloss: 0.13182183049087928, dist: 0.00980009224259366\n",
            "--------------------------------------------------\n",
            "iter: 263 \tloss: 0.12330754765616087, dist: 0.00905431554151777\n",
            "--------------------------------------------------\n",
            "iter: 264 \tloss: 0.1285505689705623, dist: 0.008944812197726368\n",
            "--------------------------------------------------\n",
            "iter: 265 \tloss: 0.11515143982041391, dist: 0.008104023962543368\n",
            "--------------------------------------------------\n",
            "iter: 266 \tloss: 0.12475866113419824, dist: 0.007422740764376748\n",
            "--------------------------------------------------\n",
            "iter: 267 \tloss: 0.13556164364667225, dist: 0.00999886684994056\n",
            "--------------------------------------------------\n",
            "iter: 268 \tloss: 0.12319715183081202, dist: 0.008131792270479552\n",
            "--------------------------------------------------\n",
            "iter: 269 \tloss: 0.1149808557361335, dist: 0.007533493050505396\n",
            "--------------------------------------------------\n",
            "iter: 270 \tloss: 0.1165969223649062, dist: 0.008162674028348207\n",
            "--------------------------------------------------\n",
            "iter: 271 \tloss: 0.13034441648612738, dist: 0.007700284117208625\n",
            "--------------------------------------------------\n",
            "iter: 272 \tloss: 0.12450844363897297, dist: 0.008214574711311968\n",
            "--------------------------------------------------\n",
            "iter: 273 \tloss: 0.12403517443975696, dist: 0.008527978022763155\n",
            "--------------------------------------------------\n",
            "iter: 274 \tloss: 0.11989812519517491, dist: 0.007312442978821067\n",
            "--------------------------------------------------\n",
            "iter: 275 \tloss: 0.11573792372059787, dist: 0.007829102966338361\n",
            "--------------------------------------------------\n",
            "iter: 276 \tloss: 0.12489481198073085, dist: 0.008085401619629078\n",
            "--------------------------------------------------\n",
            "iter: 277 \tloss: 0.12721476196104187, dist: 0.008141244863716847\n",
            "--------------------------------------------------\n",
            "iter: 278 \tloss: 0.12899413012039032, dist: 0.008179525837789258\n",
            "--------------------------------------------------\n",
            "iter: 279 \tloss: 0.11761533910313066, dist: 0.0072257102793920475\n",
            "--------------------------------------------------\n",
            "iter: 280 \tloss: 0.13639954911672414, dist: 0.00956672146012192\n",
            "--------------------------------------------------\n",
            "iter: 281 \tloss: 0.11700398955897712, dist: 0.007515300116314555\n",
            "--------------------------------------------------\n",
            "iter: 282 \tloss: 0.12389203834577045, dist: 0.007661123410760048\n",
            "--------------------------------------------------\n",
            "iter: 283 \tloss: 0.12049014077459734, dist: 0.00782377089174441\n",
            "--------------------------------------------------\n",
            "iter: 284 \tloss: 0.13430484413818278, dist: 0.00786688105769819\n",
            "--------------------------------------------------\n",
            "iter: 285 \tloss: 0.1204678424952533, dist: 0.00732542799876744\n",
            "--------------------------------------------------\n",
            "iter: 286 \tloss: 0.12794990507187, dist: 0.007575402217723634\n",
            "--------------------------------------------------\n",
            "iter: 287 \tloss: 0.12646539595641235, dist: 0.00794281056427496\n",
            "--------------------------------------------------\n",
            "iter: 288 \tloss: 0.11363087697361328, dist: 0.007465192440948417\n",
            "--------------------------------------------------\n",
            "iter: 289 \tloss: 0.11761662327721592, dist: 0.00708174752596657\n",
            "--------------------------------------------------\n",
            "iter: 290 \tloss: 0.12699887316400338, dist: 0.007597718495441595\n",
            "--------------------------------------------------\n",
            "iter: 291 \tloss: 0.11865430170296505, dist: 0.0071041168052046605\n",
            "--------------------------------------------------\n",
            "iter: 292 \tloss: 0.12919530605549723, dist: 0.007795580995747275\n",
            "--------------------------------------------------\n",
            "iter: 293 \tloss: 0.1251225264932474, dist: 0.006628242438690654\n",
            "--------------------------------------------------\n",
            "iter: 294 \tloss: 0.12016048218405789, dist: 0.008180887454776574\n",
            "--------------------------------------------------\n",
            "iter: 295 \tloss: 0.12453095462880558, dist: 0.007456804381824711\n",
            "--------------------------------------------------\n",
            "iter: 296 \tloss: 0.12101571239768973, dist: 0.006970716902407151\n",
            "--------------------------------------------------\n",
            "iter: 297 \tloss: 0.11636157767576817, dist: 0.007705448193230799\n",
            "--------------------------------------------------\n",
            "iter: 298 \tloss: 0.12487426029392967, dist: 0.006982811151421541\n",
            "--------------------------------------------------\n",
            "iter: 299 \tloss: 0.12300163677337449, dist: 0.007192727843908292\n",
            "--------------------------------------------------\n",
            "iter: 300 \tloss: 0.12360883791377293, dist: 0.007040332267463206\n",
            "--------------------------------------------------\n",
            "iter: 301 \tloss: 0.12487097951540779, dist: 0.006994762893331393\n",
            "--------------------------------------------------\n",
            "iter: 302 \tloss: 0.12002695499967539, dist: 0.006856247662767572\n",
            "--------------------------------------------------\n",
            "iter: 303 \tloss: 0.11735150753796829, dist: 0.006766626432377235\n",
            "--------------------------------------------------\n",
            "iter: 304 \tloss: 0.12219086570991063, dist: 0.007659872472330461\n",
            "--------------------------------------------------\n",
            "iter: 305 \tloss: 0.11554609493782232, dist: 0.0067577964220632385\n",
            "--------------------------------------------------\n",
            "iter: 306 \tloss: 0.1313994830083691, dist: 0.007671770168351749\n",
            "--------------------------------------------------\n",
            "iter: 307 \tloss: 0.12393430924001753, dist: 0.007047220709918198\n",
            "--------------------------------------------------\n",
            "iter: 308 \tloss: 0.11916996763684352, dist: 0.006533873586048988\n",
            "--------------------------------------------------\n",
            "iter: 309 \tloss: 0.1123742522633572, dist: 0.006658830966667536\n",
            "--------------------------------------------------\n",
            "iter: 310 \tloss: 0.1221283066026536, dist: 0.006649895028345248\n",
            "--------------------------------------------------\n",
            "iter: 311 \tloss: 0.1276753065680487, dist: 0.007159280783235905\n",
            "--------------------------------------------------\n",
            "iter: 312 \tloss: 0.10624150550640522, dist: 0.006114615252984443\n",
            "--------------------------------------------------\n",
            "iter: 313 \tloss: 0.12005040257984248, dist: 0.006741333159040499\n",
            "--------------------------------------------------\n",
            "iter: 314 \tloss: 0.11973042779347155, dist: 0.006424785177951911\n",
            "--------------------------------------------------\n",
            "iter: 315 \tloss: 0.1225477308773346, dist: 0.0075730548139333114\n",
            "--------------------------------------------------\n",
            "iter: 316 \tloss: 0.12521575574389718, dist: 0.0056783244769538785\n",
            "--------------------------------------------------\n",
            "iter: 317 \tloss: 0.12702784744449294, dist: 0.007350465422966492\n",
            "--------------------------------------------------\n",
            "iter: 318 \tloss: 0.12783323122079193, dist: 0.006402512172004938\n",
            "--------------------------------------------------\n",
            "iter: 319 \tloss: 0.11750712625617674, dist: 0.006394469835650063\n",
            "--------------------------------------------------\n",
            "iter: 320 \tloss: 0.12071181312800289, dist: 0.0069387662844666185\n",
            "--------------------------------------------------\n",
            "iter: 321 \tloss: 0.12604557580056244, dist: 0.007029754498852703\n",
            "--------------------------------------------------\n",
            "iter: 322 \tloss: 0.13145541595772675, dist: 0.007055301497823152\n",
            "--------------------------------------------------\n",
            "iter: 323 \tloss: 0.12138415642994524, dist: 0.006451797948803784\n",
            "--------------------------------------------------\n",
            "iter: 324 \tloss: 0.11980081814879401, dist: 0.006323397898791836\n",
            "--------------------------------------------------\n",
            "iter: 325 \tloss: 0.12606260398671124, dist: 0.006468833466882287\n",
            "--------------------------------------------------\n",
            "iter: 326 \tloss: 0.11788260494827588, dist: 0.006311660206529054\n",
            "--------------------------------------------------\n",
            "iter: 327 \tloss: 0.12134874572683452, dist: 0.006426796542057373\n",
            "--------------------------------------------------\n",
            "iter: 328 \tloss: 0.11862928284224435, dist: 0.006355584795171363\n",
            "--------------------------------------------------\n",
            "iter: 329 \tloss: 0.12426679075489452, dist: 0.007383440506052442\n",
            "--------------------------------------------------\n",
            "iter: 330 \tloss: 0.12243263982325647, dist: 0.006470932436174573\n",
            "--------------------------------------------------\n",
            "iter: 331 \tloss: 0.11618895064612005, dist: 0.0055717558586004055\n",
            "--------------------------------------------------\n",
            "iter: 332 \tloss: 0.1279855826982206, dist: 0.006275958035413746\n",
            "--------------------------------------------------\n",
            "iter: 333 \tloss: 0.11828543157296027, dist: 0.006311061270757579\n",
            "--------------------------------------------------\n",
            "iter: 334 \tloss: 0.12486678743371948, dist: 0.006140699001450089\n",
            "--------------------------------------------------\n",
            "iter: 335 \tloss: 0.11423275839883756, dist: 0.00589014281857849\n",
            "--------------------------------------------------\n",
            "iter: 336 \tloss: 0.11852295180136768, dist: 0.005700913891667219\n",
            "--------------------------------------------------\n",
            "iter: 337 \tloss: 0.11281689915704506, dist: 0.006138719272386847\n",
            "--------------------------------------------------\n",
            "iter: 338 \tloss: 0.12280214725567541, dist: 0.005725925456664218\n",
            "--------------------------------------------------\n",
            "iter: 339 \tloss: 0.12396898621014722, dist: 0.006430194475890208\n",
            "--------------------------------------------------\n",
            "iter: 340 \tloss: 0.12437759915313018, dist: 0.006249713863105229\n",
            "--------------------------------------------------\n",
            "iter: 341 \tloss: 0.11538205762041163, dist: 0.005306641026870013\n",
            "--------------------------------------------------\n",
            "iter: 342 \tloss: 0.12030695038108306, dist: 0.005726583522367315\n",
            "--------------------------------------------------\n",
            "iter: 343 \tloss: 0.11753952043191294, dist: 0.005650290456501599\n",
            "--------------------------------------------------\n",
            "iter: 344 \tloss: 0.12646465643921276, dist: 0.006366667802072903\n",
            "--------------------------------------------------\n",
            "iter: 345 \tloss: 0.12535756795981617, dist: 0.005898313045374638\n",
            "--------------------------------------------------\n",
            "iter: 346 \tloss: 0.12066630543949718, dist: 0.0057331545339133335\n",
            "--------------------------------------------------\n",
            "iter: 347 \tloss: 0.12108713715429542, dist: 0.005618016912869386\n",
            "--------------------------------------------------\n",
            "iter: 348 \tloss: 0.11598947080678586, dist: 0.005596066607410849\n",
            "--------------------------------------------------\n",
            "iter: 349 \tloss: 0.11727646862057303, dist: 0.005816686377422841\n",
            "--------------------------------------------------\n",
            "iter: 350 \tloss: 0.11591265859892648, dist: 0.005574548252009848\n",
            "--------------------------------------------------\n",
            "iter: 351 \tloss: 0.1262366607892855, dist: 0.005394057701763885\n",
            "--------------------------------------------------\n",
            "iter: 352 \tloss: 0.12720171766373767, dist: 0.00591981260027657\n",
            "--------------------------------------------------\n",
            "iter: 353 \tloss: 0.1258101998920977, dist: 0.005929408731163119\n",
            "--------------------------------------------------\n",
            "iter: 354 \tloss: 0.12109704108758934, dist: 0.006517598554104467\n",
            "--------------------------------------------------\n",
            "iter: 355 \tloss: 0.12835505938424044, dist: 0.005043604815339689\n",
            "--------------------------------------------------\n",
            "iter: 356 \tloss: 0.1264849251152918, dist: 0.0058665304907561675\n",
            "--------------------------------------------------\n",
            "iter: 357 \tloss: 0.11218008133200015, dist: 0.005263655457978712\n",
            "--------------------------------------------------\n",
            "iter: 358 \tloss: 0.12640080976900475, dist: 0.005572085710030549\n",
            "--------------------------------------------------\n",
            "iter: 359 \tloss: 0.11950979536279616, dist: 0.005803782703288112\n",
            "--------------------------------------------------\n",
            "iter: 360 \tloss: 0.11553361586533488, dist: 0.0050085666714664795\n",
            "--------------------------------------------------\n",
            "iter: 361 \tloss: 0.12378496031311652, dist: 0.005045944098845256\n",
            "--------------------------------------------------\n",
            "iter: 362 \tloss: 0.11518320263211167, dist: 0.005252320599593935\n",
            "--------------------------------------------------\n",
            "iter: 363 \tloss: 0.12240363933486158, dist: 0.005222974955005209\n",
            "--------------------------------------------------\n",
            "iter: 364 \tloss: 0.12236172784403482, dist: 0.005175174208646053\n",
            "--------------------------------------------------\n",
            "iter: 365 \tloss: 0.12067813839909775, dist: 0.005170109111946649\n",
            "--------------------------------------------------\n",
            "iter: 366 \tloss: 0.12099596413856538, dist: 0.005424657389708133\n",
            "--------------------------------------------------\n",
            "iter: 367 \tloss: 0.1256714701819002, dist: 0.005061888857462102\n",
            "--------------------------------------------------\n",
            "iter: 368 \tloss: 0.11955963339069604, dist: 0.005177339897099431\n",
            "--------------------------------------------------\n",
            "iter: 369 \tloss: 0.12467420226504436, dist: 0.005262953296272985\n",
            "--------------------------------------------------\n",
            "iter: 370 \tloss: 0.12646098125839453, dist: 0.005240794074183963\n",
            "--------------------------------------------------\n",
            "iter: 371 \tloss: 0.12131274872637234, dist: 0.004927850342173706\n",
            "--------------------------------------------------\n",
            "iter: 372 \tloss: 0.12373494815013539, dist: 0.0049354743189634195\n",
            "--------------------------------------------------\n",
            "iter: 373 \tloss: 0.12335588845847996, dist: 0.006236177690729052\n",
            "--------------------------------------------------\n",
            "iter: 374 \tloss: 0.12219053858141891, dist: 0.00490198753168799\n",
            "--------------------------------------------------\n",
            "iter: 375 \tloss: 0.11704991390828572, dist: 0.005131684407136575\n",
            "--------------------------------------------------\n",
            "iter: 376 \tloss: 0.12584500185498818, dist: 0.005064892719561975\n",
            "--------------------------------------------------\n",
            "iter: 377 \tloss: 0.12444253718949186, dist: 0.004985862066185384\n",
            "--------------------------------------------------\n",
            "iter: 378 \tloss: 0.12098505020908902, dist: 0.004650231283226102\n",
            "--------------------------------------------------\n",
            "iter: 379 \tloss: 0.12195019807434077, dist: 0.0045149659597230616\n",
            "--------------------------------------------------\n",
            "iter: 380 \tloss: 0.13415480251591352, dist: 0.005235307059921137\n",
            "--------------------------------------------------\n",
            "iter: 381 \tloss: 0.13514563942185231, dist: 0.005005179545149582\n",
            "--------------------------------------------------\n",
            "iter: 382 \tloss: 0.12439572148736494, dist: 0.004605595368428971\n",
            "--------------------------------------------------\n",
            "iter: 383 \tloss: 0.12378301634462777, dist: 0.005524720205467072\n",
            "--------------------------------------------------\n",
            "iter: 384 \tloss: 0.11075689936884564, dist: 0.004740717204212478\n",
            "--------------------------------------------------\n",
            "iter: 385 \tloss: 0.12407079952193832, dist: 0.0044456394206792505\n",
            "--------------------------------------------------\n",
            "iter: 386 \tloss: 0.11811386741725588, dist: 0.004685461213513914\n",
            "--------------------------------------------------\n",
            "iter: 387 \tloss: 0.11953231927698381, dist: 0.004916625843460394\n",
            "--------------------------------------------------\n",
            "iter: 388 \tloss: 0.12245489031942342, dist: 0.004517773785453195\n",
            "--------------------------------------------------\n",
            "iter: 389 \tloss: 0.11670620673077493, dist: 0.004806717835979453\n",
            "--------------------------------------------------\n",
            "iter: 390 \tloss: 0.11639351759912167, dist: 0.004183611379022697\n",
            "--------------------------------------------------\n",
            "iter: 391 \tloss: 0.12205861999582805, dist: 0.00468446873966691\n",
            "--------------------------------------------------\n",
            "iter: 392 \tloss: 0.12739646574813432, dist: 0.005249001689117907\n",
            "--------------------------------------------------\n",
            "iter: 393 \tloss: 0.13117299519447323, dist: 0.005334268791509402\n",
            "--------------------------------------------------\n",
            "iter: 394 \tloss: 0.11755855887637318, dist: 0.0045180028857646545\n",
            "--------------------------------------------------\n",
            "iter: 395 \tloss: 0.11943249987654031, dist: 0.004236893215154833\n",
            "--------------------------------------------------\n",
            "iter: 396 \tloss: 0.12720231395462084, dist: 0.005155711744228072\n",
            "--------------------------------------------------\n",
            "iter: 397 \tloss: 0.12402426158515674, dist: 0.004457308730502061\n",
            "--------------------------------------------------\n",
            "iter: 398 \tloss: 0.1114657450158747, dist: 0.004501222188099566\n",
            "--------------------------------------------------\n",
            "iter: 399 \tloss: 0.12224678976399317, dist: 0.00393358859705235\n",
            "--------------------------------------------------\n",
            "iter: 400 \tloss: 0.15113366042858326, dist: 0.005549225907401417\n",
            "--------------------------------------------------\n",
            "iter: 401 \tloss: 0.11789020103263254, dist: 0.0037695418652505387\n",
            "--------------------------------------------------\n",
            "iter: 402 \tloss: 0.12768388183134055, dist: 0.0050756645292963695\n",
            "--------------------------------------------------\n",
            "iter: 403 \tloss: 0.1158206113169813, dist: 0.0036134660822863154\n",
            "--------------------------------------------------\n",
            "iter: 404 \tloss: 0.13302332843823048, dist: 0.005364635532805032\n",
            "--------------------------------------------------\n",
            "iter: 405 \tloss: 0.1168233481434899, dist: 0.0044599129647790335\n",
            "--------------------------------------------------\n",
            "iter: 406 \tloss: 0.11066905818998522, dist: 0.003972722417767045\n",
            "--------------------------------------------------\n",
            "iter: 407 \tloss: 0.13149703249238504, dist: 0.004497067297338981\n",
            "--------------------------------------------------\n",
            "iter: 408 \tloss: 0.10967282427953338, dist: 0.003788450220844797\n",
            "--------------------------------------------------\n",
            "iter: 409 \tloss: 0.13218613227908052, dist: 0.004630034276496172\n",
            "--------------------------------------------------\n",
            "iter: 410 \tloss: 0.10770508834081496, dist: 0.003930098801053193\n",
            "--------------------------------------------------\n",
            "iter: 411 \tloss: 0.11859735906407315, dist: 0.004337870605768981\n",
            "--------------------------------------------------\n",
            "iter: 412 \tloss: 0.12700896305753848, dist: 0.004537549022999871\n",
            "--------------------------------------------------\n",
            "iter: 413 \tloss: 0.1257588699066589, dist: 0.004308928492076684\n",
            "--------------------------------------------------\n",
            "iter: 414 \tloss: 0.12923239053303057, dist: 0.004171013421391223\n",
            "--------------------------------------------------\n",
            "iter: 415 \tloss: 0.12466962649807804, dist: 0.004220856138957304\n",
            "--------------------------------------------------\n",
            "iter: 416 \tloss: 0.12319901644134887, dist: 0.004166888523840392\n",
            "--------------------------------------------------\n",
            "iter: 417 \tloss: 0.13518895011785692, dist: 0.004754644210551649\n",
            "--------------------------------------------------\n",
            "iter: 418 \tloss: 0.11917905996116086, dist: 0.0038127514516605146\n",
            "--------------------------------------------------\n",
            "iter: 419 \tloss: 0.12050656296244243, dist: 0.003925887369904099\n",
            "--------------------------------------------------\n",
            "iter: 420 \tloss: 0.11629017054265128, dist: 0.0039415795117336136\n",
            "--------------------------------------------------\n",
            "iter: 421 \tloss: 0.12255407379042933, dist: 0.003963025557059613\n",
            "--------------------------------------------------\n",
            "iter: 422 \tloss: 0.11973241618310256, dist: 0.004159084108490454\n",
            "--------------------------------------------------\n",
            "iter: 423 \tloss: 0.12123159600326444, dist: 0.004082456181128607\n",
            "--------------------------------------------------\n",
            "iter: 424 \tloss: 0.1337225956552259, dist: 0.004298222265798474\n",
            "--------------------------------------------------\n",
            "iter: 425 \tloss: 0.11313426318221591, dist: 0.003728058889567519\n",
            "--------------------------------------------------\n",
            "iter: 426 \tloss: 0.12583443954080897, dist: 0.0038926317047318667\n",
            "--------------------------------------------------\n",
            "iter: 427 \tloss: 0.12231790528331214, dist: 0.0041018066154061335\n",
            "--------------------------------------------------\n",
            "iter: 428 \tloss: 0.12437516166885626, dist: 0.0037594209434691386\n",
            "--------------------------------------------------\n",
            "iter: 429 \tloss: 0.11424914629840144, dist: 0.0034396906860703954\n",
            "--------------------------------------------------\n",
            "iter: 430 \tloss: 0.13839084842723307, dist: 0.004000350671493998\n",
            "--------------------------------------------------\n",
            "iter: 431 \tloss: 0.11407571721590232, dist: 0.0038953554905267512\n",
            "--------------------------------------------------\n",
            "iter: 432 \tloss: 0.12433996697070691, dist: 0.0038657806214579117\n",
            "--------------------------------------------------\n",
            "iter: 433 \tloss: 0.12049464695139336, dist: 0.00366856279753416\n",
            "--------------------------------------------------\n",
            "iter: 434 \tloss: 0.114404628276765, dist: 0.0035511233760059362\n",
            "--------------------------------------------------\n",
            "iter: 435 \tloss: 0.12269371720618252, dist: 0.003938115940516133\n",
            "--------------------------------------------------\n",
            "iter: 436 \tloss: 0.12247260556508457, dist: 0.003897032204997157\n",
            "--------------------------------------------------\n",
            "iter: 437 \tloss: 0.11669995705656927, dist: 0.0034461103734915316\n",
            "--------------------------------------------------\n",
            "iter: 438 \tloss: 0.12072901589458246, dist: 0.00361279944346554\n",
            "--------------------------------------------------\n",
            "iter: 439 \tloss: 0.1191800616836023, dist: 0.0035725512204854135\n",
            "--------------------------------------------------\n",
            "iter: 440 \tloss: 0.13140982941994545, dist: 0.0036379558518764205\n",
            "--------------------------------------------------\n",
            "iter: 441 \tloss: 0.11845397690727993, dist: 0.0036155669039590397\n",
            "--------------------------------------------------\n",
            "iter: 442 \tloss: 0.10896476481384454, dist: 0.003409611450029231\n",
            "--------------------------------------------------\n",
            "iter: 443 \tloss: 0.1286412905765296, dist: 0.0034605598753872854\n",
            "--------------------------------------------------\n",
            "iter: 444 \tloss: 0.12676439060302852, dist: 0.00350071566362461\n",
            "--------------------------------------------------\n",
            "iter: 445 \tloss: 0.12119630556409827, dist: 0.003634792624840198\n",
            "--------------------------------------------------\n",
            "iter: 446 \tloss: 0.11682661679125655, dist: 0.003340437502652782\n",
            "--------------------------------------------------\n",
            "iter: 447 \tloss: 0.13473436683277093, dist: 0.0035325171241537543\n",
            "--------------------------------------------------\n",
            "iter: 448 \tloss: 0.11746586831476759, dist: 0.003381925854286257\n",
            "--------------------------------------------------\n",
            "iter: 449 \tloss: 0.11539281866714202, dist: 0.0034107858432448697\n",
            "--------------------------------------------------\n",
            "iter: 450 \tloss: 0.1261623980597739, dist: 0.0032614078126138248\n",
            "--------------------------------------------------\n",
            "iter: 451 \tloss: 0.11348504208754083, dist: 0.0034279798453979746\n",
            "--------------------------------------------------\n",
            "iter: 452 \tloss: 0.11258543268151562, dist: 0.003235829208712252\n",
            "--------------------------------------------------\n",
            "iter: 453 \tloss: 0.1344704757496774, dist: 0.003813504204204682\n",
            "--------------------------------------------------\n",
            "iter: 454 \tloss: 0.11838973168331314, dist: 0.0031956915827872495\n",
            "--------------------------------------------------\n",
            "iter: 455 \tloss: 0.12292123800493669, dist: 0.0034306658537334943\n",
            "--------------------------------------------------\n",
            "iter: 456 \tloss: 0.10946063427072228, dist: 0.0029089273044449197\n",
            "--------------------------------------------------\n",
            "iter: 457 \tloss: 0.12910964256063462, dist: 0.0035415041321175786\n",
            "--------------------------------------------------\n",
            "iter: 458 \tloss: 0.1238087564129091, dist: 0.003395341995291254\n",
            "--------------------------------------------------\n",
            "iter: 459 \tloss: 0.12207996309775826, dist: 0.0033704107668327756\n",
            "--------------------------------------------------\n",
            "iter: 460 \tloss: 0.11781948388147113, dist: 0.003365551761933922\n",
            "--------------------------------------------------\n",
            "iter: 461 \tloss: 0.12226671058021181, dist: 0.00329241341805903\n",
            "--------------------------------------------------\n",
            "iter: 462 \tloss: 0.12334332173908935, dist: 0.0032876654895523526\n",
            "--------------------------------------------------\n",
            "iter: 463 \tloss: 0.1291172320650967, dist: 0.0031899858330616897\n",
            "--------------------------------------------------\n",
            "iter: 464 \tloss: 0.12164764998217409, dist: 0.0031333471786980194\n",
            "--------------------------------------------------\n",
            "iter: 465 \tloss: 0.11729571825312357, dist: 0.0031841251193332255\n",
            "--------------------------------------------------\n",
            "iter: 466 \tloss: 0.1206771763867574, dist: 0.0032005261559610847\n",
            "--------------------------------------------------\n",
            "iter: 467 \tloss: 0.12252185544120225, dist: 0.0028380451474666994\n",
            "--------------------------------------------------\n",
            "iter: 468 \tloss: 0.11127775722348013, dist: 0.0031856759608104145\n",
            "--------------------------------------------------\n",
            "iter: 469 \tloss: 0.1358601934063367, dist: 0.0028777058551384235\n",
            "--------------------------------------------------\n",
            "iter: 470 \tloss: 0.12969003093976547, dist: 0.0032408855391123296\n",
            "--------------------------------------------------\n",
            "iter: 471 \tloss: 0.10410799036060063, dist: 0.002672339249839096\n",
            "--------------------------------------------------\n",
            "iter: 472 \tloss: 0.12715641042564316, dist: 0.0032297503974215037\n",
            "--------------------------------------------------\n",
            "iter: 473 \tloss: 0.11746686131510034, dist: 0.003762877731789431\n",
            "--------------------------------------------------\n",
            "iter: 474 \tloss: 0.113594915085825, dist: 0.0027063536593174855\n",
            "--------------------------------------------------\n",
            "iter: 475 \tloss: 0.1278926332191826, dist: 0.00359654631268712\n",
            "--------------------------------------------------\n",
            "iter: 476 \tloss: 0.1173092972570858, dist: 0.002601870316766374\n",
            "--------------------------------------------------\n",
            "iter: 477 \tloss: 0.10892071321630051, dist: 0.002829023578035944\n",
            "--------------------------------------------------\n",
            "iter: 478 \tloss: 0.13518488715697563, dist: 0.0037097736253054385\n",
            "--------------------------------------------------\n",
            "iter: 479 \tloss: 0.11201550187218857, dist: 0.0028503292487490448\n",
            "--------------------------------------------------\n",
            "iter: 480 \tloss: 0.12235419279305937, dist: 0.0029859116568686337\n",
            "--------------------------------------------------\n",
            "iter: 481 \tloss: 0.11806493174936086, dist: 0.002937763523898042\n",
            "--------------------------------------------------\n",
            "iter: 482 \tloss: 0.1141143108887607, dist: 0.002589920807733442\n",
            "--------------------------------------------------\n",
            "iter: 483 \tloss: 0.12064447943739341, dist: 0.002932686072770348\n",
            "--------------------------------------------------\n",
            "iter: 484 \tloss: 0.11948436979096132, dist: 0.0028266474649665104\n",
            "--------------------------------------------------\n",
            "iter: 485 \tloss: 0.11895732971382723, dist: 0.0028208884353953982\n",
            "--------------------------------------------------\n",
            "iter: 486 \tloss: 0.11600164369980855, dist: 0.0027725556311512557\n",
            "--------------------------------------------------\n",
            "iter: 487 \tloss: 0.11868405766144288, dist: 0.0028857863904828865\n",
            "--------------------------------------------------\n",
            "iter: 488 \tloss: 0.12002738369031758, dist: 0.0027436512432778707\n",
            "--------------------------------------------------\n",
            "iter: 489 \tloss: 0.11158689680362312, dist: 0.0025206089496260966\n",
            "--------------------------------------------------\n",
            "iter: 490 \tloss: 0.11189065518465227, dist: 0.002527446053886292\n",
            "--------------------------------------------------\n",
            "iter: 491 \tloss: 0.12837317957285668, dist: 0.0029941073470292987\n",
            "--------------------------------------------------\n",
            "iter: 492 \tloss: 0.11327086658551751, dist: 0.0024772211126260642\n",
            "--------------------------------------------------\n",
            "iter: 493 \tloss: 0.1110135808308871, dist: 0.0025403600101432033\n",
            "--------------------------------------------------\n",
            "iter: 494 \tloss: 0.1334435595563872, dist: 0.0031202593698035114\n",
            "--------------------------------------------------\n",
            "iter: 495 \tloss: 0.11285262024429897, dist: 0.002453649936024457\n",
            "--------------------------------------------------\n",
            "iter: 496 \tloss: 0.1307993678067697, dist: 0.0032258511320718583\n",
            "--------------------------------------------------\n",
            "iter: 497 \tloss: 0.10702900582364588, dist: 0.0023802626598372198\n",
            "--------------------------------------------------\n",
            "iter: 498 \tloss: 0.1296711447654932, dist: 0.0028663296434703295\n",
            "--------------------------------------------------\n",
            "iter: 499 \tloss: 0.12578120574660961, dist: 0.0025475847118780655\n",
            "--------------------------------------------------\n",
            "iter: 500 \tloss: 0.11061901964331479, dist: 0.0025511311428300554\n",
            "--------------------------------------------------\n",
            "iter: 501 \tloss: 0.12217335532358059, dist: 0.002699342734826112\n",
            "--------------------------------------------------\n",
            "iter: 502 \tloss: 0.12969379083177943, dist: 0.002661121355935424\n",
            "--------------------------------------------------\n",
            "iter: 503 \tloss: 0.13135971980899103, dist: 0.00296974891191491\n",
            "--------------------------------------------------\n",
            "iter: 504 \tloss: 0.1171348938866064, dist: 0.0025546130506309693\n",
            "--------------------------------------------------\n",
            "iter: 505 \tloss: 0.11989164458007893, dist: 0.0024601707827068423\n",
            "--------------------------------------------------\n",
            "iter: 506 \tloss: 0.11660357968666131, dist: 0.002453864325819024\n",
            "--------------------------------------------------\n",
            "iter: 507 \tloss: 0.11969243744665325, dist: 0.0024923981074732723\n",
            "--------------------------------------------------\n",
            "iter: 508 \tloss: 0.1184023471130271, dist: 0.002474173438131009\n",
            "--------------------------------------------------\n",
            "iter: 509 \tloss: 0.12943795619063114, dist: 0.0024234650684427108\n",
            "--------------------------------------------------\n",
            "iter: 510 \tloss: 0.12163120427705407, dist: 0.0022936500781929473\n",
            "--------------------------------------------------\n",
            "iter: 511 \tloss: 0.1111063228955209, dist: 0.00225025798760499\n",
            "--------------------------------------------------\n",
            "iter: 512 \tloss: 0.1241769543468183, dist: 0.002377913735758743\n",
            "--------------------------------------------------\n",
            "iter: 513 \tloss: 0.11358902378550705, dist: 0.0021009212752887345\n",
            "--------------------------------------------------\n",
            "iter: 514 \tloss: 0.1144258868167986, dist: 0.002348613476138726\n",
            "--------------------------------------------------\n",
            "iter: 515 \tloss: 0.11637596420447553, dist: 0.0022347947089724125\n",
            "--------------------------------------------------\n",
            "iter: 516 \tloss: 0.12607098013113036, dist: 0.002352003492786263\n",
            "--------------------------------------------------\n",
            "iter: 517 \tloss: 0.12138143813158177, dist: 0.002393274872471008\n",
            "--------------------------------------------------\n",
            "iter: 518 \tloss: 0.11275727708392856, dist: 0.0020587871383643953\n",
            "--------------------------------------------------\n",
            "iter: 519 \tloss: 0.1220710609591056, dist: 0.0025012491204521696\n",
            "--------------------------------------------------\n",
            "iter: 520 \tloss: 0.11181098171254851, dist: 0.0020304389185541926\n",
            "--------------------------------------------------\n",
            "iter: 521 \tloss: 0.11737015923938694, dist: 0.002357083643537041\n",
            "--------------------------------------------------\n",
            "iter: 522 \tloss: 0.13101672500865943, dist: 0.0022460050126345992\n",
            "--------------------------------------------------\n",
            "iter: 523 \tloss: 0.11092203085707054, dist: 0.0020527521092952705\n",
            "--------------------------------------------------\n",
            "iter: 524 \tloss: 0.12043752512068534, dist: 0.0024128968633994725\n",
            "--------------------------------------------------\n",
            "iter: 525 \tloss: 0.12617923013395993, dist: 0.002162051205016535\n",
            "--------------------------------------------------\n",
            "iter: 526 \tloss: 0.12989379264248582, dist: 0.0020940677387881064\n",
            "--------------------------------------------------\n",
            "iter: 527 \tloss: 0.12182074143385485, dist: 0.002140998473061465\n",
            "--------------------------------------------------\n",
            "iter: 528 \tloss: 0.11493842545838319, dist: 0.002032766239443284\n",
            "--------------------------------------------------\n",
            "iter: 529 \tloss: 0.12579144450039395, dist: 0.002222176428099299\n",
            "--------------------------------------------------\n",
            "iter: 530 \tloss: 0.12038628342340019, dist: 0.002085787391407466\n",
            "--------------------------------------------------\n",
            "iter: 531 \tloss: 0.12020550830456443, dist: 0.002058363933602421\n",
            "--------------------------------------------------\n",
            "iter: 532 \tloss: 0.11678540734597331, dist: 0.002048807599562507\n",
            "--------------------------------------------------\n",
            "iter: 533 \tloss: 0.12296920392890795, dist: 0.0019845753845901637\n",
            "--------------------------------------------------\n",
            "iter: 534 \tloss: 0.12642046278637842, dist: 0.002101775160223365\n",
            "--------------------------------------------------\n",
            "iter: 535 \tloss: 0.11950894167481177, dist: 0.0021321476746822676\n",
            "--------------------------------------------------\n",
            "iter: 536 \tloss: 0.12104274055628382, dist: 0.00225042310419827\n",
            "--------------------------------------------------\n",
            "iter: 537 \tloss: 0.12610168070929625, dist: 0.0021808010320745357\n",
            "--------------------------------------------------\n",
            "iter: 538 \tloss: 0.11485498701172049, dist: 0.002039761225498916\n",
            "--------------------------------------------------\n",
            "iter: 539 \tloss: 0.11804545995755152, dist: 0.0018856533808796706\n",
            "--------------------------------------------------\n",
            "iter: 540 \tloss: 0.11454727647893596, dist: 0.0018570643171254447\n",
            "--------------------------------------------------\n",
            "iter: 541 \tloss: 0.12348967929219795, dist: 0.0019349643850125888\n",
            "--------------------------------------------------\n",
            "iter: 542 \tloss: 0.11089624291299698, dist: 0.0018370534886877662\n",
            "--------------------------------------------------\n",
            "iter: 543 \tloss: 0.11937626092612583, dist: 0.001977699015402587\n",
            "--------------------------------------------------\n",
            "iter: 544 \tloss: 0.12692898961247134, dist: 0.002022254612254307\n",
            "--------------------------------------------------\n",
            "iter: 545 \tloss: 0.12689212094923047, dist: 0.002206058673277925\n",
            "--------------------------------------------------\n",
            "iter: 546 \tloss: 0.11458938281698572, dist: 0.0016528087695949678\n",
            "--------------------------------------------------\n",
            "iter: 547 \tloss: 0.1292778764833118, dist: 0.002006048351572837\n",
            "--------------------------------------------------\n",
            "iter: 548 \tloss: 0.1149488945278358, dist: 0.0016959397665363967\n",
            "--------------------------------------------------\n",
            "iter: 549 \tloss: 0.12398785119511825, dist: 0.0018962304165431015\n",
            "--------------------------------------------------\n",
            "iter: 550 \tloss: 0.1164264495441953, dist: 0.001665730807589634\n",
            "--------------------------------------------------\n",
            "iter: 551 \tloss: 0.11822108540775472, dist: 0.0018455366200937117\n",
            "--------------------------------------------------\n",
            "iter: 552 \tloss: 0.11748811944718618, dist: 0.0018987702429313828\n",
            "--------------------------------------------------\n",
            "iter: 553 \tloss: 0.11897846526998875, dist: 0.0017734970510206798\n",
            "--------------------------------------------------\n",
            "iter: 554 \tloss: 0.11688678713547772, dist: 0.0017591466635897863\n",
            "--------------------------------------------------\n",
            "iter: 555 \tloss: 0.12525377613960043, dist: 0.0017153004737371622\n",
            "--------------------------------------------------\n",
            "iter: 556 \tloss: 0.1246888378480374, dist: 0.0018760835210508665\n",
            "--------------------------------------------------\n",
            "iter: 557 \tloss: 0.12121043153483253, dist: 0.0016886806897567724\n",
            "--------------------------------------------------\n",
            "iter: 558 \tloss: 0.1265364573613666, dist: 0.0016215783193214252\n",
            "--------------------------------------------------\n",
            "iter: 559 \tloss: 0.12221353546359333, dist: 0.0020632682828344378\n",
            "--------------------------------------------------\n",
            "iter: 560 \tloss: 0.12282236850834173, dist: 0.0017236244590216865\n",
            "--------------------------------------------------\n",
            "iter: 561 \tloss: 0.11752871984036249, dist: 0.0014869407771821712\n",
            "--------------------------------------------------\n",
            "iter: 562 \tloss: 0.12089425909950621, dist: 0.001896698992061914\n",
            "--------------------------------------------------\n",
            "iter: 563 \tloss: 0.1170661196594844, dist: 0.0016616617192613343\n",
            "--------------------------------------------------\n",
            "iter: 564 \tloss: 0.11497485066953174, dist: 0.0015897278850133618\n",
            "--------------------------------------------------\n",
            "iter: 565 \tloss: 0.11905632617806171, dist: 0.0015627413691031106\n",
            "--------------------------------------------------\n",
            "iter: 566 \tloss: 0.11985133862941325, dist: 0.0017194888780491706\n",
            "--------------------------------------------------\n",
            "iter: 567 \tloss: 0.12038379274938048, dist: 0.0014661976353253512\n",
            "--------------------------------------------------\n",
            "iter: 568 \tloss: 0.13355472331826726, dist: 0.001927305738272697\n",
            "--------------------------------------------------\n",
            "iter: 569 \tloss: 0.11212701528440575, dist: 0.001561339370782564\n",
            "--------------------------------------------------\n",
            "iter: 570 \tloss: 0.11673481543044538, dist: 0.001460474813613706\n",
            "--------------------------------------------------\n",
            "iter: 571 \tloss: 0.1279998253436808, dist: 0.0016294860583186586\n",
            "--------------------------------------------------\n",
            "iter: 572 \tloss: 0.1163507327375552, dist: 0.0015889399207845087\n",
            "--------------------------------------------------\n",
            "iter: 573 \tloss: 0.1072546249891375, dist: 0.001464069885916354\n",
            "--------------------------------------------------\n",
            "iter: 574 \tloss: 0.11690449922303127, dist: 0.0014791354266891793\n",
            "--------------------------------------------------\n",
            "iter: 575 \tloss: 0.12010250186417654, dist: 0.0015467063477176627\n",
            "--------------------------------------------------\n",
            "iter: 576 \tloss: 0.119556299671722, dist: 0.0014847339143052884\n",
            "--------------------------------------------------\n",
            "iter: 577 \tloss: 0.11945128714279303, dist: 0.0014528437712637591\n",
            "--------------------------------------------------\n",
            "iter: 578 \tloss: 0.12303190364180651, dist: 0.00161528575518189\n",
            "--------------------------------------------------\n",
            "iter: 579 \tloss: 0.12675417471222364, dist: 0.0015256570213057026\n",
            "--------------------------------------------------\n",
            "iter: 580 \tloss: 0.12003698769713327, dist: 0.0013947883912778314\n",
            "--------------------------------------------------\n",
            "iter: 581 \tloss: 0.11895234181522168, dist: 0.0014876999919917083\n",
            "--------------------------------------------------\n",
            "iter: 582 \tloss: 0.10869206499218804, dist: 0.0015096849740075914\n",
            "--------------------------------------------------\n",
            "iter: 583 \tloss: 0.12061210022474271, dist: 0.0013867120704873382\n",
            "--------------------------------------------------\n",
            "iter: 584 \tloss: 0.12431445735960907, dist: 0.001468332837416191\n",
            "--------------------------------------------------\n",
            "iter: 585 \tloss: 0.12118335106171498, dist: 0.0013582021950728671\n",
            "--------------------------------------------------\n",
            "iter: 586 \tloss: 0.11508767202578225, dist: 0.0013519155511916632\n",
            "--------------------------------------------------\n",
            "iter: 587 \tloss: 0.1215795616475431, dist: 0.0013701766892584891\n",
            "--------------------------------------------------\n",
            "iter: 588 \tloss: 0.13695911203144734, dist: 0.0014771230954208569\n",
            "--------------------------------------------------\n",
            "iter: 589 \tloss: 0.12401666600863463, dist: 0.0014186554815468848\n",
            "--------------------------------------------------\n",
            "iter: 590 \tloss: 0.11406345434758061, dist: 0.001191952026376627\n",
            "--------------------------------------------------\n",
            "iter: 591 \tloss: 0.1250825832358937, dist: 0.0013183152692452145\n",
            "--------------------------------------------------\n",
            "iter: 592 \tloss: 0.12394576078174935, dist: 0.0013146502802003522\n",
            "--------------------------------------------------\n",
            "iter: 593 \tloss: 0.10831637362617653, dist: 0.0012764172247467379\n",
            "--------------------------------------------------\n",
            "iter: 594 \tloss: 0.12135181018484893, dist: 0.001133300388819449\n",
            "--------------------------------------------------\n",
            "iter: 595 \tloss: 0.12498772400296489, dist: 0.001371352885618677\n",
            "--------------------------------------------------\n",
            "iter: 596 \tloss: 0.126910984788285, dist: 0.0012219698105051023\n",
            "--------------------------------------------------\n",
            "iter: 597 \tloss: 0.11387466409728152, dist: 0.0012631719003321504\n",
            "--------------------------------------------------\n",
            "iter: 598 \tloss: 0.12807620733961508, dist: 0.0014258097507303877\n",
            "--------------------------------------------------\n",
            "iter: 599 \tloss: 0.11960433703916475, dist: 0.0012884947641815512\n",
            "--------------------------------------------------\n",
            "iter: 600 \tloss: 0.11872238166399046, dist: 0.0012703499841086729\n",
            "--------------------------------------------------\n",
            "iter: 601 \tloss: 0.11530368616076346, dist: 0.0011299989315277218\n",
            "--------------------------------------------------\n",
            "iter: 602 \tloss: 0.12312899473419651, dist: 0.00123257235027561\n",
            "--------------------------------------------------\n",
            "iter: 603 \tloss: 0.12170119470675092, dist: 0.001321485413806083\n",
            "--------------------------------------------------\n",
            "iter: 604 \tloss: 0.12095717623894701, dist: 0.0012026273912569658\n",
            "--------------------------------------------------\n",
            "iter: 605 \tloss: 0.11761553200805058, dist: 0.0011673034445408926\n",
            "--------------------------------------------------\n",
            "iter: 606 \tloss: 0.12263227601150545, dist: 0.001083341151416966\n",
            "--------------------------------------------------\n",
            "iter: 607 \tloss: 0.12160164230406023, dist: 0.0012120618320106207\n",
            "--------------------------------------------------\n",
            "iter: 608 \tloss: 0.11171178697166231, dist: 0.0011212767925298575\n",
            "--------------------------------------------------\n",
            "iter: 609 \tloss: 0.13247080973857797, dist: 0.0012015019853812575\n",
            "--------------------------------------------------\n",
            "iter: 610 \tloss: 0.13242850205777018, dist: 0.001284846583845401\n",
            "--------------------------------------------------\n",
            "iter: 611 \tloss: 0.1178628196296343, dist: 0.0010907348947022245\n",
            "--------------------------------------------------\n",
            "iter: 612 \tloss: 0.11561537398536217, dist: 0.001106919669966857\n",
            "--------------------------------------------------\n",
            "iter: 613 \tloss: 0.11939374993319464, dist: 0.001217491292883151\n",
            "--------------------------------------------------\n",
            "iter: 614 \tloss: 0.11561335989626075, dist: 0.0009894544606570347\n",
            "--------------------------------------------------\n",
            "iter: 615 \tloss: 0.1235309142583488, dist: 0.001215604833294935\n",
            "--------------------------------------------------\n",
            "iter: 616 \tloss: 0.12067906634143295, dist: 0.0010457157518144369\n",
            "--------------------------------------------------\n",
            "iter: 617 \tloss: 0.11781792812834292, dist: 0.0010955564776843358\n",
            "--------------------------------------------------\n",
            "iter: 618 \tloss: 0.12694406219548035, dist: 0.0011569313673839442\n",
            "--------------------------------------------------\n",
            "iter: 619 \tloss: 0.1217385059275034, dist: 0.0011566718188111694\n",
            "--------------------------------------------------\n",
            "iter: 620 \tloss: 0.1312431313376107, dist: 0.0014462853006482325\n",
            "--------------------------------------------------\n",
            "iter: 621 \tloss: 0.1288609812549248, dist: 0.0011910425963573693\n",
            "--------------------------------------------------\n",
            "iter: 622 \tloss: 0.12332094691888341, dist: 0.001068189034552611\n",
            "--------------------------------------------------\n",
            "iter: 623 \tloss: 0.12639056673981278, dist: 0.0011190917631757511\n",
            "--------------------------------------------------\n",
            "iter: 624 \tloss: 0.11535163759193749, dist: 0.001016130038302924\n",
            "--------------------------------------------------\n",
            "iter: 625 \tloss: 0.13837706923541082, dist: 0.0009610630874833034\n",
            "--------------------------------------------------\n",
            "iter: 626 \tloss: 0.11618784737616857, dist: 0.0011363948962541612\n",
            "--------------------------------------------------\n",
            "iter: 627 \tloss: 0.12332098794333944, dist: 0.000980182354003445\n",
            "--------------------------------------------------\n",
            "iter: 628 \tloss: 0.11844840392005906, dist: 0.0009763020888798283\n",
            "--------------------------------------------------\n",
            "iter: 629 \tloss: 0.11922568458168131, dist: 0.001117526474410284\n",
            "--------------------------------------------------\n",
            "iter: 630 \tloss: 0.1303476114710911, dist: 0.0010605505250357303\n",
            "--------------------------------------------------\n",
            "iter: 631 \tloss: 0.12286880447574605, dist: 0.0009716076786669407\n",
            "--------------------------------------------------\n",
            "iter: 632 \tloss: 0.11402702886442459, dist: 0.0010046507171044548\n",
            "--------------------------------------------------\n",
            "iter: 633 \tloss: 0.1259606190904033, dist: 0.0010040441994502477\n",
            "--------------------------------------------------\n",
            "iter: 634 \tloss: 0.11772583032132915, dist: 0.0009539534806759602\n",
            "--------------------------------------------------\n",
            "iter: 635 \tloss: 0.11243651951398004, dist: 0.0010288514802437607\n",
            "--------------------------------------------------\n",
            "iter: 636 \tloss: 0.11857743800163609, dist: 0.0009605191573450286\n",
            "--------------------------------------------------\n",
            "iter: 637 \tloss: 0.10853119818937505, dist: 0.0008969413086010373\n",
            "--------------------------------------------------\n",
            "iter: 638 \tloss: 0.12884503471931336, dist: 0.0010154856860292252\n",
            "--------------------------------------------------\n",
            "iter: 639 \tloss: 0.11642034810732703, dist: 0.0010172572007189219\n",
            "--------------------------------------------------\n",
            "iter: 640 \tloss: 0.11964320316798872, dist: 0.0010292354483134756\n",
            "--------------------------------------------------\n",
            "iter: 641 \tloss: 0.1238051049641039, dist: 0.0009408736382925207\n",
            "--------------------------------------------------\n",
            "iter: 642 \tloss: 0.11693051386324185, dist: 0.0009725039277975188\n",
            "--------------------------------------------------\n",
            "iter: 643 \tloss: 0.12982951586041738, dist: 0.0012058417066453534\n",
            "--------------------------------------------------\n",
            "iter: 644 \tloss: 0.12489272064114063, dist: 0.001004274092309366\n",
            "--------------------------------------------------\n",
            "iter: 645 \tloss: 0.1210373960778691, dist: 0.0010274648792991136\n",
            "--------------------------------------------------\n",
            "iter: 646 \tloss: 0.11815938517235579, dist: 0.0010738880969844078\n",
            "--------------------------------------------------\n",
            "iter: 647 \tloss: 0.13957468497252143, dist: 0.0009626441150305336\n",
            "--------------------------------------------------\n",
            "iter: 648 \tloss: 0.1159545740032962, dist: 0.0009070635294900546\n",
            "--------------------------------------------------\n",
            "iter: 649 \tloss: 0.11626517868004434, dist: 0.0009521375500388902\n",
            "--------------------------------------------------\n",
            "iter: 650 \tloss: 0.11456764798856583, dist: 0.0008026349558410253\n",
            "--------------------------------------------------\n",
            "iter: 651 \tloss: 0.14415656833716195, dist: 0.0010652363080172807\n",
            "--------------------------------------------------\n",
            "iter: 652 \tloss: 0.12801422779968663, dist: 0.0008899209038016494\n",
            "--------------------------------------------------\n",
            "iter: 653 \tloss: 0.10633088931747015, dist: 0.0009182357655171853\n",
            "--------------------------------------------------\n",
            "iter: 654 \tloss: 0.1328391508137984, dist: 0.0008459715903024489\n",
            "--------------------------------------------------\n",
            "iter: 655 \tloss: 0.12616500254989205, dist: 0.001047200914946556\n",
            "--------------------------------------------------\n",
            "iter: 656 \tloss: 0.11815567784206356, dist: 0.0009044122206584205\n",
            "--------------------------------------------------\n",
            "iter: 657 \tloss: 0.10707471564700151, dist: 0.000718871272502728\n",
            "--------------------------------------------------\n",
            "iter: 658 \tloss: 0.12601948451845993, dist: 0.0009584532167243794\n",
            "--------------------------------------------------\n",
            "iter: 659 \tloss: 0.132564570277313, dist: 0.0010447610728767517\n",
            "--------------------------------------------------\n",
            "iter: 660 \tloss: 0.11659424617030134, dist: 0.0008624510220037115\n",
            "--------------------------------------------------\n",
            "iter: 661 \tloss: 0.10906643712457291, dist: 0.0008145925262113117\n",
            "--------------------------------------------------\n",
            "iter: 662 \tloss: 0.11412629937488492, dist: 0.0009729631021839057\n",
            "--------------------------------------------------\n",
            "iter: 663 \tloss: 0.12790144860759514, dist: 0.0008154566265318911\n",
            "--------------------------------------------------\n",
            "iter: 664 \tloss: 0.11799095180173685, dist: 0.0008573119841741521\n",
            "--------------------------------------------------\n",
            "iter: 665 \tloss: 0.12001895820010441, dist: 0.0008927412482835346\n",
            "--------------------------------------------------\n",
            "iter: 666 \tloss: 0.11998775417534899, dist: 0.0008277721381083115\n",
            "--------------------------------------------------\n",
            "iter: 667 \tloss: 0.11452382059779304, dist: 0.0008901895145811812\n",
            "--------------------------------------------------\n",
            "iter: 668 \tloss: 0.12001982066091128, dist: 0.000900256143761853\n",
            "--------------------------------------------------\n",
            "iter: 669 \tloss: 0.12019765846051066, dist: 0.0008975388218460916\n",
            "--------------------------------------------------\n",
            "iter: 670 \tloss: 0.1179957288160865, dist: 0.000872537855686452\n",
            "--------------------------------------------------\n",
            "iter: 671 \tloss: 0.12258589054593574, dist: 0.0009345724779331842\n",
            "--------------------------------------------------\n",
            "iter: 672 \tloss: 0.11758928840905658, dist: 0.000896736633052671\n",
            "--------------------------------------------------\n",
            "iter: 673 \tloss: 0.12567932268400683, dist: 0.0007953176015530458\n",
            "--------------------------------------------------\n",
            "iter: 674 \tloss: 0.12872295115991375, dist: 0.0010960748783243558\n",
            "--------------------------------------------------\n",
            "iter: 675 \tloss: 0.10628757211072118, dist: 0.0009078640982652241\n",
            "--------------------------------------------------\n",
            "iter: 676 \tloss: 0.1408405731997856, dist: 0.0008253304103120852\n",
            "--------------------------------------------------\n",
            "iter: 677 \tloss: 0.11860948807815408, dist: 0.0009221688676911674\n",
            "--------------------------------------------------\n",
            "iter: 678 \tloss: 0.1167807083435708, dist: 0.0008868396108910774\n",
            "--------------------------------------------------\n",
            "iter: 679 \tloss: 0.13142656971743646, dist: 0.0010838828780220576\n",
            "--------------------------------------------------\n",
            "iter: 680 \tloss: 0.13102075587006642, dist: 0.0010982911075060695\n",
            "--------------------------------------------------\n",
            "iter: 681 \tloss: 0.11480879262027933, dist: 0.0008280058375724108\n",
            "--------------------------------------------------\n",
            "iter: 682 \tloss: 0.12375461401961184, dist: 0.00100540241985759\n",
            "--------------------------------------------------\n",
            "iter: 683 \tloss: 0.12031559678342242, dist: 0.0008195940213819739\n",
            "--------------------------------------------------\n",
            "iter: 684 \tloss: 0.11505514312665013, dist: 0.0008384733566299821\n",
            "--------------------------------------------------\n",
            "iter: 685 \tloss: 0.1257019459379537, dist: 0.0009341421095803629\n",
            "--------------------------------------------------\n",
            "iter: 686 \tloss: 0.12584270660563715, dist: 0.0008744727644174861\n",
            "--------------------------------------------------\n",
            "iter: 687 \tloss: 0.11910755567730216, dist: 0.0008177443418359344\n",
            "--------------------------------------------------\n",
            "iter: 688 \tloss: 0.11781465199450435, dist: 0.0009091625754392463\n",
            "--------------------------------------------------\n",
            "iter: 689 \tloss: 0.11995212628561712, dist: 0.0008503991945490227\n",
            "--------------------------------------------------\n",
            "iter: 690 \tloss: 0.1190387540121299, dist: 0.0008410521193232388\n",
            "--------------------------------------------------\n",
            "iter: 691 \tloss: 0.11981104674861097, dist: 0.0008563300465210861\n",
            "--------------------------------------------------\n",
            "iter: 692 \tloss: 0.1193391086289155, dist: 0.0008388046290607001\n",
            "--------------------------------------------------\n",
            "iter: 693 \tloss: 0.12467673188712342, dist: 0.0009543080520057434\n",
            "--------------------------------------------------\n",
            "iter: 694 \tloss: 0.11780799382419133, dist: 0.0009451546156704464\n",
            "--------------------------------------------------\n",
            "iter: 695 \tloss: 0.12028525240849114, dist: 0.0008954271968482985\n",
            "--------------------------------------------------\n",
            "iter: 696 \tloss: 0.11101849572743706, dist: 0.0008002329608663633\n",
            "--------------------------------------------------\n",
            "iter: 697 \tloss: 0.1261871094076852, dist: 0.000956862436890853\n",
            "--------------------------------------------------\n",
            "iter: 698 \tloss: 0.11839787429849674, dist: 0.0007560767405125349\n",
            "--------------------------------------------------\n",
            "iter: 699 \tloss: 0.1273334189363182, dist: 0.0007579864479608366\n",
            "--------------------------------------------------\n",
            "iter: 700 \tloss: 0.11732120248587928, dist: 0.0008722082857471965\n",
            "--------------------------------------------------\n",
            "iter: 701 \tloss: 0.12209141998575604, dist: 0.0008312857001137972\n",
            "--------------------------------------------------\n",
            "iter: 702 \tloss: 0.11150700693525056, dist: 0.0008650125022854169\n",
            "--------------------------------------------------\n",
            "iter: 703 \tloss: 0.12192150189077898, dist: 0.0007269569940269865\n",
            "--------------------------------------------------\n",
            "iter: 704 \tloss: 0.12354351976398062, dist: 0.0010158493445062296\n",
            "--------------------------------------------------\n",
            "iter: 705 \tloss: 0.11840107140822925, dist: 0.0007583921066480936\n",
            "--------------------------------------------------\n",
            "iter: 706 \tloss: 0.12497116655173164, dist: 0.000823926695568813\n",
            "--------------------------------------------------\n",
            "iter: 707 \tloss: 0.1194856553461392, dist: 0.0009562626995920805\n",
            "--------------------------------------------------\n",
            "iter: 708 \tloss: 0.1240160321052779, dist: 0.0007399042198545443\n",
            "--------------------------------------------------\n",
            "iter: 709 \tloss: 0.1188733283117645, dist: 0.0010984164569488213\n",
            "--------------------------------------------------\n",
            "iter: 710 \tloss: 0.11917596446636143, dist: 0.0008992348769421414\n",
            "--------------------------------------------------\n",
            "iter: 711 \tloss: 0.11588972432805365, dist: 0.0007782725658766616\n",
            "--------------------------------------------------\n",
            "iter: 712 \tloss: 0.11835402599278165, dist: 0.000838887423631341\n",
            "--------------------------------------------------\n",
            "iter: 713 \tloss: 0.11951586822783179, dist: 0.0008438754492076268\n",
            "--------------------------------------------------\n",
            "iter: 714 \tloss: 0.11903914816608405, dist: 0.0008425249466189735\n",
            "--------------------------------------------------\n",
            "iter: 715 \tloss: 0.11848907955408317, dist: 0.0008207083524595936\n",
            "--------------------------------------------------\n",
            "iter: 716 \tloss: 0.11930713310651918, dist: 0.0008586703232900179\n",
            "--------------------------------------------------\n",
            "iter: 717 \tloss: 0.12003449178825432, dist: 0.0008829727088391254\n",
            "--------------------------------------------------\n",
            "iter: 718 \tloss: 0.12800369987570648, dist: 0.0009402975397012881\n",
            "--------------------------------------------------\n",
            "iter: 719 \tloss: 0.1142537642505276, dist: 0.0007612170944248813\n",
            "--------------------------------------------------\n",
            "iter: 720 \tloss: 0.1224694377523055, dist: 0.0010259434752241082\n",
            "--------------------------------------------------\n",
            "iter: 721 \tloss: 0.1265952872255242, dist: 0.0008509527938863784\n",
            "--------------------------------------------------\n",
            "iter: 722 \tloss: 0.1187428130524213, dist: 0.0009270085819557225\n",
            "--------------------------------------------------\n",
            "iter: 723 \tloss: 0.12779686375497465, dist: 0.0008476877608673613\n",
            "--------------------------------------------------\n",
            "iter: 724 \tloss: 0.11358919211052448, dist: 0.0007975643768113795\n",
            "--------------------------------------------------\n",
            "iter: 725 \tloss: 0.12374275578953658, dist: 0.000899413914047216\n",
            "--------------------------------------------------\n",
            "iter: 726 \tloss: 0.11348116991528294, dist: 0.00072218964169766\n",
            "--------------------------------------------------\n",
            "iter: 727 \tloss: 0.11629235828823346, dist: 0.0009151067773351712\n",
            "--------------------------------------------------\n",
            "iter: 728 \tloss: 0.12365710314441075, dist: 0.0007480843990509534\n",
            "--------------------------------------------------\n",
            "iter: 729 \tloss: 0.11271681453620004, dist: 0.0007852736062169143\n",
            "--------------------------------------------------\n",
            "iter: 730 \tloss: 0.12619512380465206, dist: 0.0009428627138243453\n",
            "--------------------------------------------------\n",
            "iter: 731 \tloss: 0.11738090670187629, dist: 0.0008515834338354255\n",
            "--------------------------------------------------\n",
            "iter: 732 \tloss: 0.12224100651417201, dist: 0.0008772903632997321\n",
            "--------------------------------------------------\n",
            "iter: 733 \tloss: 0.11442706717872154, dist: 0.0008430892803559086\n",
            "--------------------------------------------------\n",
            "iter: 734 \tloss: 0.12008586449074539, dist: 0.0007873768525501784\n",
            "--------------------------------------------------\n",
            "iter: 735 \tloss: 0.14066682516481077, dist: 0.0008710348409602839\n",
            "--------------------------------------------------\n",
            "iter: 736 \tloss: 0.1251978291561516, dist: 0.0010376864410683668\n",
            "--------------------------------------------------\n",
            "iter: 737 \tloss: 0.12276734511648421, dist: 0.0009157967343812559\n",
            "--------------------------------------------------\n",
            "iter: 738 \tloss: 0.12495370497099713, dist: 0.0009354366252509342\n",
            "--------------------------------------------------\n",
            "iter: 739 \tloss: 0.11548999662231546, dist: 0.000884141510948017\n",
            "--------------------------------------------------\n",
            "iter: 740 \tloss: 0.1163254906757525, dist: 0.0007849135884701146\n",
            "--------------------------------------------------\n",
            "iter: 741 \tloss: 0.11552384367754573, dist: 0.0008861533176345405\n",
            "--------------------------------------------------\n",
            "iter: 742 \tloss: 0.12024377320629007, dist: 0.0008285905354324874\n",
            "--------------------------------------------------\n",
            "iter: 743 \tloss: 0.11819052989794122, dist: 0.0007711478938201726\n",
            "--------------------------------------------------\n",
            "iter: 744 \tloss: 0.12419207430897586, dist: 0.0007544608343224711\n",
            "--------------------------------------------------\n",
            "iter: 745 \tloss: 0.11457081128486592, dist: 0.0008521611798649534\n",
            "--------------------------------------------------\n",
            "iter: 746 \tloss: 0.1255731446529742, dist: 0.0008773728340909733\n",
            "--------------------------------------------------\n",
            "iter: 747 \tloss: 0.1256005531820902, dist: 0.0006856558366238298\n",
            "--------------------------------------------------\n",
            "iter: 748 \tloss: 0.12104219437995666, dist: 0.0007773514370991477\n",
            "--------------------------------------------------\n",
            "iter: 749 \tloss: 0.11389883582146897, dist: 0.0009140828261402621\n",
            "--------------------------------------------------\n",
            "iter: 750 \tloss: 0.11330892925133704, dist: 0.0006987187173376232\n",
            "--------------------------------------------------\n",
            "iter: 751 \tloss: 0.12747229602445698, dist: 0.0008237602326179958\n",
            "--------------------------------------------------\n",
            "iter: 752 \tloss: 0.1312591747519185, dist: 0.0008111262752660091\n",
            "--------------------------------------------------\n",
            "iter: 753 \tloss: 0.1167421470397796, dist: 0.0007376197980584723\n",
            "--------------------------------------------------\n",
            "iter: 754 \tloss: 0.12663439870341728, dist: 0.0009301308472385315\n",
            "--------------------------------------------------\n",
            "iter: 755 \tloss: 0.11087748036645224, dist: 0.0006960093588463462\n",
            "--------------------------------------------------\n",
            "iter: 756 \tloss: 0.11800767045003893, dist: 0.0007720814458070136\n",
            "--------------------------------------------------\n",
            "iter: 757 \tloss: 0.1195375500396962, dist: 0.0007354272588132285\n",
            "--------------------------------------------------\n",
            "iter: 758 \tloss: 0.13840594696905206, dist: 0.0010459566662754502\n",
            "--------------------------------------------------\n",
            "iter: 759 \tloss: 0.11242967469200232, dist: 0.000758117325737723\n",
            "--------------------------------------------------\n",
            "iter: 760 \tloss: 0.11995449958070256, dist: 0.0007923076710042335\n",
            "--------------------------------------------------\n",
            "iter: 761 \tloss: 0.11790114741705357, dist: 0.0007738520809732068\n",
            "--------------------------------------------------\n",
            "iter: 762 \tloss: 0.13475177902529076, dist: 0.0008043198769665337\n",
            "--------------------------------------------------\n",
            "iter: 763 \tloss: 0.11637148351620684, dist: 0.0006913712161208873\n",
            "--------------------------------------------------\n",
            "iter: 764 \tloss: 0.12664464335192205, dist: 0.0010116215159960036\n",
            "--------------------------------------------------\n",
            "iter: 765 \tloss: 0.11197339368803835, dist: 0.0007761514537894563\n",
            "--------------------------------------------------\n",
            "iter: 766 \tloss: 0.1286815655484417, dist: 0.0008131899522425715\n",
            "--------------------------------------------------\n",
            "iter: 767 \tloss: 0.11062923895591563, dist: 0.0008542780449624649\n",
            "--------------------------------------------------\n",
            "iter: 768 \tloss: 0.13004539805321808, dist: 0.0007235627430385157\n",
            "--------------------------------------------------\n",
            "iter: 769 \tloss: 0.10665952387579815, dist: 0.0008186436106044255\n",
            "--------------------------------------------------\n",
            "iter: 770 \tloss: 0.11901922390847684, dist: 0.0007985986566974044\n",
            "--------------------------------------------------\n",
            "iter: 771 \tloss: 0.1435571175968197, dist: 0.0008650573177795464\n",
            "--------------------------------------------------\n",
            "iter: 772 \tloss: 0.11054037855021583, dist: 0.0007028284185182494\n",
            "--------------------------------------------------\n",
            "iter: 773 \tloss: 0.12027105307758114, dist: 0.0007245272873537507\n",
            "--------------------------------------------------\n",
            "iter: 774 \tloss: 0.11850303903856908, dist: 0.0007866152372525539\n",
            "--------------------------------------------------\n",
            "iter: 775 \tloss: 0.12093486338635309, dist: 0.000985589067065749\n",
            "--------------------------------------------------\n",
            "iter: 776 \tloss: 0.1204345396759423, dist: 0.0007274103882567662\n",
            "--------------------------------------------------\n",
            "iter: 777 \tloss: 0.11667432800469967, dist: 0.0008054772317608452\n",
            "--------------------------------------------------\n",
            "iter: 778 \tloss: 0.11765276256600998, dist: 0.0007264890033504996\n",
            "--------------------------------------------------\n",
            "iter: 779 \tloss: 0.12720365021228855, dist: 0.0006971795705597701\n",
            "--------------------------------------------------\n",
            "iter: 780 \tloss: 0.12415722303707044, dist: 0.0007735556229752695\n",
            "--------------------------------------------------\n",
            "iter: 781 \tloss: 0.11753239827441435, dist: 0.0008915897439990605\n",
            "--------------------------------------------------\n",
            "iter: 782 \tloss: 0.11820137757089445, dist: 0.0007141923398791144\n",
            "--------------------------------------------------\n",
            "iter: 783 \tloss: 0.11876820964477823, dist: 0.0007855330611670621\n",
            "--------------------------------------------------\n",
            "iter: 784 \tloss: 0.11899139775575715, dist: 0.0007586880016382237\n",
            "--------------------------------------------------\n",
            "iter: 785 \tloss: 0.1140691433172376, dist: 0.0007542483053557935\n",
            "--------------------------------------------------\n",
            "iter: 786 \tloss: 0.11775788856071276, dist: 0.0007695713968897904\n",
            "--------------------------------------------------\n",
            "iter: 787 \tloss: 0.11531747698697, dist: 0.0007801231698709768\n",
            "--------------------------------------------------\n",
            "iter: 788 \tloss: 0.12103222046939459, dist: 0.0008529722425012161\n",
            "--------------------------------------------------\n",
            "iter: 789 \tloss: 0.1132292759262925, dist: 0.000706428854174546\n",
            "--------------------------------------------------\n",
            "iter: 790 \tloss: 0.12250063046434204, dist: 0.0007430289419279625\n",
            "--------------------------------------------------\n",
            "iter: 791 \tloss: 0.1103317721597525, dist: 0.0007834675300399979\n",
            "--------------------------------------------------\n",
            "iter: 792 \tloss: 0.11906707318860475, dist: 0.0008476451625043918\n",
            "--------------------------------------------------\n",
            "iter: 793 \tloss: 0.12026455664732963, dist: 0.0006504559355886188\n",
            "--------------------------------------------------\n",
            "iter: 794 \tloss: 0.1179677574981739, dist: 0.0007437572701347629\n",
            "--------------------------------------------------\n",
            "iter: 795 \tloss: 0.11745820113722195, dist: 0.0010834150661441343\n",
            "--------------------------------------------------\n",
            "iter: 796 \tloss: 0.12330028368216031, dist: 0.0007064850255082575\n",
            "--------------------------------------------------\n",
            "iter: 797 \tloss: 0.1238288142136195, dist: 0.000978841078544925\n",
            "--------------------------------------------------\n",
            "iter: 798 \tloss: 0.11934306258682946, dist: 0.0009355110379813681\n",
            "--------------------------------------------------\n",
            "iter: 799 \tloss: 0.11940666809703959, dist: 0.0007560266846037196\n",
            "--------------------------------------------------\n",
            "iter: 800 \tloss: 0.12164356230345294, dist: 0.0007041919233547845\n",
            "--------------------------------------------------\n",
            "iter: 801 \tloss: 0.11370783978111247, dist: 0.0009240958956296663\n",
            "--------------------------------------------------\n",
            "iter: 802 \tloss: 0.12768699369836772, dist: 0.0007046654448186597\n",
            "--------------------------------------------------\n",
            "iter: 803 \tloss: 0.12171705446004168, dist: 0.0008088037373752213\n",
            "--------------------------------------------------\n",
            "iter: 804 \tloss: 0.11385585072148423, dist: 0.000710472780863257\n",
            "--------------------------------------------------\n",
            "iter: 805 \tloss: 0.11646325425078126, dist: 0.0007658622381983254\n",
            "--------------------------------------------------\n",
            "iter: 806 \tloss: 0.12106239340979817, dist: 0.0007517711312047319\n",
            "--------------------------------------------------\n",
            "iter: 807 \tloss: 0.12390171533122407, dist: 0.0008468403861679282\n",
            "--------------------------------------------------\n",
            "iter: 808 \tloss: 0.1165628323572161, dist: 0.0007475483020924991\n",
            "--------------------------------------------------\n",
            "iter: 809 \tloss: 0.12452453324324742, dist: 0.000867093445459862\n",
            "--------------------------------------------------\n",
            "iter: 810 \tloss: 0.11594417415971638, dist: 0.000720573153000659\n",
            "--------------------------------------------------\n",
            "iter: 811 \tloss: 0.12381351532760405, dist: 0.0008403679467713867\n",
            "--------------------------------------------------\n",
            "iter: 812 \tloss: 0.12031864551780481, dist: 0.0007014558060901655\n",
            "--------------------------------------------------\n",
            "iter: 813 \tloss: 0.1168950899651538, dist: 0.0006981553005533974\n",
            "--------------------------------------------------\n",
            "iter: 814 \tloss: 0.11790004021918507, dist: 0.000909258543441242\n",
            "--------------------------------------------------\n",
            "iter: 815 \tloss: 0.10855814772147991, dist: 0.0006662587164774211\n",
            "--------------------------------------------------\n",
            "iter: 816 \tloss: 0.13293817049711998, dist: 0.0007736613076034514\n",
            "--------------------------------------------------\n",
            "iter: 817 \tloss: 0.11528127985476679, dist: 0.000754117121097986\n",
            "--------------------------------------------------\n",
            "iter: 818 \tloss: 0.11153024723096823, dist: 0.0007412803231055029\n",
            "--------------------------------------------------\n",
            "iter: 819 \tloss: 0.12800751785806608, dist: 0.00079247545768457\n",
            "--------------------------------------------------\n",
            "iter: 820 \tloss: 0.12103116588457406, dist: 0.0007363342817251026\n",
            "--------------------------------------------------\n",
            "iter: 821 \tloss: 0.11463823960443582, dist: 0.0006749400632064197\n",
            "--------------------------------------------------\n",
            "iter: 822 \tloss: 0.12017324026030579, dist: 0.000827560494527218\n",
            "--------------------------------------------------\n",
            "iter: 823 \tloss: 0.11612525134139765, dist: 0.0007059172767620633\n",
            "--------------------------------------------------\n",
            "iter: 824 \tloss: 0.11268568506064353, dist: 0.000673724287239813\n",
            "--------------------------------------------------\n",
            "iter: 825 \tloss: 0.11821190712672681, dist: 0.0007320339700753939\n",
            "--------------------------------------------------\n",
            "iter: 826 \tloss: 0.11705175003912109, dist: 0.000722677572692541\n",
            "--------------------------------------------------\n",
            "iter: 827 \tloss: 0.1179599674563002, dist: 0.0007707414615799855\n",
            "--------------------------------------------------\n",
            "iter: 828 \tloss: 0.11473575668180706, dist: 0.000689491838802334\n",
            "--------------------------------------------------\n",
            "iter: 829 \tloss: 0.12003218517876596, dist: 0.0007534583703915945\n",
            "--------------------------------------------------\n",
            "iter: 830 \tloss: 0.11809277110047808, dist: 0.0007486279399797586\n",
            "--------------------------------------------------\n",
            "iter: 831 \tloss: 0.12775969575358703, dist: 0.0007108621483211962\n",
            "--------------------------------------------------\n",
            "iter: 832 \tloss: 0.12241017112565931, dist: 0.0007490564677523446\n",
            "--------------------------------------------------\n",
            "iter: 833 \tloss: 0.12111775577443282, dist: 0.0007732076236132039\n",
            "--------------------------------------------------\n",
            "iter: 834 \tloss: 0.12371250634932697, dist: 0.0006943104333875324\n",
            "--------------------------------------------------\n",
            "iter: 835 \tloss: 0.11959934469995087, dist: 0.000832921863299564\n",
            "--------------------------------------------------\n",
            "iter: 836 \tloss: 0.1226909669002699, dist: 0.0007867838318895281\n",
            "--------------------------------------------------\n",
            "iter: 837 \tloss: 0.1144486319859888, dist: 0.0006905777439805328\n",
            "--------------------------------------------------\n",
            "iter: 838 \tloss: 0.1151171288969687, dist: 0.0007772707231263012\n",
            "--------------------------------------------------\n",
            "iter: 839 \tloss: 0.12230553181608773, dist: 0.0008327798301571071\n",
            "--------------------------------------------------\n",
            "iter: 840 \tloss: 0.11203787971922173, dist: 0.0007139726297341883\n",
            "--------------------------------------------------\n",
            "iter: 841 \tloss: 0.12644682885081157, dist: 0.0007935990060932794\n",
            "--------------------------------------------------\n",
            "iter: 842 \tloss: 0.11172442893103841, dist: 0.0007033229881549035\n",
            "--------------------------------------------------\n",
            "iter: 843 \tloss: 0.12290982752185832, dist: 0.0006874491903526894\n",
            "--------------------------------------------------\n",
            "iter: 844 \tloss: 0.11493264041019327, dist: 0.0008182845667902425\n",
            "--------------------------------------------------\n",
            "iter: 845 \tloss: 0.12844590239229223, dist: 0.0008681796753765822\n",
            "--------------------------------------------------\n",
            "iter: 846 \tloss: 0.11284634201924794, dist: 0.0007628175362355077\n",
            "--------------------------------------------------\n",
            "iter: 847 \tloss: 0.12177630852528167, dist: 0.0007383715324942911\n",
            "--------------------------------------------------\n",
            "iter: 848 \tloss: 0.11320059023268864, dist: 0.0008002808155462466\n",
            "--------------------------------------------------\n",
            "iter: 849 \tloss: 0.12757738225471452, dist: 0.0007084518616007386\n",
            "--------------------------------------------------\n",
            "iter: 850 \tloss: 0.11319214404080333, dist: 0.0007300802764841941\n",
            "--------------------------------------------------\n",
            "iter: 851 \tloss: 0.12286713336895085, dist: 0.000790117211560474\n",
            "--------------------------------------------------\n",
            "iter: 852 \tloss: 0.11828865500199054, dist: 0.0007659777792593622\n",
            "--------------------------------------------------\n",
            "iter: 853 \tloss: 0.11848095927374702, dist: 0.000734546795231827\n",
            "--------------------------------------------------\n",
            "iter: 854 \tloss: 0.11885949027749909, dist: 0.0008199995093888342\n",
            "--------------------------------------------------\n",
            "iter: 855 \tloss: 0.11576320454218916, dist: 0.0008230418555824416\n",
            "--------------------------------------------------\n",
            "iter: 856 \tloss: 0.11808501315836537, dist: 0.0007623008246181372\n",
            "--------------------------------------------------\n",
            "iter: 857 \tloss: 0.12007387912293564, dist: 0.0007620277000489711\n",
            "--------------------------------------------------\n",
            "iter: 858 \tloss: 0.125957654710816, dist: 0.0008217396361254\n",
            "--------------------------------------------------\n",
            "iter: 859 \tloss: 0.12425273265390446, dist: 0.0008189239281203698\n",
            "--------------------------------------------------\n",
            "iter: 860 \tloss: 0.12027724066801197, dist: 0.0008171064395368524\n",
            "--------------------------------------------------\n",
            "iter: 861 \tloss: 0.12260441565486163, dist: 0.0007243888277437793\n",
            "--------------------------------------------------\n",
            "iter: 862 \tloss: 0.11252395595110797, dist: 0.0009751508889184044\n",
            "--------------------------------------------------\n",
            "iter: 863 \tloss: 0.13691882304210423, dist: 0.0008053894582305234\n",
            "--------------------------------------------------\n",
            "iter: 864 \tloss: 0.11705387765894205, dist: 0.0009058531748253007\n",
            "--------------------------------------------------\n",
            "iter: 865 \tloss: 0.13235068251649176, dist: 0.0008843559710365264\n",
            "--------------------------------------------------\n",
            "iter: 866 \tloss: 0.11251844514824474, dist: 0.0006972084077966488\n",
            "--------------------------------------------------\n",
            "iter: 867 \tloss: 0.12421753217068784, dist: 0.0007651773161095374\n",
            "--------------------------------------------------\n",
            "iter: 868 \tloss: 0.12565567965498106, dist: 0.0007487344711588048\n",
            "--------------------------------------------------\n",
            "iter: 869 \tloss: 0.10638314176888852, dist: 0.0008025870809331997\n",
            "--------------------------------------------------\n",
            "iter: 870 \tloss: 0.1264562845861078, dist: 0.000766037606683993\n",
            "--------------------------------------------------\n",
            "iter: 871 \tloss: 0.11004824119887056, dist: 0.0006780497251948826\n",
            "--------------------------------------------------\n",
            "iter: 872 \tloss: 0.12005623370921487, dist: 0.0008333284845167378\n",
            "--------------------------------------------------\n",
            "iter: 873 \tloss: 0.12204059984436944, dist: 0.0007031905661555245\n",
            "--------------------------------------------------\n",
            "iter: 874 \tloss: 0.11289197899674681, dist: 0.0007357568215501112\n",
            "--------------------------------------------------\n",
            "iter: 875 \tloss: 0.11856147704045114, dist: 0.0007787930519705922\n",
            "--------------------------------------------------\n",
            "iter: 876 \tloss: 0.11872995397561152, dist: 0.0007973732574461628\n",
            "--------------------------------------------------\n",
            "iter: 877 \tloss: 0.11348492151353406, dist: 0.0007463066934066617\n",
            "--------------------------------------------------\n",
            "iter: 878 \tloss: 0.12289226984093622, dist: 0.000749729510758861\n",
            "--------------------------------------------------\n",
            "iter: 879 \tloss: 0.10941763562508376, dist: 0.0007644619179058235\n",
            "--------------------------------------------------\n",
            "iter: 880 \tloss: 0.12204923227662236, dist: 0.0007496805000387195\n",
            "--------------------------------------------------\n",
            "iter: 881 \tloss: 0.12315121399893071, dist: 0.0007647935412142685\n",
            "--------------------------------------------------\n",
            "iter: 882 \tloss: 0.12338859295286614, dist: 0.0007955449583272634\n",
            "--------------------------------------------------\n",
            "iter: 883 \tloss: 0.13138150480278687, dist: 0.0009405608109708708\n",
            "--------------------------------------------------\n",
            "iter: 884 \tloss: 0.11971417866678068, dist: 0.0007271702933555182\n",
            "--------------------------------------------------\n",
            "iter: 885 \tloss: 0.11425568520061172, dist: 0.0007614503571047246\n",
            "--------------------------------------------------\n",
            "iter: 886 \tloss: 0.1176513061061606, dist: 0.0008329097439299216\n",
            "--------------------------------------------------\n",
            "iter: 887 \tloss: 0.11784186411701364, dist: 0.000843077935362849\n",
            "--------------------------------------------------\n",
            "iter: 888 \tloss: 0.10813052400480153, dist: 0.0006620651748592198\n",
            "--------------------------------------------------\n",
            "iter: 889 \tloss: 0.12550158103162082, dist: 0.0009050434824736191\n",
            "--------------------------------------------------\n",
            "iter: 890 \tloss: 0.11282320035130863, dist: 0.0007630790064262309\n",
            "--------------------------------------------------\n",
            "iter: 891 \tloss: 0.11843231572519773, dist: 0.0006799833055489735\n",
            "--------------------------------------------------\n",
            "iter: 892 \tloss: 0.11192360765281717, dist: 0.0007809000427371544\n",
            "--------------------------------------------------\n",
            "iter: 893 \tloss: 0.11710949429361976, dist: 0.0007216909022137344\n",
            "--------------------------------------------------\n",
            "iter: 894 \tloss: 0.12001544382099395, dist: 0.0007535643412792975\n",
            "--------------------------------------------------\n",
            "iter: 895 \tloss: 0.11575852575991576, dist: 0.0007979730923731099\n",
            "--------------------------------------------------\n",
            "iter: 896 \tloss: 0.11376550975525075, dist: 0.0006837780578122813\n",
            "--------------------------------------------------\n",
            "iter: 897 \tloss: 0.11859012950183095, dist: 0.0007679624017563735\n",
            "--------------------------------------------------\n",
            "iter: 898 \tloss: 0.12111389172039383, dist: 0.0008507915949325109\n",
            "--------------------------------------------------\n",
            "iter: 899 \tloss: 0.1239803293795917, dist: 0.0008716447625580805\n",
            "--------------------------------------------------\n",
            "iter: 900 \tloss: 0.1167351021166576, dist: 0.0006870074810012857\n",
            "--------------------------------------------------\n",
            "iter: 901 \tloss: 0.12687112299097017, dist: 0.000824400636070181\n",
            "--------------------------------------------------\n",
            "iter: 902 \tloss: 0.1130378057909672, dist: 0.0008056487766533334\n",
            "--------------------------------------------------\n",
            "iter: 903 \tloss: 0.12202422509687265, dist: 0.0007547692144654239\n",
            "--------------------------------------------------\n",
            "iter: 904 \tloss: 0.11804485138173941, dist: 0.0007598145071721732\n",
            "--------------------------------------------------\n",
            "iter: 905 \tloss: 0.12051689662365472, dist: 0.0007287851301450662\n",
            "--------------------------------------------------\n",
            "iter: 906 \tloss: 0.12048773810343037, dist: 0.0007713073904483734\n",
            "--------------------------------------------------\n",
            "iter: 907 \tloss: 0.12675818641638473, dist: 0.0006869635061082517\n",
            "--------------------------------------------------\n",
            "iter: 908 \tloss: 0.11375173971456552, dist: 0.0008652113762264337\n",
            "--------------------------------------------------\n",
            "iter: 909 \tloss: 0.12163609840135446, dist: 0.0008593144365194456\n",
            "--------------------------------------------------\n",
            "iter: 910 \tloss: 0.11996928274787468, dist: 0.0007732925641429646\n",
            "--------------------------------------------------\n",
            "iter: 911 \tloss: 0.12044732474603133, dist: 0.0006760812987452572\n",
            "--------------------------------------------------\n",
            "iter: 912 \tloss: 0.11286050194493603, dist: 0.0008446069405225364\n",
            "--------------------------------------------------\n",
            "iter: 913 \tloss: 0.11028322114026122, dist: 0.0007361033148283847\n",
            "--------------------------------------------------\n",
            "iter: 914 \tloss: 0.13088430153600125, dist: 0.0009032215085449505\n",
            "--------------------------------------------------\n",
            "iter: 915 \tloss: 0.1113333111045392, dist: 0.0007193345954317549\n",
            "--------------------------------------------------\n",
            "iter: 916 \tloss: 0.12161671396402435, dist: 0.0008030467271913231\n",
            "--------------------------------------------------\n",
            "iter: 917 \tloss: 0.11026335681857043, dist: 0.0007432760882552293\n",
            "--------------------------------------------------\n",
            "iter: 918 \tloss: 0.1346695812178903, dist: 0.0008211695921925472\n",
            "--------------------------------------------------\n",
            "iter: 919 \tloss: 0.13127720006105845, dist: 0.0008070993395237103\n",
            "--------------------------------------------------\n",
            "iter: 920 \tloss: 0.11760863844245573, dist: 0.0008131312255080446\n",
            "--------------------------------------------------\n",
            "iter: 921 \tloss: 0.11988359286493488, dist: 0.0007761420361397993\n",
            "--------------------------------------------------\n",
            "iter: 922 \tloss: 0.11984340645275658, dist: 0.0007773407659600952\n",
            "--------------------------------------------------\n",
            "iter: 923 \tloss: 0.12319248973241631, dist: 0.0007227386204800755\n",
            "--------------------------------------------------\n",
            "iter: 924 \tloss: 0.11351823063749436, dist: 0.0007900060284840485\n",
            "--------------------------------------------------\n",
            "iter: 925 \tloss: 0.11800155351888784, dist: 0.0008086267395345845\n",
            "--------------------------------------------------\n",
            "iter: 926 \tloss: 0.1234546977662148, dist: 0.0007228123616581875\n",
            "--------------------------------------------------\n",
            "iter: 927 \tloss: 0.11853500469249556, dist: 0.0007228726866592721\n",
            "--------------------------------------------------\n",
            "iter: 928 \tloss: 0.11321756187000219, dist: 0.0008523876017190583\n",
            "--------------------------------------------------\n",
            "iter: 929 \tloss: 0.12358932517308019, dist: 0.0007434546328213648\n",
            "--------------------------------------------------\n",
            "iter: 930 \tloss: 0.12494920611826536, dist: 0.0007643011418585817\n",
            "--------------------------------------------------\n",
            "iter: 931 \tloss: 0.12132188635269969, dist: 0.0007145562063455906\n",
            "--------------------------------------------------\n",
            "iter: 932 \tloss: 0.1242788674534851, dist: 0.0008224657248154404\n",
            "--------------------------------------------------\n",
            "iter: 933 \tloss: 0.12461156245209228, dist: 0.0006926892338249633\n",
            "--------------------------------------------------\n",
            "iter: 934 \tloss: 0.11847752178205337, dist: 0.0008256582081163071\n",
            "--------------------------------------------------\n",
            "iter: 935 \tloss: 0.11043066235761331, dist: 0.0006945760522353943\n",
            "--------------------------------------------------\n",
            "iter: 936 \tloss: 0.11950286027763295, dist: 0.0007974320191550969\n",
            "--------------------------------------------------\n",
            "iter: 937 \tloss: 0.12246753894275933, dist: 0.0007514353361437541\n",
            "--------------------------------------------------\n",
            "iter: 938 \tloss: 0.12747141832091888, dist: 0.0008029060887146584\n",
            "--------------------------------------------------\n",
            "iter: 939 \tloss: 0.11574167167162783, dist: 0.0007866172510901694\n",
            "--------------------------------------------------\n",
            "iter: 940 \tloss: 0.11792696879015775, dist: 0.0007164231963261321\n",
            "--------------------------------------------------\n",
            "iter: 941 \tloss: 0.11765881039499884, dist: 0.0007308583205767608\n",
            "--------------------------------------------------\n",
            "iter: 942 \tloss: 0.11392704252565894, dist: 0.0007802734299390768\n",
            "--------------------------------------------------\n",
            "iter: 943 \tloss: 0.1201282621248696, dist: 0.0007835330240066166\n",
            "--------------------------------------------------\n",
            "iter: 944 \tloss: 0.12289903118537246, dist: 0.0007922282942743647\n",
            "--------------------------------------------------\n",
            "iter: 945 \tloss: 0.12362471245621438, dist: 0.0007883645554698401\n",
            "--------------------------------------------------\n",
            "iter: 946 \tloss: 0.11954268357447816, dist: 0.0007552744456284098\n",
            "--------------------------------------------------\n",
            "iter: 947 \tloss: 0.11928796829060889, dist: 0.0009711031759424526\n",
            "--------------------------------------------------\n",
            "iter: 948 \tloss: 0.13558953351355785, dist: 0.0007044398461708824\n",
            "--------------------------------------------------\n",
            "iter: 949 \tloss: 0.1153685792396299, dist: 0.0007859638912613878\n",
            "--------------------------------------------------\n",
            "iter: 950 \tloss: 0.1276026641967709, dist: 0.0008079226596884655\n",
            "--------------------------------------------------\n",
            "iter: 951 \tloss: 0.11706060810388212, dist: 0.0007019145969116331\n",
            "--------------------------------------------------\n",
            "iter: 952 \tloss: 0.12454639920419824, dist: 0.000742314266866087\n",
            "--------------------------------------------------\n",
            "iter: 953 \tloss: 0.11039405848571088, dist: 0.0008171393559265407\n",
            "--------------------------------------------------\n",
            "iter: 954 \tloss: 0.12048527895383011, dist: 0.0006845099666428356\n",
            "--------------------------------------------------\n",
            "iter: 955 \tloss: 0.12375869288591916, dist: 0.0008329343111274899\n",
            "--------------------------------------------------\n",
            "iter: 956 \tloss: 0.11882617767875986, dist: 0.0008407895146105795\n",
            "--------------------------------------------------\n",
            "iter: 957 \tloss: 0.10972645441239952, dist: 0.0007515507955662154\n",
            "--------------------------------------------------\n",
            "iter: 958 \tloss: 0.12055702532028163, dist: 0.0008178126587869088\n",
            "--------------------------------------------------\n",
            "iter: 959 \tloss: 0.11849541395446792, dist: 0.0007969999039842319\n",
            "--------------------------------------------------\n",
            "iter: 960 \tloss: 0.11774476178275287, dist: 0.0007362548361447224\n",
            "--------------------------------------------------\n",
            "iter: 961 \tloss: 0.12539824520721468, dist: 0.000798119996038238\n",
            "--------------------------------------------------\n",
            "iter: 962 \tloss: 0.1306898330639281, dist: 0.0008056600694813825\n",
            "--------------------------------------------------\n",
            "iter: 963 \tloss: 0.1192931121631763, dist: 0.000715143980833558\n",
            "--------------------------------------------------\n",
            "iter: 964 \tloss: 0.1278574835321816, dist: 0.0009220042016885797\n",
            "--------------------------------------------------\n",
            "iter: 965 \tloss: 0.12133047085014743, dist: 0.0006787595875744133\n",
            "--------------------------------------------------\n",
            "iter: 966 \tloss: 0.11722229335303756, dist: 0.0007883124173177342\n",
            "--------------------------------------------------\n",
            "iter: 967 \tloss: 0.11919527965356193, dist: 0.0007958889345412682\n",
            "--------------------------------------------------\n",
            "iter: 968 \tloss: 0.11978644417824169, dist: 0.0007794735026591927\n",
            "--------------------------------------------------\n",
            "iter: 969 \tloss: 0.11842061614645276, dist: 0.0007894490864728802\n",
            "--------------------------------------------------\n",
            "iter: 970 \tloss: 0.12058596192227725, dist: 0.0007014760416822646\n",
            "--------------------------------------------------\n",
            "iter: 971 \tloss: 0.11621825888201359, dist: 0.0008463955278119419\n",
            "--------------------------------------------------\n",
            "iter: 972 \tloss: 0.12709899117314702, dist: 0.0008287911296928161\n",
            "--------------------------------------------------\n",
            "iter: 973 \tloss: 0.11735682763134045, dist: 0.0006836238535927626\n",
            "--------------------------------------------------\n",
            "iter: 974 \tloss: 0.12617858592578593, dist: 0.0009927772795311146\n",
            "--------------------------------------------------\n",
            "iter: 975 \tloss: 0.12667693223891996, dist: 0.0008435936738652422\n",
            "--------------------------------------------------\n",
            "iter: 976 \tloss: 0.11416880782112396, dist: 0.0007260527880748549\n",
            "--------------------------------------------------\n",
            "iter: 977 \tloss: 0.11934818511759705, dist: 0.000810487636153234\n",
            "--------------------------------------------------\n",
            "iter: 978 \tloss: 0.10544383512830574, dist: 0.0006735827296427318\n",
            "--------------------------------------------------\n",
            "iter: 979 \tloss: 0.1294527891965842, dist: 0.0008164540349627446\n",
            "--------------------------------------------------\n",
            "iter: 980 \tloss: 0.11212420944954991, dist: 0.000896725819479579\n",
            "--------------------------------------------------\n",
            "iter: 981 \tloss: 0.11930076114998789, dist: 0.0007072139257494595\n",
            "--------------------------------------------------\n",
            "iter: 982 \tloss: 0.11758442008032435, dist: 0.0008281399435510991\n",
            "--------------------------------------------------\n",
            "iter: 983 \tloss: 0.11885487237562264, dist: 0.0010353048943051607\n",
            "--------------------------------------------------\n",
            "iter: 984 \tloss: 0.1348682467468209, dist: 0.0009726681650086429\n",
            "--------------------------------------------------\n",
            "iter: 985 \tloss: 0.11826483201932195, dist: 0.0008192064834581145\n",
            "--------------------------------------------------\n",
            "iter: 986 \tloss: 0.11507790179409155, dist: 0.0008655745262729464\n",
            "--------------------------------------------------\n",
            "iter: 987 \tloss: 0.11954026017878289, dist: 0.0008378124978278952\n",
            "--------------------------------------------------\n",
            "iter: 988 \tloss: 0.12051201790354504, dist: 0.0009049535060500085\n",
            "--------------------------------------------------\n",
            "iter: 989 \tloss: 0.114032013651621, dist: 0.0006776609630233534\n",
            "--------------------------------------------------\n",
            "iter: 990 \tloss: 0.11911030973277531, dist: 0.0007929321503605401\n",
            "--------------------------------------------------\n",
            "iter: 991 \tloss: 0.11716686442275216, dist: 0.0008104619554783836\n",
            "--------------------------------------------------\n",
            "iter: 992 \tloss: 0.13397617485437324, dist: 0.000804827391153349\n",
            "--------------------------------------------------\n",
            "iter: 993 \tloss: 0.12374630798890002, dist: 0.0007729856958080455\n",
            "--------------------------------------------------\n",
            "iter: 994 \tloss: 0.11606557325971195, dist: 0.0007519735663184612\n",
            "--------------------------------------------------\n",
            "iter: 995 \tloss: 0.11601266437748503, dist: 0.0008127407290746438\n",
            "--------------------------------------------------\n",
            "iter: 996 \tloss: 0.12757581862871042, dist: 0.0008957405213386733\n",
            "--------------------------------------------------\n",
            "iter: 997 \tloss: 0.10853395435335735, dist: 0.000875140484379781\n",
            "--------------------------------------------------\n",
            "iter: 998 \tloss: 0.12744894859816416, dist: 0.0008035820005035996\n",
            "--------------------------------------------------\n",
            "iter: 999 \tloss: 0.11683918906889862, dist: 0.0007937489750475823\n",
            "--------------------------------------------------\n",
            "iter: 1000 \tloss: 0.11392962227118981, dist: 0.0007917267745932322\n",
            "--------------------------------------------------\n",
            "iter: 1001 \tloss: 0.12842601507586263, dist: 0.0007302394211971038\n",
            "--------------------------------------------------\n",
            "iter: 1002 \tloss: 0.13082353039531125, dist: 0.0008620201406268345\n",
            "--------------------------------------------------\n",
            "iter: 1003 \tloss: 0.11735790450108029, dist: 0.0007677844220958334\n",
            "--------------------------------------------------\n",
            "iter: 1004 \tloss: 0.1139493892448011, dist: 0.0007828440236485382\n",
            "--------------------------------------------------\n",
            "iter: 1005 \tloss: 0.11972080779878667, dist: 0.000874708344460028\n",
            "--------------------------------------------------\n",
            "iter: 1006 \tloss: 0.12401937909820485, dist: 0.0007740706631216735\n",
            "--------------------------------------------------\n",
            "iter: 1007 \tloss: 0.13431085106492732, dist: 0.0008827086580474153\n",
            "--------------------------------------------------\n",
            "iter: 1008 \tloss: 0.11285942735685696, dist: 0.0007514093815867047\n",
            "--------------------------------------------------\n",
            "iter: 1009 \tloss: 0.11513984561093843, dist: 0.0007290823867881207\n",
            "--------------------------------------------------\n",
            "iter: 1010 \tloss: 0.11632582512222106, dist: 0.0008616492366727891\n",
            "--------------------------------------------------\n",
            "iter: 1011 \tloss: 0.1259340628349009, dist: 0.0007489944092371629\n",
            "--------------------------------------------------\n",
            "iter: 1012 \tloss: 0.11623012982310628, dist: 0.000773821230433359\n",
            "--------------------------------------------------\n",
            "iter: 1013 \tloss: 0.12113615986546006, dist: 0.0008563728676093324\n",
            "--------------------------------------------------\n",
            "iter: 1014 \tloss: 0.1198603143817024, dist: 0.0007637022044151745\n",
            "--------------------------------------------------\n",
            "iter: 1015 \tloss: 0.13482847550485225, dist: 0.0009989490434500984\n",
            "--------------------------------------------------\n",
            "iter: 1016 \tloss: 0.11824210818311949, dist: 0.0007449982788570183\n",
            "--------------------------------------------------\n",
            "iter: 1017 \tloss: 0.1212607865944372, dist: 0.0008794402749338852\n",
            "--------------------------------------------------\n",
            "iter: 1018 \tloss: 0.13044949498994518, dist: 0.0007862008003710903\n",
            "--------------------------------------------------\n",
            "iter: 1019 \tloss: 0.13043140896945418, dist: 0.0008890926277269944\n",
            "--------------------------------------------------\n",
            "iter: 1020 \tloss: 0.11693608313346739, dist: 0.0008150168605174826\n",
            "--------------------------------------------------\n",
            "iter: 1021 \tloss: 0.12055732517410132, dist: 0.0008080038314853398\n",
            "--------------------------------------------------\n",
            "iter: 1022 \tloss: 0.11176945979956826, dist: 0.0007741027746309949\n",
            "--------------------------------------------------\n",
            "iter: 1023 \tloss: 0.12897987742897185, dist: 0.0007429069494508347\n",
            "--------------------------------------------------\n",
            "iter: 1024 \tloss: 0.13593140259383224, dist: 0.0008298693001633464\n",
            "--------------------------------------------------\n",
            "iter: 1025 \tloss: 0.1098396525042507, dist: 0.0009328456214153044\n",
            "--------------------------------------------------\n",
            "iter: 1026 \tloss: 0.1116695867803768, dist: 0.0007594730221807734\n",
            "--------------------------------------------------\n",
            "iter: 1027 \tloss: 0.11078957077046456, dist: 0.0008514651327600916\n",
            "--------------------------------------------------\n",
            "iter: 1028 \tloss: 0.11903262308558299, dist: 0.0008011054507647969\n",
            "--------------------------------------------------\n",
            "iter: 1029 \tloss: 0.12108164197900793, dist: 0.0008428301591533265\n",
            "--------------------------------------------------\n",
            "iter: 1030 \tloss: 0.1208287354253401, dist: 0.0009715166940928138\n",
            "--------------------------------------------------\n",
            "iter: 1031 \tloss: 0.11343506304464962, dist: 0.0007373119770878304\n",
            "--------------------------------------------------\n",
            "iter: 1032 \tloss: 0.11960916138534781, dist: 0.0007666023222880486\n",
            "--------------------------------------------------\n",
            "iter: 1033 \tloss: 0.12190716255046152, dist: 0.0009142401038604898\n",
            "--------------------------------------------------\n",
            "iter: 1034 \tloss: 0.11696928519669499, dist: 0.0008131415472173709\n",
            "--------------------------------------------------\n",
            "iter: 1035 \tloss: 0.11727737153619903, dist: 0.0008412632234947848\n",
            "--------------------------------------------------\n",
            "iter: 1036 \tloss: 0.11748313130812155, dist: 0.0008347257463688792\n",
            "--------------------------------------------------\n",
            "iter: 1037 \tloss: 0.11708337508097194, dist: 0.0008304743287476739\n",
            "--------------------------------------------------\n",
            "iter: 1038 \tloss: 0.1099070510593659, dist: 0.0007943129431317073\n",
            "--------------------------------------------------\n",
            "iter: 1039 \tloss: 0.12427686980021659, dist: 0.0008084636260900321\n",
            "--------------------------------------------------\n",
            "iter: 1040 \tloss: 0.11991965274132403, dist: 0.000898919523546856\n",
            "--------------------------------------------------\n",
            "iter: 1041 \tloss: 0.12233115741241224, dist: 0.000838706918888903\n",
            "--------------------------------------------------\n",
            "iter: 1042 \tloss: 0.10750340237824023, dist: 0.0008294036005754575\n",
            "--------------------------------------------------\n",
            "iter: 1043 \tloss: 0.12714668338288773, dist: 0.0008544822183183511\n",
            "--------------------------------------------------\n",
            "iter: 1044 \tloss: 0.12421675990108567, dist: 0.0007883362572164182\n",
            "--------------------------------------------------\n",
            "iter: 1045 \tloss: 0.12528131476931487, dist: 0.0007941619066552004\n",
            "--------------------------------------------------\n",
            "iter: 1046 \tloss: 0.12396922649000984, dist: 0.0008870513713343573\n",
            "--------------------------------------------------\n",
            "iter: 1047 \tloss: 0.11400479651837636, dist: 0.0006989563486291643\n",
            "--------------------------------------------------\n",
            "iter: 1048 \tloss: 0.11657629875454743, dist: 0.0008901838407808522\n",
            "--------------------------------------------------\n",
            "iter: 1049 \tloss: 0.1201951316500196, dist: 0.0008515186034898351\n",
            "--------------------------------------------------\n",
            "iter: 1050 \tloss: 0.11647531745259611, dist: 0.0008579066092405537\n",
            "--------------------------------------------------\n",
            "iter: 1051 \tloss: 0.12495422375889972, dist: 0.000816371881371205\n",
            "--------------------------------------------------\n",
            "iter: 1052 \tloss: 0.11352041096112714, dist: 0.0009286122126163791\n",
            "--------------------------------------------------\n",
            "iter: 1053 \tloss: 0.1168182487825053, dist: 0.0008010908785162622\n",
            "--------------------------------------------------\n",
            "iter: 1054 \tloss: 0.11600269698206499, dist: 0.0009310334635341009\n",
            "--------------------------------------------------\n",
            "iter: 1055 \tloss: 0.11740166277771244, dist: 0.0007874694716769113\n",
            "--------------------------------------------------\n",
            "iter: 1056 \tloss: 0.1173121581052491, dist: 0.0009193739932816288\n",
            "--------------------------------------------------\n",
            "iter: 1057 \tloss: 0.12508618341632266, dist: 0.0008504655411166397\n",
            "--------------------------------------------------\n",
            "iter: 1058 \tloss: 0.12585942779741602, dist: 0.0008583720231316368\n",
            "--------------------------------------------------\n",
            "iter: 1059 \tloss: 0.1222265664263396, dist: 0.0008647579831420443\n",
            "--------------------------------------------------\n",
            "iter: 1060 \tloss: 0.12320424830166808, dist: 0.0008641715699212155\n",
            "--------------------------------------------------\n",
            "iter: 1061 \tloss: 0.11983431249704393, dist: 0.0008970246339996972\n",
            "--------------------------------------------------\n",
            "iter: 1062 \tloss: 0.11906281933894966, dist: 0.0008659589978209045\n",
            "--------------------------------------------------\n",
            "iter: 1063 \tloss: 0.12584773836343485, dist: 0.000896281408323642\n",
            "--------------------------------------------------\n",
            "iter: 1064 \tloss: 0.1193917393809485, dist: 0.0008710216234368713\n",
            "--------------------------------------------------\n",
            "iter: 1065 \tloss: 0.12066514573710002, dist: 0.0008815607246852291\n",
            "--------------------------------------------------\n",
            "iter: 1066 \tloss: 0.12494584690068276, dist: 0.0008388827356369928\n",
            "--------------------------------------------------\n",
            "iter: 1067 \tloss: 0.11371222967706567, dist: 0.0007851725726788493\n",
            "--------------------------------------------------\n",
            "iter: 1068 \tloss: 0.1164426817139828, dist: 0.0008408732131947389\n",
            "--------------------------------------------------\n",
            "iter: 1069 \tloss: 0.11347043689222622, dist: 0.000854412511135633\n",
            "--------------------------------------------------\n",
            "iter: 1070 \tloss: 0.11446319821924834, dist: 0.0008603469029289719\n",
            "--------------------------------------------------\n",
            "iter: 1071 \tloss: 0.12232921920741564, dist: 0.0009213520654309675\n",
            "--------------------------------------------------\n",
            "iter: 1072 \tloss: 0.1268047617451646, dist: 0.0008741612864424645\n",
            "--------------------------------------------------\n",
            "iter: 1073 \tloss: 0.1093077662766553, dist: 0.0008460553111868125\n",
            "--------------------------------------------------\n",
            "iter: 1074 \tloss: 0.11868837726246671, dist: 0.0009404849664296969\n",
            "--------------------------------------------------\n",
            "iter: 1075 \tloss: 0.11559600053116387, dist: 0.0007334928545651106\n",
            "--------------------------------------------------\n",
            "iter: 1076 \tloss: 0.12595673662104384, dist: 0.0010612583760581616\n",
            "--------------------------------------------------\n",
            "iter: 1077 \tloss: 0.12633464679046025, dist: 0.0007579403659449299\n",
            "--------------------------------------------------\n",
            "iter: 1078 \tloss: 0.14806730892553158, dist: 0.0011576235039351206\n",
            "--------------------------------------------------\n",
            "iter: 1079 \tloss: 0.12500899380314207, dist: 0.0008962117984773825\n",
            "--------------------------------------------------\n",
            "iter: 1080 \tloss: 0.12388138571084929, dist: 0.0009454799709104907\n",
            "--------------------------------------------------\n",
            "iter: 1081 \tloss: 0.11212328358884244, dist: 0.0008447675045465862\n",
            "--------------------------------------------------\n",
            "iter: 1082 \tloss: 0.11854356675721817, dist: 0.0009052155224038007\n",
            "--------------------------------------------------\n",
            "iter: 1083 \tloss: 0.11933813284020313, dist: 0.0009113245933983724\n",
            "--------------------------------------------------\n",
            "iter: 1084 \tloss: 0.11324229804216478, dist: 0.0008956522865240911\n",
            "--------------------------------------------------\n",
            "iter: 1085 \tloss: 0.11865227684814007, dist: 0.0008505995964022003\n",
            "--------------------------------------------------\n",
            "iter: 1086 \tloss: 0.11776014884597519, dist: 0.0009397999876117359\n",
            "--------------------------------------------------\n",
            "iter: 1087 \tloss: 0.11688107622956954, dist: 0.0008640432851293611\n",
            "--------------------------------------------------\n",
            "iter: 1088 \tloss: 0.12448251602248672, dist: 0.0009785497060395359\n",
            "--------------------------------------------------\n",
            "iter: 1089 \tloss: 0.11923942110227735, dist: 0.0008224219055558347\n",
            "--------------------------------------------------\n",
            "iter: 1090 \tloss: 0.11526995087972863, dist: 0.0008886747247115797\n",
            "--------------------------------------------------\n",
            "iter: 1091 \tloss: 0.12230636025985314, dist: 0.0008886611835570573\n",
            "--------------------------------------------------\n",
            "iter: 1092 \tloss: 0.11987764940898916, dist: 0.0009106154517176193\n",
            "--------------------------------------------------\n",
            "iter: 1093 \tloss: 0.11713410412521519, dist: 0.0011359514151179622\n",
            "--------------------------------------------------\n",
            "iter: 1094 \tloss: 0.1283901269248615, dist: 0.0009195943021348292\n",
            "--------------------------------------------------\n",
            "iter: 1095 \tloss: 0.10704061789818636, dist: 0.0007002398326038961\n",
            "--------------------------------------------------\n",
            "iter: 1096 \tloss: 0.1379862865249293, dist: 0.0009775254822259806\n",
            "--------------------------------------------------\n",
            "iter: 1097 \tloss: 0.12426584209233832, dist: 0.001078461397492543\n",
            "--------------------------------------------------\n",
            "iter: 1098 \tloss: 0.12503003826085832, dist: 0.0009559384137091842\n",
            "--------------------------------------------------\n",
            "iter: 1099 \tloss: 0.11218513180619666, dist: 0.0009919134458858739\n",
            "--------------------------------------------------\n",
            "iter: 1100 \tloss: 0.12408144056710321, dist: 0.0009591446723594782\n",
            "--------------------------------------------------\n",
            "iter: 1101 \tloss: 0.12236356417326903, dist: 0.0008747068309632905\n",
            "--------------------------------------------------\n",
            "iter: 1102 \tloss: 0.12215859780928894, dist: 0.0009211109624719913\n",
            "--------------------------------------------------\n",
            "iter: 1103 \tloss: 0.1235041781948135, dist: 0.000999783353904241\n",
            "--------------------------------------------------\n",
            "iter: 1104 \tloss: 0.12164897457752141, dist: 0.0009018327794270403\n",
            "--------------------------------------------------\n",
            "iter: 1105 \tloss: 0.12052622499068123, dist: 0.0009190003786493452\n",
            "--------------------------------------------------\n",
            "iter: 1106 \tloss: 0.12089784362044803, dist: 0.0009662897848855749\n",
            "--------------------------------------------------\n",
            "iter: 1107 \tloss: 0.12109150285772806, dist: 0.0008796880931987675\n",
            "--------------------------------------------------\n",
            "iter: 1108 \tloss: 0.11140242247989475, dist: 0.000880803057001056\n",
            "--------------------------------------------------\n",
            "iter: 1109 \tloss: 0.12472135028337127, dist: 0.0010546950884045311\n",
            "--------------------------------------------------\n",
            "iter: 1110 \tloss: 0.1182507366842111, dist: 0.0009209284455693848\n",
            "--------------------------------------------------\n",
            "iter: 1111 \tloss: 0.13145502691627164, dist: 0.0009654627664559701\n",
            "--------------------------------------------------\n",
            "iter: 1112 \tloss: 0.12728541318787376, dist: 0.0009747361188820084\n",
            "--------------------------------------------------\n",
            "iter: 1113 \tloss: 0.12160161119886764, dist: 0.0010469311113653701\n",
            "--------------------------------------------------\n",
            "iter: 1114 \tloss: 0.1165089466217605, dist: 0.0010867400307807583\n",
            "--------------------------------------------------\n",
            "iter: 1115 \tloss: 0.11109623121051068, dist: 0.0008743106873358421\n",
            "--------------------------------------------------\n",
            "iter: 1116 \tloss: 0.12106586023219416, dist: 0.0009519996774602558\n",
            "--------------------------------------------------\n",
            "iter: 1117 \tloss: 0.11418900851705134, dist: 0.0009487122192866442\n",
            "--------------------------------------------------\n",
            "iter: 1118 \tloss: 0.12154912109649176, dist: 0.0008861763223551415\n",
            "--------------------------------------------------\n",
            "iter: 1119 \tloss: 0.11685609435050069, dist: 0.0009230450408340099\n",
            "--------------------------------------------------\n",
            "iter: 1120 \tloss: 0.12430634213235207, dist: 0.0009891090401343153\n",
            "--------------------------------------------------\n",
            "iter: 1121 \tloss: 0.12188367929878187, dist: 0.0009427120383548409\n",
            "--------------------------------------------------\n",
            "iter: 1122 \tloss: 0.12097412952591825, dist: 0.0009981150927195566\n",
            "--------------------------------------------------\n",
            "iter: 1123 \tloss: 0.10784654636984087, dist: 0.000939575160350703\n",
            "--------------------------------------------------\n",
            "iter: 1124 \tloss: 0.12058589520803671, dist: 0.0009858062546960233\n",
            "--------------------------------------------------\n",
            "iter: 1125 \tloss: 0.11865767232948285, dist: 0.0009196038976763389\n",
            "--------------------------------------------------\n",
            "iter: 1126 \tloss: 0.12311709275692348, dist: 0.0010544807558362835\n",
            "--------------------------------------------------\n",
            "iter: 1127 \tloss: 0.11868863399828382, dist: 0.0009339827368163258\n",
            "--------------------------------------------------\n",
            "iter: 1128 \tloss: 0.12202289045015355, dist: 0.0009526375427523644\n",
            "--------------------------------------------------\n",
            "iter: 1129 \tloss: 0.124469364040117, dist: 0.0009975988577224813\n",
            "--------------------------------------------------\n",
            "iter: 1130 \tloss: 0.12130884027141585, dist: 0.0009997410336910404\n",
            "--------------------------------------------------\n",
            "iter: 1131 \tloss: 0.11687829242166835, dist: 0.0009019281101270944\n",
            "--------------------------------------------------\n",
            "iter: 1132 \tloss: 0.12021509191125905, dist: 0.0009437387269221648\n",
            "--------------------------------------------------\n",
            "iter: 1133 \tloss: 0.12308299028444329, dist: 0.0011295891339006081\n",
            "--------------------------------------------------\n",
            "iter: 1134 \tloss: 0.1106557675770706, dist: 0.0009595090647295031\n",
            "--------------------------------------------------\n",
            "iter: 1135 \tloss: 0.12023004865956657, dist: 0.0008684239189595418\n",
            "--------------------------------------------------\n",
            "iter: 1136 \tloss: 0.11530386382262181, dist: 0.000949411794113423\n",
            "--------------------------------------------------\n",
            "iter: 1137 \tloss: 0.13121344251700065, dist: 0.0010573604791810434\n",
            "--------------------------------------------------\n",
            "iter: 1138 \tloss: 0.10891562212887405, dist: 0.0008870146230592311\n",
            "--------------------------------------------------\n",
            "iter: 1139 \tloss: 0.12795753416100922, dist: 0.0009530822985158624\n",
            "--------------------------------------------------\n",
            "iter: 1140 \tloss: 0.11618987084298377, dist: 0.0009033347135309628\n",
            "--------------------------------------------------\n",
            "iter: 1141 \tloss: 0.12733046922958108, dist: 0.001023551554916337\n",
            "--------------------------------------------------\n",
            "iter: 1142 \tloss: 0.11742526897937168, dist: 0.0008894139362778313\n",
            "--------------------------------------------------\n",
            "iter: 1143 \tloss: 0.11877313247007364, dist: 0.0008137331732036284\n",
            "--------------------------------------------------\n",
            "iter: 1144 \tloss: 0.11601034283366368, dist: 0.0009377215397956499\n",
            "--------------------------------------------------\n",
            "iter: 1145 \tloss: 0.12356434415782336, dist: 0.0009269422236745696\n",
            "--------------------------------------------------\n",
            "iter: 1146 \tloss: 0.11119534268031206, dist: 0.0009262758912283499\n",
            "--------------------------------------------------\n",
            "iter: 1147 \tloss: 0.12147910112461151, dist: 0.000861192015058831\n",
            "--------------------------------------------------\n",
            "iter: 1148 \tloss: 0.11580074172217608, dist: 0.0009600966937289359\n",
            "--------------------------------------------------\n",
            "iter: 1149 \tloss: 0.11720329897851135, dist: 0.0009037850623237133\n",
            "--------------------------------------------------\n",
            "iter: 1150 \tloss: 0.12420718429752546, dist: 0.000896896845711534\n",
            "--------------------------------------------------\n",
            "iter: 1151 \tloss: 0.12304197118398098, dist: 0.0009299336903043863\n",
            "--------------------------------------------------\n",
            "iter: 1152 \tloss: 0.12466550024923705, dist: 0.0009025494056396741\n",
            "--------------------------------------------------\n",
            "iter: 1153 \tloss: 0.11776915501719647, dist: 0.0010525186385857766\n",
            "--------------------------------------------------\n",
            "iter: 1154 \tloss: 0.11714010699920402, dist: 0.000878404290607641\n",
            "--------------------------------------------------\n",
            "iter: 1155 \tloss: 0.12251959968183784, dist: 0.0009480612763229833\n",
            "--------------------------------------------------\n",
            "iter: 1156 \tloss: 0.12389280765335209, dist: 0.0009464520702903262\n",
            "--------------------------------------------------\n",
            "iter: 1157 \tloss: 0.11784383437420062, dist: 0.00091976282854233\n",
            "--------------------------------------------------\n",
            "iter: 1158 \tloss: 0.11347760264817358, dist: 0.000953425275412473\n",
            "--------------------------------------------------\n",
            "iter: 1159 \tloss: 0.13012934680435845, dist: 0.0009814824305413757\n",
            "--------------------------------------------------\n",
            "iter: 1160 \tloss: 0.11143910909424193, dist: 0.0009790088871924662\n",
            "--------------------------------------------------\n",
            "iter: 1161 \tloss: 0.11988574550353172, dist: 0.0009204731468869714\n",
            "--------------------------------------------------\n",
            "iter: 1162 \tloss: 0.11738979940954054, dist: 0.0009493796923263858\n",
            "--------------------------------------------------\n",
            "iter: 1163 \tloss: 0.12495292422965892, dist: 0.001010268738246677\n",
            "--------------------------------------------------\n",
            "iter: 1164 \tloss: 0.12236679735623847, dist: 0.001071953243070215\n",
            "--------------------------------------------------\n",
            "iter: 1165 \tloss: 0.10686032642222688, dist: 0.0009428457460086872\n",
            "--------------------------------------------------\n",
            "iter: 1166 \tloss: 0.1308816448486951, dist: 0.0010279228356359954\n",
            "--------------------------------------------------\n",
            "iter: 1167 \tloss: 0.11774888345254383, dist: 0.001029374411854281\n",
            "--------------------------------------------------\n",
            "iter: 1168 \tloss: 0.12095155124293214, dist: 0.000950469626800949\n",
            "--------------------------------------------------\n",
            "iter: 1169 \tloss: 0.1122520379484311, dist: 0.0009228377225375551\n",
            "--------------------------------------------------\n",
            "iter: 1170 \tloss: 0.12731183693144438, dist: 0.0011583323870503614\n",
            "--------------------------------------------------\n",
            "iter: 1171 \tloss: 0.12192887128122099, dist: 0.0009535755318328493\n",
            "--------------------------------------------------\n",
            "iter: 1172 \tloss: 0.11268353118261652, dist: 0.0009378327706717166\n",
            "--------------------------------------------------\n",
            "iter: 1173 \tloss: 0.12345389065630878, dist: 0.0009551636804992057\n",
            "--------------------------------------------------\n",
            "iter: 1174 \tloss: 0.11924685182652674, dist: 0.0010060782158210094\n",
            "--------------------------------------------------\n",
            "iter: 1175 \tloss: 0.12020743493000174, dist: 0.000999214901288179\n",
            "--------------------------------------------------\n",
            "iter: 1176 \tloss: 0.11295795154334537, dist: 0.0009228083566008943\n",
            "--------------------------------------------------\n",
            "iter: 1177 \tloss: 0.1306214920446574, dist: 0.0011792266521731423\n",
            "--------------------------------------------------\n",
            "iter: 1178 \tloss: 0.11857380188437415, dist: 0.000909146845285479\n",
            "--------------------------------------------------\n",
            "iter: 1179 \tloss: 0.12011881842291093, dist: 0.0009829214199999564\n",
            "--------------------------------------------------\n",
            "iter: 1180 \tloss: 0.1201908013676162, dist: 0.0009019126480885643\n",
            "--------------------------------------------------\n",
            "iter: 1181 \tloss: 0.1195754693594084, dist: 0.0009838800051670388\n",
            "--------------------------------------------------\n",
            "iter: 1182 \tloss: 0.11513105225785834, dist: 0.0009782064371229174\n",
            "--------------------------------------------------\n",
            "iter: 1183 \tloss: 0.13330768624267347, dist: 0.001084638572442878\n",
            "--------------------------------------------------\n",
            "iter: 1184 \tloss: 0.11049703381531967, dist: 0.0009274907787135189\n",
            "--------------------------------------------------\n",
            "iter: 1185 \tloss: 0.11137451063892576, dist: 0.001018028107640655\n",
            "--------------------------------------------------\n",
            "iter: 1186 \tloss: 0.12261094486051806, dist: 0.001004352403359822\n",
            "--------------------------------------------------\n",
            "iter: 1187 \tloss: 0.11659266797449036, dist: 0.0009881730982975443\n",
            "--------------------------------------------------\n",
            "iter: 1188 \tloss: 0.12433671011086464, dist: 0.000916304427252643\n",
            "--------------------------------------------------\n",
            "iter: 1189 \tloss: 0.11966483751185061, dist: 0.0009552700796432598\n",
            "--------------------------------------------------\n",
            "iter: 1190 \tloss: 0.11209404974444111, dist: 0.0009757491929679745\n",
            "--------------------------------------------------\n",
            "iter: 1191 \tloss: 0.11912593666259329, dist: 0.0009358748374119569\n",
            "--------------------------------------------------\n",
            "iter: 1192 \tloss: 0.1237212999244519, dist: 0.0011566034864759066\n",
            "--------------------------------------------------\n",
            "iter: 1193 \tloss: 0.11452068424652223, dist: 0.0010188555977667874\n",
            "--------------------------------------------------\n",
            "iter: 1194 \tloss: 0.12066003270139349, dist: 0.0009226817941495356\n",
            "--------------------------------------------------\n",
            "iter: 1195 \tloss: 0.11991477421285421, dist: 0.0010026895962533347\n",
            "--------------------------------------------------\n",
            "iter: 1196 \tloss: 0.11828127251184163, dist: 0.0008594082405079076\n",
            "--------------------------------------------------\n",
            "iter: 1197 \tloss: 0.11910825633176504, dist: 0.0009417719266199318\n",
            "--------------------------------------------------\n",
            "iter: 1198 \tloss: 0.11998976767956132, dist: 0.0010061311619051598\n",
            "--------------------------------------------------\n",
            "iter: 1199 \tloss: 0.11484749005983645, dist: 0.0009466912620240023\n",
            "--------------------------------------------------\n",
            "iter: 1200 \tloss: 0.11864872952699192, dist: 0.001033762671871627\n",
            "--------------------------------------------------\n",
            "iter: 1201 \tloss: 0.11881760536750917, dist: 0.0009237729009990262\n",
            "--------------------------------------------------\n",
            "iter: 1202 \tloss: 0.11869550823800973, dist: 0.00101664246883252\n",
            "--------------------------------------------------\n",
            "iter: 1203 \tloss: 0.11377865101615087, dist: 0.0008586199544612993\n",
            "--------------------------------------------------\n",
            "iter: 1204 \tloss: 0.13008550305720948, dist: 0.0008956380219667915\n",
            "--------------------------------------------------\n",
            "iter: 1205 \tloss: 0.10444633570907712, dist: 0.00091948378371947\n",
            "--------------------------------------------------\n",
            "iter: 1206 \tloss: 0.14623062905344908, dist: 0.0008862153085190962\n",
            "--------------------------------------------------\n",
            "iter: 1207 \tloss: 0.11509232707079949, dist: 0.001098877966776571\n",
            "--------------------------------------------------\n",
            "iter: 1208 \tloss: 0.12021909141668942, dist: 0.0009379365724739388\n",
            "--------------------------------------------------\n",
            "iter: 1209 \tloss: 0.12072545066433306, dist: 0.0010271228576082816\n",
            "--------------------------------------------------\n",
            "iter: 1210 \tloss: 0.11857620657033491, dist: 0.0009556752867826775\n",
            "--------------------------------------------------\n",
            "iter: 1211 \tloss: 0.12058731984350356, dist: 0.0009522295466375778\n",
            "--------------------------------------------------\n",
            "iter: 1212 \tloss: 0.12179272773718913, dist: 0.0009972308478059073\n",
            "--------------------------------------------------\n",
            "iter: 1213 \tloss: 0.10993929395731411, dist: 0.0009525005744213826\n",
            "--------------------------------------------------\n",
            "iter: 1214 \tloss: 0.12653579513520433, dist: 0.000950719134630676\n",
            "--------------------------------------------------\n",
            "iter: 1215 \tloss: 0.12863356705230056, dist: 0.0009845420366644448\n",
            "--------------------------------------------------\n",
            "iter: 1216 \tloss: 0.11373271287056347, dist: 0.0010848966707183293\n",
            "--------------------------------------------------\n",
            "iter: 1217 \tloss: 0.11613078350086875, dist: 0.0009382133776050734\n",
            "--------------------------------------------------\n",
            "iter: 1218 \tloss: 0.12534190777298282, dist: 0.0009816343377637574\n",
            "--------------------------------------------------\n",
            "iter: 1219 \tloss: 0.11938578722931993, dist: 0.0009593229337289674\n",
            "--------------------------------------------------\n",
            "iter: 1220 \tloss: 0.11907238983626424, dist: 0.0010020402082156227\n",
            "--------------------------------------------------\n",
            "iter: 1221 \tloss: 0.11889251876216816, dist: 0.0009467837140041918\n",
            "--------------------------------------------------\n",
            "iter: 1222 \tloss: 0.11618441359541815, dist: 0.000984284791173601\n",
            "--------------------------------------------------\n",
            "iter: 1223 \tloss: 0.11293787110480341, dist: 0.0009401129425849319\n",
            "--------------------------------------------------\n",
            "iter: 1224 \tloss: 0.12107342475706276, dist: 0.0009845943093407612\n",
            "--------------------------------------------------\n",
            "iter: 1225 \tloss: 0.12469092251056688, dist: 0.000932614056397944\n",
            "--------------------------------------------------\n",
            "iter: 1226 \tloss: 0.11239812973712565, dist: 0.001007119957976359\n",
            "--------------------------------------------------\n",
            "iter: 1227 \tloss: 0.1266156329858517, dist: 0.0009933157131545975\n",
            "--------------------------------------------------\n",
            "iter: 1228 \tloss: 0.11770869951754638, dist: 0.0009965113550772504\n",
            "--------------------------------------------------\n",
            "iter: 1229 \tloss: 0.11250889899542145, dist: 0.0010163588338020749\n",
            "--------------------------------------------------\n",
            "iter: 1230 \tloss: 0.11881235987200743, dist: 0.0010199061155107356\n",
            "--------------------------------------------------\n",
            "iter: 1231 \tloss: 0.11836398250183194, dist: 0.0009429708049350935\n",
            "--------------------------------------------------\n",
            "iter: 1232 \tloss: 0.10624492624854805, dist: 0.0010238085740179358\n",
            "--------------------------------------------------\n",
            "iter: 1233 \tloss: 0.125679113920593, dist: 0.001123058322249059\n",
            "--------------------------------------------------\n",
            "iter: 1234 \tloss: 0.11996486375484623, dist: 0.0010458185816306705\n",
            "--------------------------------------------------\n",
            "iter: 1235 \tloss: 0.13729604308567692, dist: 0.0010639969459898613\n",
            "--------------------------------------------------\n",
            "iter: 1236 \tloss: 0.12816961147653946, dist: 0.0009094656961902224\n",
            "--------------------------------------------------\n",
            "iter: 1237 \tloss: 0.11725829475518487, dist: 0.00111050865256465\n",
            "--------------------------------------------------\n",
            "iter: 1238 \tloss: 0.13629046720942348, dist: 0.000999191017639057\n",
            "--------------------------------------------------\n",
            "iter: 1239 \tloss: 0.1182685844865952, dist: 0.001077399707664528\n",
            "--------------------------------------------------\n",
            "iter: 1240 \tloss: 0.11530109861974397, dist: 0.0009604590081736882\n",
            "--------------------------------------------------\n",
            "iter: 1241 \tloss: 0.12211576248595239, dist: 0.0010708580830688578\n",
            "--------------------------------------------------\n",
            "iter: 1242 \tloss: 0.1183226541617124, dist: 0.0009700728845986003\n",
            "--------------------------------------------------\n",
            "iter: 1243 \tloss: 0.12108244505227725, dist: 0.0010226584489400762\n",
            "--------------------------------------------------\n",
            "iter: 1244 \tloss: 0.12212790560796494, dist: 0.0009528090349019154\n",
            "--------------------------------------------------\n",
            "iter: 1245 \tloss: 0.1108306535610474, dist: 0.0010782476969033205\n",
            "--------------------------------------------------\n",
            "iter: 1246 \tloss: 0.12432005177769091, dist: 0.0009520469992607277\n",
            "--------------------------------------------------\n",
            "iter: 1247 \tloss: 0.12061557153343673, dist: 0.001144141579913144\n",
            "--------------------------------------------------\n",
            "iter: 1248 \tloss: 0.11383595096872655, dist: 0.0009441687074779327\n",
            "--------------------------------------------------\n",
            "iter: 1249 \tloss: 0.1256325699883831, dist: 0.0010922919608206143\n",
            "--------------------------------------------------\n",
            "iter: 1250 \tloss: 0.11243278466960893, dist: 0.0010941821654241262\n",
            "--------------------------------------------------\n",
            "iter: 1251 \tloss: 0.11928854776738639, dist: 0.0010077563848242242\n",
            "--------------------------------------------------\n",
            "iter: 1252 \tloss: 0.11425988410803548, dist: 0.0010233641345667576\n",
            "--------------------------------------------------\n",
            "iter: 1253 \tloss: 0.12139066506570294, dist: 0.000986128310226114\n",
            "--------------------------------------------------\n",
            "iter: 1254 \tloss: 0.12177285681099365, dist: 0.0010084731753680779\n",
            "--------------------------------------------------\n",
            "iter: 1255 \tloss: 0.11709014105041306, dist: 0.000928498458390721\n",
            "--------------------------------------------------\n",
            "iter: 1256 \tloss: 0.1232276764854051, dist: 0.0010491255177176638\n",
            "--------------------------------------------------\n",
            "iter: 1257 \tloss: 0.11204892521978795, dist: 0.0010706231979306872\n",
            "--------------------------------------------------\n",
            "iter: 1258 \tloss: 0.11595391162725638, dist: 0.001002045969676295\n",
            "--------------------------------------------------\n",
            "iter: 1259 \tloss: 0.11184355087486049, dist: 0.0009940890883304162\n",
            "--------------------------------------------------\n",
            "iter: 1260 \tloss: 0.13091106568278976, dist: 0.0010304119594925106\n",
            "--------------------------------------------------\n",
            "iter: 1261 \tloss: 0.11205245911068898, dist: 0.0010972452264666924\n",
            "--------------------------------------------------\n",
            "iter: 1262 \tloss: 0.12416360172219308, dist: 0.0010071442183686356\n",
            "--------------------------------------------------\n",
            "iter: 1263 \tloss: 0.12337620031428558, dist: 0.0010398228633619942\n",
            "--------------------------------------------------\n",
            "iter: 1264 \tloss: 0.1150219929396065, dist: 0.0009461474449758196\n",
            "--------------------------------------------------\n",
            "iter: 1265 \tloss: 0.12673556519556373, dist: 0.001031611117571037\n",
            "--------------------------------------------------\n",
            "iter: 1266 \tloss: 0.12146220563438291, dist: 0.001006240083022501\n",
            "--------------------------------------------------\n",
            "iter: 1267 \tloss: 0.11790972898373787, dist: 0.0010197634296375928\n",
            "--------------------------------------------------\n",
            "iter: 1268 \tloss: 0.1428946990126715, dist: 0.0009656738002117416\n",
            "--------------------------------------------------\n",
            "iter: 1269 \tloss: 0.12640908525883943, dist: 0.0010758095491218327\n",
            "--------------------------------------------------\n",
            "iter: 1270 \tloss: 0.11996619000013706, dist: 0.0010844018458542784\n",
            "--------------------------------------------------\n",
            "iter: 1271 \tloss: 0.11524326490353207, dist: 0.0009300823372987026\n",
            "--------------------------------------------------\n",
            "iter: 1272 \tloss: 0.11515751348245491, dist: 0.0010509846064243676\n",
            "--------------------------------------------------\n",
            "iter: 1273 \tloss: 0.11845597321990335, dist: 0.0010005389907691725\n",
            "--------------------------------------------------\n",
            "iter: 1274 \tloss: 0.11990268346689109, dist: 0.0010852677129484156\n",
            "--------------------------------------------------\n",
            "iter: 1275 \tloss: 0.12694069253949317, dist: 0.0009613905793317813\n",
            "--------------------------------------------------\n",
            "iter: 1276 \tloss: 0.1095541578408523, dist: 0.0009711250390999998\n",
            "--------------------------------------------------\n",
            "iter: 1277 \tloss: 0.11626358818824581, dist: 0.001001563638885058\n",
            "--------------------------------------------------\n",
            "iter: 1278 \tloss: 0.11429298360819834, dist: 0.0011245677090686397\n",
            "--------------------------------------------------\n",
            "iter: 1279 \tloss: 0.12516246744244947, dist: 0.0010189283446973372\n",
            "--------------------------------------------------\n",
            "iter: 1280 \tloss: 0.1192511695659134, dist: 0.001144081070842861\n",
            "--------------------------------------------------\n",
            "iter: 1281 \tloss: 0.11560840002255558, dist: 0.0010340337467798844\n",
            "--------------------------------------------------\n",
            "iter: 1282 \tloss: 0.11606571006937448, dist: 0.0010232805570975984\n",
            "--------------------------------------------------\n",
            "iter: 1283 \tloss: 0.11419828313510628, dist: 0.0009616176137268489\n",
            "--------------------------------------------------\n",
            "iter: 1284 \tloss: 0.11574893624067116, dist: 0.001068706858791842\n",
            "--------------------------------------------------\n",
            "iter: 1285 \tloss: 0.12423229662425504, dist: 0.0011317524748833217\n",
            "--------------------------------------------------\n",
            "iter: 1286 \tloss: 0.1213847343022124, dist: 0.0010817474252742534\n",
            "--------------------------------------------------\n",
            "iter: 1287 \tloss: 0.11826872842165397, dist: 0.0011281267121043875\n",
            "--------------------------------------------------\n",
            "iter: 1288 \tloss: 0.14213394057394185, dist: 0.001088518151102691\n",
            "--------------------------------------------------\n",
            "iter: 1289 \tloss: 0.12030261674096653, dist: 0.0010707033906556006\n",
            "--------------------------------------------------\n",
            "iter: 1290 \tloss: 0.11943406115654587, dist: 0.0011178980985187942\n",
            "--------------------------------------------------\n",
            "iter: 1291 \tloss: 0.12490537234530588, dist: 0.001091507642078649\n",
            "--------------------------------------------------\n",
            "iter: 1292 \tloss: 0.12366944720908925, dist: 0.001034353845193841\n",
            "--------------------------------------------------\n",
            "iter: 1293 \tloss: 0.12449535139568063, dist: 0.00098405312152286\n",
            "--------------------------------------------------\n",
            "iter: 1294 \tloss: 0.12208588041390955, dist: 0.0010717548510884563\n",
            "--------------------------------------------------\n",
            "iter: 1295 \tloss: 0.11867167380518602, dist: 0.000965567680470619\n",
            "--------------------------------------------------\n",
            "iter: 1296 \tloss: 0.12633327936247418, dist: 0.0010330856239338592\n",
            "--------------------------------------------------\n",
            "iter: 1297 \tloss: 0.11707859894008944, dist: 0.0010943518616423566\n",
            "--------------------------------------------------\n",
            "iter: 1298 \tloss: 0.10982416051496502, dist: 0.0011120527797876822\n",
            "--------------------------------------------------\n",
            "iter: 1299 \tloss: 0.12957671399295864, dist: 0.0011407764031133977\n",
            "--------------------------------------------------\n",
            "iter: 1300 \tloss: 0.11422346378719168, dist: 0.0010906364887099122\n",
            "--------------------------------------------------\n",
            "iter: 1301 \tloss: 0.12024204251123359, dist: 0.000998176711989822\n",
            "--------------------------------------------------\n",
            "iter: 1302 \tloss: 0.12377968195083668, dist: 0.0011213671078752737\n",
            "--------------------------------------------------\n",
            "iter: 1303 \tloss: 0.12998418711310478, dist: 0.0010386245388411149\n",
            "--------------------------------------------------\n",
            "iter: 1304 \tloss: 0.11261706065298037, dist: 0.0010192470130375901\n",
            "--------------------------------------------------\n",
            "iter: 1305 \tloss: 0.12977401489993273, dist: 0.0011322130371588083\n",
            "--------------------------------------------------\n",
            "iter: 1306 \tloss: 0.11851961672921202, dist: 0.0011335612789668916\n",
            "--------------------------------------------------\n",
            "iter: 1307 \tloss: 0.11355501064817192, dist: 0.0010322425913255476\n",
            "--------------------------------------------------\n",
            "iter: 1308 \tloss: 0.11538576240556422, dist: 0.001104925917298606\n",
            "--------------------------------------------------\n",
            "iter: 1309 \tloss: 0.1133577394608689, dist: 0.0010775053501710364\n",
            "--------------------------------------------------\n",
            "iter: 1310 \tloss: 0.11863385148674306, dist: 0.0011391455425806727\n",
            "--------------------------------------------------\n",
            "iter: 1311 \tloss: 0.11677174641192822, dist: 0.001099885875233386\n",
            "--------------------------------------------------\n",
            "iter: 1312 \tloss: 0.12091418692089716, dist: 0.001112690709043823\n",
            "--------------------------------------------------\n",
            "iter: 1313 \tloss: 0.12226928211766847, dist: 0.0011186192787266765\n",
            "--------------------------------------------------\n",
            "iter: 1314 \tloss: 0.11401588170193779, dist: 0.0011697292573309933\n",
            "--------------------------------------------------\n",
            "iter: 1315 \tloss: 0.12170868934526062, dist: 0.001146724070245045\n",
            "--------------------------------------------------\n",
            "iter: 1316 \tloss: 0.1180550222845843, dist: 0.0011940493568753222\n",
            "--------------------------------------------------\n",
            "iter: 1317 \tloss: 0.11349751790148023, dist: 0.0011662270238376085\n",
            "--------------------------------------------------\n",
            "iter: 1318 \tloss: 0.11669831167800039, dist: 0.0010417290531229211\n",
            "--------------------------------------------------\n",
            "iter: 1319 \tloss: 0.13371452482783194, dist: 0.0011527152916233523\n",
            "--------------------------------------------------\n",
            "iter: 1320 \tloss: 0.10618890480113415, dist: 0.0010474349585664392\n",
            "--------------------------------------------------\n",
            "iter: 1321 \tloss: 0.12895300089803088, dist: 0.0012306790893195595\n",
            "--------------------------------------------------\n",
            "iter: 1322 \tloss: 0.12205045736689729, dist: 0.0012106339384920309\n",
            "--------------------------------------------------\n",
            "iter: 1323 \tloss: 0.11488379201649003, dist: 0.0010428528486653374\n",
            "--------------------------------------------------\n",
            "iter: 1324 \tloss: 0.1233604431263477, dist: 0.0011751054654643015\n",
            "--------------------------------------------------\n",
            "iter: 1325 \tloss: 0.11636064889081453, dist: 0.0010776939657629828\n",
            "--------------------------------------------------\n",
            "iter: 1326 \tloss: 0.124333141123808, dist: 0.0011038770739067343\n",
            "--------------------------------------------------\n",
            "iter: 1327 \tloss: 0.12054487477536814, dist: 0.0011267858130041498\n",
            "--------------------------------------------------\n",
            "iter: 1328 \tloss: 0.12131975574290461, dist: 0.0010960705667157995\n",
            "--------------------------------------------------\n",
            "iter: 1329 \tloss: 0.11772949785809385, dist: 0.00114447086342291\n",
            "--------------------------------------------------\n",
            "iter: 1330 \tloss: 0.12421706504646307, dist: 0.001341819784913259\n",
            "--------------------------------------------------\n",
            "iter: 1331 \tloss: 0.11410292285043039, dist: 0.0011529591044224178\n",
            "--------------------------------------------------\n",
            "iter: 1332 \tloss: 0.11929447851994417, dist: 0.001134629027667746\n",
            "--------------------------------------------------\n",
            "iter: 1333 \tloss: 0.1223244347979529, dist: 0.0012309763462308694\n",
            "--------------------------------------------------\n",
            "iter: 1334 \tloss: 0.11542164389522037, dist: 0.0011154300134153016\n",
            "--------------------------------------------------\n",
            "iter: 1335 \tloss: 0.12265703973294212, dist: 0.0012333208213129326\n",
            "--------------------------------------------------\n",
            "iter: 1336 \tloss: 0.12258306618929944, dist: 0.0012597940388215244\n",
            "--------------------------------------------------\n",
            "iter: 1337 \tloss: 0.12663500613863984, dist: 0.001174643590850642\n",
            "--------------------------------------------------\n",
            "iter: 1338 \tloss: 0.12391599793767942, dist: 0.0012063335800811638\n",
            "--------------------------------------------------\n",
            "iter: 1339 \tloss: 0.11666171973225822, dist: 0.0011711026861410613\n",
            "--------------------------------------------------\n",
            "iter: 1340 \tloss: 0.11531823898015255, dist: 0.0010470875438598896\n",
            "--------------------------------------------------\n",
            "iter: 1341 \tloss: 0.12454745777423536, dist: 0.00107616074911846\n",
            "--------------------------------------------------\n",
            "iter: 1342 \tloss: 0.11725088251133302, dist: 0.0012113393368070747\n",
            "--------------------------------------------------\n",
            "iter: 1343 \tloss: 0.11097300798425729, dist: 0.0010649468756368586\n",
            "--------------------------------------------------\n",
            "iter: 1344 \tloss: 0.11286982860142564, dist: 0.001231358787853072\n",
            "--------------------------------------------------\n",
            "iter: 1345 \tloss: 0.12458608661610499, dist: 0.0011475684334786836\n",
            "--------------------------------------------------\n",
            "iter: 1346 \tloss: 0.11587298422554858, dist: 0.001166698448650783\n",
            "--------------------------------------------------\n",
            "iter: 1347 \tloss: 0.11940514327549488, dist: 0.001091460755968079\n",
            "--------------------------------------------------\n",
            "iter: 1348 \tloss: 0.10438292187809588, dist: 0.0012299806608158373\n",
            "--------------------------------------------------\n",
            "iter: 1349 \tloss: 0.12688451560284222, dist: 0.0012118002803600358\n",
            "--------------------------------------------------\n",
            "iter: 1350 \tloss: 0.12426896001968264, dist: 0.0011438692591319115\n",
            "--------------------------------------------------\n",
            "iter: 1351 \tloss: 0.1188156373255752, dist: 0.0011440522964450172\n",
            "--------------------------------------------------\n",
            "iter: 1352 \tloss: 0.1266483547719, dist: 0.0012530739543026093\n",
            "--------------------------------------------------\n",
            "iter: 1353 \tloss: 0.11498247319237898, dist: 0.0011770959769256727\n",
            "--------------------------------------------------\n",
            "iter: 1354 \tloss: 0.11826972089932747, dist: 0.0010529111602326914\n",
            "--------------------------------------------------\n",
            "iter: 1355 \tloss: 0.11775028602292767, dist: 0.0011714908754151517\n",
            "--------------------------------------------------\n",
            "iter: 1356 \tloss: 0.117979149039201, dist: 0.0011915952132481645\n",
            "--------------------------------------------------\n",
            "iter: 1357 \tloss: 0.1157114742646466, dist: 0.0011223326918207244\n",
            "--------------------------------------------------\n",
            "iter: 1358 \tloss: 0.1200737732167565, dist: 0.001215885060587343\n",
            "--------------------------------------------------\n",
            "iter: 1359 \tloss: 0.11992850529454602, dist: 0.0012563953271625943\n",
            "--------------------------------------------------\n",
            "iter: 1360 \tloss: 0.12182410174472834, dist: 0.001184032348514216\n",
            "--------------------------------------------------\n",
            "iter: 1361 \tloss: 0.12051813142900167, dist: 0.0011777778277550774\n",
            "--------------------------------------------------\n",
            "iter: 1362 \tloss: 0.11927637226529911, dist: 0.0011849630724871075\n",
            "--------------------------------------------------\n",
            "iter: 1363 \tloss: 0.11757380479185647, dist: 0.0012793433244600656\n",
            "--------------------------------------------------\n",
            "iter: 1364 \tloss: 0.11877874768606533, dist: 0.0011011343497228433\n",
            "--------------------------------------------------\n",
            "iter: 1365 \tloss: 0.13103330502472266, dist: 0.0013031371713751391\n",
            "--------------------------------------------------\n",
            "iter: 1366 \tloss: 0.1135783480460309, dist: 0.0010962578473274127\n",
            "--------------------------------------------------\n",
            "iter: 1367 \tloss: 0.12936471862257404, dist: 0.0011171474150587118\n",
            "--------------------------------------------------\n",
            "iter: 1368 \tloss: 0.10526008409312158, dist: 0.0012063161725212974\n",
            "--------------------------------------------------\n",
            "iter: 1369 \tloss: 0.12715174263456755, dist: 0.0012435120480488854\n",
            "--------------------------------------------------\n",
            "iter: 1370 \tloss: 0.13351486013472808, dist: 0.0011801575004123923\n",
            "--------------------------------------------------\n",
            "iter: 1371 \tloss: 0.1101651321613306, dist: 0.00107648013593618\n",
            "--------------------------------------------------\n",
            "iter: 1372 \tloss: 0.11891701474791652, dist: 0.0011901822949106123\n",
            "--------------------------------------------------\n",
            "iter: 1373 \tloss: 0.11683442270548192, dist: 0.0011939536839256247\n",
            "--------------------------------------------------\n",
            "iter: 1374 \tloss: 0.11999390485325788, dist: 0.0010674020979751422\n",
            "--------------------------------------------------\n",
            "iter: 1375 \tloss: 0.11981916434210889, dist: 0.001147696450306766\n",
            "--------------------------------------------------\n",
            "iter: 1376 \tloss: 0.12323287473864414, dist: 0.0011734390757828774\n",
            "--------------------------------------------------\n",
            "iter: 1377 \tloss: 0.1214452421144956, dist: 0.0011823423188005479\n",
            "--------------------------------------------------\n",
            "iter: 1378 \tloss: 0.11403401147055368, dist: 0.0011420159731839164\n",
            "--------------------------------------------------\n",
            "iter: 1379 \tloss: 0.11660024585571231, dist: 0.0011945003707851353\n",
            "--------------------------------------------------\n",
            "iter: 1380 \tloss: 0.11861597117186697, dist: 0.0012177176633169629\n",
            "--------------------------------------------------\n",
            "iter: 1381 \tloss: 0.1196628835404334, dist: 0.0011333360393840776\n",
            "--------------------------------------------------\n",
            "iter: 1382 \tloss: 0.11913735970719604, dist: 0.0012969398905013267\n",
            "--------------------------------------------------\n",
            "iter: 1383 \tloss: 0.12504839677719942, dist: 0.001288946178647214\n",
            "--------------------------------------------------\n",
            "iter: 1384 \tloss: 0.11889904997376367, dist: 0.0012023438591808706\n",
            "--------------------------------------------------\n",
            "iter: 1385 \tloss: 0.12354523304782145, dist: 0.0012229568448009114\n",
            "--------------------------------------------------\n",
            "iter: 1386 \tloss: 0.12146798051414866, dist: 0.0013154815394870504\n",
            "--------------------------------------------------\n",
            "iter: 1387 \tloss: 0.11974523998801873, dist: 0.0011971710707278423\n",
            "--------------------------------------------------\n",
            "iter: 1388 \tloss: 0.11404566537755621, dist: 0.0011871467719077827\n",
            "--------------------------------------------------\n",
            "iter: 1389 \tloss: 0.12198279996410032, dist: 0.00150175398339715\n",
            "--------------------------------------------------\n",
            "iter: 1390 \tloss: 0.13018523487912373, dist: 0.001399450736696449\n",
            "--------------------------------------------------\n",
            "iter: 1391 \tloss: 0.11220934549082727, dist: 0.0012205562828606828\n",
            "--------------------------------------------------\n",
            "iter: 1392 \tloss: 0.11459553550320792, dist: 0.0012270904617927138\n",
            "--------------------------------------------------\n",
            "iter: 1393 \tloss: 0.11272505846135548, dist: 0.001207245560590195\n",
            "--------------------------------------------------\n",
            "iter: 1394 \tloss: 0.11951577417816109, dist: 0.001218783470374512\n",
            "--------------------------------------------------\n",
            "iter: 1395 \tloss: 0.12070909235413828, dist: 0.0012792737727039287\n",
            "--------------------------------------------------\n",
            "iter: 1396 \tloss: 0.11668561328720715, dist: 0.0011541648673851054\n",
            "--------------------------------------------------\n",
            "iter: 1397 \tloss: 0.1079560887863002, dist: 0.0012200515923262737\n",
            "--------------------------------------------------\n",
            "iter: 1398 \tloss: 0.12704383083743262, dist: 0.0011820682514421975\n",
            "--------------------------------------------------\n",
            "iter: 1399 \tloss: 0.11181475299150997, dist: 0.0013113666638184095\n",
            "--------------------------------------------------\n",
            "iter: 1400 \tloss: 0.12299637451512754, dist: 0.0012443110359458805\n",
            "--------------------------------------------------\n",
            "iter: 1401 \tloss: 0.10692722956752121, dist: 0.0012855005786606083\n",
            "--------------------------------------------------\n",
            "iter: 1402 \tloss: 0.12107769873269775, dist: 0.00121416687539962\n",
            "--------------------------------------------------\n",
            "iter: 1403 \tloss: 0.1208552193865246, dist: 0.0013581213747468062\n",
            "--------------------------------------------------\n",
            "iter: 1404 \tloss: 0.1185070693176118, dist: 0.0012253836331296366\n",
            "--------------------------------------------------\n",
            "iter: 1405 \tloss: 0.11780185899115626, dist: 0.0012625204598897327\n",
            "--------------------------------------------------\n",
            "iter: 1406 \tloss: 0.11470600784060224, dist: 0.0012274094312118705\n",
            "--------------------------------------------------\n",
            "iter: 1407 \tloss: 0.11530570490498702, dist: 0.0012448740834885236\n",
            "--------------------------------------------------\n",
            "iter: 1408 \tloss: 0.1257776548577039, dist: 0.0012266754463954204\n",
            "--------------------------------------------------\n",
            "iter: 1409 \tloss: 0.12055735130494316, dist: 0.001188895123394193\n",
            "--------------------------------------------------\n",
            "iter: 1410 \tloss: 0.1218458881603866, dist: 0.0012939901724364443\n",
            "--------------------------------------------------\n",
            "iter: 1411 \tloss: 0.12103078552273386, dist: 0.0012010301138339358\n",
            "--------------------------------------------------\n",
            "iter: 1412 \tloss: 0.11745117583106443, dist: 0.001220314354951594\n",
            "--------------------------------------------------\n",
            "iter: 1413 \tloss: 0.11820180281345388, dist: 0.001207982411524854\n",
            "--------------------------------------------------\n",
            "iter: 1414 \tloss: 0.11217407835202557, dist: 0.0011922223570175771\n",
            "--------------------------------------------------\n",
            "iter: 1415 \tloss: 0.12156617097070552, dist: 0.0013202337835046005\n",
            "--------------------------------------------------\n",
            "iter: 1416 \tloss: 0.11530332611287147, dist: 0.0011523775795945449\n",
            "--------------------------------------------------\n",
            "iter: 1417 \tloss: 0.11710449052377317, dist: 0.0012504763218439219\n",
            "--------------------------------------------------\n",
            "iter: 1418 \tloss: 0.118084683001636, dist: 0.001222510307712167\n",
            "--------------------------------------------------\n",
            "iter: 1419 \tloss: 0.13080735154339582, dist: 0.0014294061300740132\n",
            "--------------------------------------------------\n",
            "iter: 1420 \tloss: 0.1251198589259084, dist: 0.0012728084699325118\n",
            "--------------------------------------------------\n",
            "iter: 1421 \tloss: 0.11922519239806963, dist: 0.0012171942239338725\n",
            "--------------------------------------------------\n",
            "iter: 1422 \tloss: 0.12230560918100454, dist: 0.0014568431659691356\n",
            "--------------------------------------------------\n",
            "iter: 1423 \tloss: 0.12289654032272121, dist: 0.0012323586122641702\n",
            "--------------------------------------------------\n",
            "iter: 1424 \tloss: 0.11189443309325715, dist: 0.0013331412398117739\n",
            "--------------------------------------------------\n",
            "iter: 1425 \tloss: 0.12042608279205752, dist: 0.0013197316664192436\n",
            "--------------------------------------------------\n",
            "iter: 1426 \tloss: 0.1210906637385954, dist: 0.0013848739075247073\n",
            "--------------------------------------------------\n",
            "iter: 1427 \tloss: 0.12021269409576812, dist: 0.0014127198867474131\n",
            "--------------------------------------------------\n",
            "iter: 1428 \tloss: 0.12147711895278762, dist: 0.0013970026717057956\n",
            "--------------------------------------------------\n",
            "iter: 1429 \tloss: 0.11721703277149183, dist: 0.001279005394190436\n",
            "--------------------------------------------------\n",
            "iter: 1430 \tloss: 0.11393080218572958, dist: 0.0013238081323717632\n",
            "--------------------------------------------------\n",
            "iter: 1431 \tloss: 0.11109279520595336, dist: 0.0012245542733328428\n",
            "--------------------------------------------------\n",
            "iter: 1432 \tloss: 0.12313715263499131, dist: 0.0013014180774721612\n",
            "--------------------------------------------------\n",
            "iter: 1433 \tloss: 0.1068159117657539, dist: 0.0012786589496580946\n",
            "--------------------------------------------------\n",
            "iter: 1434 \tloss: 0.11663842403528743, dist: 0.0012858083952524912\n",
            "--------------------------------------------------\n",
            "iter: 1435 \tloss: 0.11874482731650747, dist: 0.0012478652029634532\n",
            "--------------------------------------------------\n",
            "iter: 1436 \tloss: 0.1188958834876701, dist: 0.0013673589100263262\n",
            "--------------------------------------------------\n",
            "iter: 1437 \tloss: 0.12787839643252744, dist: 0.001337597054544625\n",
            "--------------------------------------------------\n",
            "iter: 1438 \tloss: 0.12740648640390767, dist: 0.001211939854284224\n",
            "--------------------------------------------------\n",
            "iter: 1439 \tloss: 0.13021114017507102, dist: 0.0013948137980273516\n",
            "--------------------------------------------------\n",
            "iter: 1440 \tloss: 0.11370578804713372, dist: 0.0013440446701560996\n",
            "--------------------------------------------------\n",
            "iter: 1441 \tloss: 0.11785043073492148, dist: 0.0011637325963384688\n",
            "--------------------------------------------------\n",
            "iter: 1442 \tloss: 0.1253315764812852, dist: 0.0013075940505478617\n",
            "--------------------------------------------------\n",
            "iter: 1443 \tloss: 0.1308687261162708, dist: 0.0013969381991807744\n",
            "--------------------------------------------------\n",
            "iter: 1444 \tloss: 0.11356261789444058, dist: 0.0012258332956817477\n",
            "--------------------------------------------------\n",
            "iter: 1445 \tloss: 0.1256256111714748, dist: 0.0012712481319294936\n",
            "--------------------------------------------------\n",
            "iter: 1446 \tloss: 0.10683299688769687, dist: 0.001338063081871092\n",
            "--------------------------------------------------\n",
            "iter: 1447 \tloss: 0.11531460148177398, dist: 0.0012791948647902349\n",
            "--------------------------------------------------\n",
            "iter: 1448 \tloss: 0.1144109174246097, dist: 0.0011949553618080698\n",
            "--------------------------------------------------\n",
            "iter: 1449 \tloss: 0.12801157623361034, dist: 0.0013031864749169174\n",
            "--------------------------------------------------\n",
            "iter: 1450 \tloss: 0.12382911032713895, dist: 0.00137278984865879\n",
            "--------------------------------------------------\n",
            "iter: 1451 \tloss: 0.1256582237719014, dist: 0.0013013428286049566\n",
            "--------------------------------------------------\n",
            "iter: 1452 \tloss: 0.11692822760053918, dist: 0.0012634968445402403\n",
            "--------------------------------------------------\n",
            "iter: 1453 \tloss: 0.12488295864343132, dist: 0.0013393158250328953\n",
            "--------------------------------------------------\n",
            "iter: 1454 \tloss: 0.12442959960839955, dist: 0.0012829375182337756\n",
            "--------------------------------------------------\n",
            "iter: 1455 \tloss: 0.1157378195832549, dist: 0.0012740998128481476\n",
            "--------------------------------------------------\n",
            "iter: 1456 \tloss: 0.1142718190457726, dist: 0.0012204988768968442\n",
            "--------------------------------------------------\n",
            "iter: 1457 \tloss: 0.13591141901631743, dist: 0.0013619354120659954\n",
            "--------------------------------------------------\n",
            "iter: 1458 \tloss: 0.1106512688574273, dist: 0.0012970261965487325\n",
            "--------------------------------------------------\n",
            "iter: 1459 \tloss: 0.11806493638177008, dist: 0.0011766213439388556\n",
            "--------------------------------------------------\n",
            "iter: 1460 \tloss: 0.11070012938954296, dist: 0.0013756413005694362\n",
            "--------------------------------------------------\n",
            "iter: 1461 \tloss: 0.12389344027906403, dist: 0.0013334275736518178\n",
            "--------------------------------------------------\n",
            "iter: 1462 \tloss: 0.11439052756809467, dist: 0.0011767179515449252\n",
            "--------------------------------------------------\n",
            "iter: 1463 \tloss: 0.11486761719693153, dist: 0.0015726930002444163\n",
            "--------------------------------------------------\n",
            "iter: 1464 \tloss: 0.12203359389509409, dist: 0.0012860930627787562\n",
            "--------------------------------------------------\n",
            "iter: 1465 \tloss: 0.11166350949848769, dist: 0.0012180630435416678\n",
            "--------------------------------------------------\n",
            "iter: 1466 \tloss: 0.1220430903984766, dist: 0.0012803421274981926\n",
            "--------------------------------------------------\n",
            "iter: 1467 \tloss: 0.12578495827235273, dist: 0.0014854565793576103\n",
            "--------------------------------------------------\n",
            "iter: 1468 \tloss: 0.10940500446380322, dist: 0.0012583283821145958\n",
            "--------------------------------------------------\n",
            "iter: 1469 \tloss: 0.12667090270355363, dist: 0.0013522164598408846\n",
            "--------------------------------------------------\n",
            "iter: 1470 \tloss: 0.11435372366773508, dist: 0.0012938483813581404\n",
            "--------------------------------------------------\n",
            "iter: 1471 \tloss: 0.12026786072595173, dist: 0.0013356101361692962\n",
            "--------------------------------------------------\n",
            "iter: 1472 \tloss: 0.11421800408716161, dist: 0.0013667680131982253\n",
            "--------------------------------------------------\n",
            "iter: 1473 \tloss: 0.12032968709192587, dist: 0.0013302771717994572\n",
            "--------------------------------------------------\n",
            "iter: 1474 \tloss: 0.12034854465063641, dist: 0.001335171578341071\n",
            "--------------------------------------------------\n",
            "iter: 1475 \tloss: 0.1186950382754938, dist: 0.0013259781581222941\n",
            "--------------------------------------------------\n",
            "iter: 1476 \tloss: 0.11716711279793873, dist: 0.001397486204225224\n",
            "--------------------------------------------------\n",
            "iter: 1477 \tloss: 0.1305024952516761, dist: 0.001298865047046016\n",
            "--------------------------------------------------\n",
            "iter: 1478 \tloss: 0.12831281476821907, dist: 0.001327578769521867\n",
            "--------------------------------------------------\n",
            "iter: 1479 \tloss: 0.11228223222001862, dist: 0.0012916154228490486\n",
            "--------------------------------------------------\n",
            "iter: 1480 \tloss: 0.11687356207596342, dist: 0.0012710787853769086\n",
            "--------------------------------------------------\n",
            "iter: 1481 \tloss: 0.11040461687443383, dist: 0.0012935841096343075\n",
            "--------------------------------------------------\n",
            "iter: 1482 \tloss: 0.13006177599702703, dist: 0.0014047664714345058\n",
            "--------------------------------------------------\n",
            "iter: 1483 \tloss: 0.11588096053558782, dist: 0.0012557406990457692\n",
            "--------------------------------------------------\n",
            "iter: 1484 \tloss: 0.11284192551555905, dist: 0.0012909026648505349\n",
            "--------------------------------------------------\n",
            "iter: 1485 \tloss: 0.11664484490702538, dist: 0.001321887814754453\n",
            "--------------------------------------------------\n",
            "iter: 1486 \tloss: 0.12289931967844024, dist: 0.0011991154717813015\n",
            "--------------------------------------------------\n",
            "iter: 1487 \tloss: 0.11049849647300379, dist: 0.0012678551821602017\n",
            "--------------------------------------------------\n",
            "iter: 1488 \tloss: 0.11891853494898384, dist: 0.0013795207355268627\n",
            "--------------------------------------------------\n",
            "iter: 1489 \tloss: 0.1234939918037803, dist: 0.001484969118389162\n",
            "--------------------------------------------------\n",
            "iter: 1490 \tloss: 0.11081995512921959, dist: 0.0012713742028722822\n",
            "--------------------------------------------------\n",
            "iter: 1491 \tloss: 0.1373448645053487, dist: 0.001252978475912447\n",
            "--------------------------------------------------\n",
            "iter: 1492 \tloss: 0.11048451852119694, dist: 0.001400455774189945\n",
            "--------------------------------------------------\n",
            "iter: 1493 \tloss: 0.1319703198716509, dist: 0.0013416572898998127\n",
            "--------------------------------------------------\n",
            "iter: 1494 \tloss: 0.11310838181486652, dist: 0.0013536310361846294\n",
            "--------------------------------------------------\n",
            "iter: 1495 \tloss: 0.12377867436667643, dist: 0.0013293034796071704\n",
            "--------------------------------------------------\n",
            "iter: 1496 \tloss: 0.11829234099498512, dist: 0.0013169608945108277\n",
            "--------------------------------------------------\n",
            "iter: 1497 \tloss: 0.11845268268796766, dist: 0.0014001314126329113\n",
            "--------------------------------------------------\n",
            "iter: 1498 \tloss: 0.1150987958383315, dist: 0.0013138908411154955\n",
            "--------------------------------------------------\n",
            "iter: 1499 \tloss: 0.1280801084825053, dist: 0.001324880877861876\n",
            "--------------------------------------------------\n",
            "iter: 1500 \tloss: 0.11585734118774225, dist: 0.001643541348381028\n",
            "--------------------------------------------------\n",
            "iter: 1501 \tloss: 0.12188077762990018, dist: 0.001260441406072521\n",
            "--------------------------------------------------\n",
            "iter: 1502 \tloss: 0.10553113843084398, dist: 0.0013171927282084178\n",
            "--------------------------------------------------\n",
            "iter: 1503 \tloss: 0.1156268282184731, dist: 0.00141851178689148\n",
            "--------------------------------------------------\n",
            "iter: 1504 \tloss: 0.14562333159108487, dist: 0.0014932314125725864\n",
            "--------------------------------------------------\n",
            "iter: 1505 \tloss: 0.12401856531578086, dist: 0.0012865305039914485\n",
            "--------------------------------------------------\n",
            "iter: 1506 \tloss: 0.1413541392555242, dist: 0.0013382229115482174\n",
            "--------------------------------------------------\n",
            "iter: 1507 \tloss: 0.11044512720334819, dist: 0.001364844384359628\n",
            "--------------------------------------------------\n",
            "iter: 1508 \tloss: 0.12802354069728866, dist: 0.0014182301654190902\n",
            "--------------------------------------------------\n",
            "iter: 1509 \tloss: 0.10842216083586992, dist: 0.0012623765655418301\n",
            "--------------------------------------------------\n",
            "iter: 1510 \tloss: 0.12388100524290695, dist: 0.0013626536860667368\n",
            "--------------------------------------------------\n",
            "iter: 1511 \tloss: 0.11769777910702875, dist: 0.0012629968652616704\n",
            "--------------------------------------------------\n",
            "iter: 1512 \tloss: 0.11842894103899061, dist: 0.0013630538943336414\n",
            "--------------------------------------------------\n",
            "iter: 1513 \tloss: 0.11240155035693063, dist: 0.0013348498041626888\n",
            "--------------------------------------------------\n",
            "iter: 1514 \tloss: 0.12829600850503126, dist: 0.0013749165826435364\n",
            "--------------------------------------------------\n",
            "iter: 1515 \tloss: 0.11491161000845117, dist: 0.0013077718590108858\n",
            "--------------------------------------------------\n",
            "iter: 1516 \tloss: 0.1266720978596924, dist: 0.0013969024553911718\n",
            "--------------------------------------------------\n",
            "iter: 1517 \tloss: 0.12213869309623909, dist: 0.0015034061570597913\n",
            "--------------------------------------------------\n",
            "iter: 1518 \tloss: 0.11578084272706719, dist: 0.001320330914586393\n",
            "--------------------------------------------------\n",
            "iter: 1519 \tloss: 0.11828056468069904, dist: 0.0013543554055130034\n",
            "--------------------------------------------------\n",
            "iter: 1520 \tloss: 0.11836106546901518, dist: 0.0013755630931593704\n",
            "--------------------------------------------------\n",
            "iter: 1521 \tloss: 0.11415042834009918, dist: 0.0012964735630492544\n",
            "--------------------------------------------------\n",
            "iter: 1522 \tloss: 0.12410288910900982, dist: 0.0014069213643392118\n",
            "--------------------------------------------------\n",
            "iter: 1523 \tloss: 0.11288103903902838, dist: 0.0013690968868215981\n",
            "--------------------------------------------------\n",
            "iter: 1524 \tloss: 0.10718354208343656, dist: 0.0012814597641055707\n",
            "--------------------------------------------------\n",
            "iter: 1525 \tloss: 0.1300118598700599, dist: 0.0015202478862905019\n",
            "--------------------------------------------------\n",
            "iter: 1526 \tloss: 0.11345235913776376, dist: 0.0013668698917928305\n",
            "--------------------------------------------------\n",
            "iter: 1527 \tloss: 0.11086740941039336, dist: 0.0013614423393004049\n",
            "--------------------------------------------------\n",
            "iter: 1528 \tloss: 0.1229362685117002, dist: 0.001379307597166345\n",
            "--------------------------------------------------\n",
            "iter: 1529 \tloss: 0.11604494956533724, dist: 0.0014187588846251674\n",
            "--------------------------------------------------\n",
            "iter: 1530 \tloss: 0.11183223043031411, dist: 0.0013576803190810502\n",
            "--------------------------------------------------\n",
            "iter: 1531 \tloss: 0.11872646611826904, dist: 0.0014361520937649212\n",
            "--------------------------------------------------\n",
            "iter: 1532 \tloss: 0.11677893231249756, dist: 0.0013239163407863773\n",
            "--------------------------------------------------\n",
            "iter: 1533 \tloss: 0.11040812471421443, dist: 0.001369990855964707\n",
            "--------------------------------------------------\n",
            "iter: 1534 \tloss: 0.12881290200435125, dist: 0.0014077739681109145\n",
            "--------------------------------------------------\n",
            "iter: 1535 \tloss: 0.13822544129603, dist: 0.0014462504355186008\n",
            "--------------------------------------------------\n",
            "iter: 1536 \tloss: 0.10986566712648395, dist: 0.0013739258160454502\n",
            "--------------------------------------------------\n",
            "iter: 1537 \tloss: 0.120959019714179, dist: 0.0013770050346031228\n",
            "--------------------------------------------------\n",
            "iter: 1538 \tloss: 0.12101245756193008, dist: 0.0013899493313199495\n",
            "--------------------------------------------------\n",
            "iter: 1539 \tloss: 0.11765277484441337, dist: 0.0012765121714909406\n",
            "--------------------------------------------------\n",
            "iter: 1540 \tloss: 0.12635254440424493, dist: 0.0014262150353249715\n",
            "--------------------------------------------------\n",
            "iter: 1541 \tloss: 0.12001318673099824, dist: 0.0014715384816808424\n",
            "--------------------------------------------------\n",
            "iter: 1542 \tloss: 0.11996473891473522, dist: 0.0013753459260445527\n",
            "--------------------------------------------------\n",
            "iter: 1543 \tloss: 0.11997983365540174, dist: 0.0014023640687114277\n",
            "--------------------------------------------------\n",
            "iter: 1544 \tloss: 0.11961439859493499, dist: 0.001385774605761268\n",
            "--------------------------------------------------\n",
            "iter: 1545 \tloss: 0.11869167286997008, dist: 0.0013831460783286175\n",
            "--------------------------------------------------\n",
            "iter: 1546 \tloss: 0.11820291461511469, dist: 0.0013614739986878331\n",
            "--------------------------------------------------\n",
            "iter: 1547 \tloss: 0.10860721053492019, dist: 0.0013906936708975271\n",
            "--------------------------------------------------\n",
            "iter: 1548 \tloss: 0.11641166527749312, dist: 0.0013547458868373747\n",
            "--------------------------------------------------\n",
            "iter: 1549 \tloss: 0.11704220438416411, dist: 0.0013640407139564623\n",
            "--------------------------------------------------\n",
            "iter: 1550 \tloss: 0.11558269908001333, dist: 0.0015215631386094588\n",
            "--------------------------------------------------\n",
            "iter: 1551 \tloss: 0.12277960211610396, dist: 0.0013979974263836506\n",
            "--------------------------------------------------\n",
            "iter: 1552 \tloss: 0.1131892774000082, dist: 0.0014914680263517068\n",
            "--------------------------------------------------\n",
            "iter: 1553 \tloss: 0.12707648226653886, dist: 0.0014378724218105062\n",
            "--------------------------------------------------\n",
            "iter: 1554 \tloss: 0.110981692396556, dist: 0.0013049291420327365\n",
            "--------------------------------------------------\n",
            "iter: 1555 \tloss: 0.13161577441754552, dist: 0.0013422229784757272\n",
            "--------------------------------------------------\n",
            "iter: 1556 \tloss: 0.11249347424863777, dist: 0.0013866704940496486\n",
            "--------------------------------------------------\n",
            "iter: 1557 \tloss: 0.11494180987779533, dist: 0.0014424914436501326\n",
            "--------------------------------------------------\n",
            "iter: 1558 \tloss: 0.11907249475180309, dist: 0.0013558118843815302\n",
            "--------------------------------------------------\n",
            "iter: 1559 \tloss: 0.1160598975011118, dist: 0.0013998078497893076\n",
            "--------------------------------------------------\n",
            "iter: 1560 \tloss: 0.11639632920885347, dist: 0.001423094343835108\n",
            "--------------------------------------------------\n",
            "iter: 1561 \tloss: 0.12425016065336783, dist: 0.0014103945790118213\n",
            "--------------------------------------------------\n",
            "iter: 1562 \tloss: 0.11735401503784683, dist: 0.0015252260256814678\n",
            "--------------------------------------------------\n",
            "iter: 1563 \tloss: 0.12102882119812745, dist: 0.0013890387350916896\n",
            "--------------------------------------------------\n",
            "iter: 1564 \tloss: 0.1209161245541161, dist: 0.001324454267247335\n",
            "--------------------------------------------------\n",
            "iter: 1565 \tloss: 0.11714353822365214, dist: 0.0013860666404968446\n",
            "--------------------------------------------------\n",
            "iter: 1566 \tloss: 0.11624225560166851, dist: 0.001430454648010941\n",
            "--------------------------------------------------\n",
            "iter: 1567 \tloss: 0.12201975401063157, dist: 0.001452024929282371\n",
            "--------------------------------------------------\n",
            "iter: 1568 \tloss: 0.11739034216446119, dist: 0.0014048033589707076\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "zxOCDR9cdtEN",
        "outputId": "93f76d22-1807-4e16-d297-4d82c39409bc"
      },
      "source": [
        "def plot_classifier_neoplastic(classifier):\n",
        "    f, (ax1) = plt.subplots(1, 1, figsize=(7, 7))\n",
        "    plt.xticks(fontsize=20)\n",
        "    plt.yticks(fontsize=20)\n",
        "    im1 = ax1.imshow(classifier, cmap=\"jet\")\n",
        "    ax1.invert_yaxis()\n",
        "    divider1 = make_axes_locatable(ax1)\n",
        "    cax1 = divider1.append_axes(\"right\", size=\"3%\", pad=0.2)\n",
        "    cbar1 = plt.colorbar(im1, cax=cax1, format=\"%.1f\")\n",
        "    cbar1.ax.tick_params(labelsize=20)\n",
        "    plt.show()\n",
        "\n",
        "def compute_scaled_kde_neoplastic(lattice_size, locs_arr):\n",
        "    x = np.arange(0, lattice_size[0])\n",
        "    y = np.arange(0, lattice_size[1])\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    positions = np.vstack([Y.ravel(), X.ravel()]).T\n",
        "    kernel_normal = stats.gaussian_kde(locs_arr.T)\n",
        "    Z = np.reshape(kernel_normal([positions[:, 0], positions[:, 1]]), X.shape)\n",
        "    Z_scaled = (Z - Z.min()) / (Z.max() - Z.min())\n",
        "    plot_classifier_neoplastic(Z_scaled)\n",
        "    return Z, Z_scaled\n",
        "\n",
        "#Obtain locations of samples and plot as a heatmap\n",
        "locs = deep_map.get_locations(X_train.astype('float32')) \n",
        "_ = compute_scaled_kde_neoplastic(lattice_size, np.array(locs))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "OjPpTFnsrsV7",
        "outputId": "866d8f58-4f4f-42a8-d653-9052082a0f31"
      },
      "source": [
        "def find_largest_activation(i, j, locs, labs):\n",
        "  arr = [0] * len(set(labs))\n",
        "  for l in range(len(locs)):\n",
        "    x, y = locs[l]\n",
        "    if x == i and y == j:\n",
        "      arr[labs[l]] = arr[labs[l]] + 1\n",
        "  num = np.argwhere(arr == np.amax(arr))\n",
        "  \n",
        "  if len(num) > 1:\n",
        "    return \"Blank\", 0\n",
        "  if np.sum(arr) == (np.max(arr)):\n",
        "    return np.argmax(arr), np.max(arr)\n",
        "  else:\n",
        "    return np.argmax(arr), np.max(arr)/(np.sum(arr))\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "def labelled_plot(lattice_size, locs, labels, cols):\n",
        "  for i in range(lattice_size[0]):\n",
        "    for j in range(lattice_size[1]):\n",
        "      win, num = find_largest_activation(i, j, locs, labels)\n",
        "      if win != 'Blank':\n",
        "        plt.figure(1, figsize=(7, 7))\n",
        "        plt.scatter(j, i, s=num*65, color=cols[win])\n",
        "\n",
        "  plt.xticks(np.arange(0,lattice_size[0],2),fontsize=20)\n",
        "  plt.yticks(np.arange(0,lattice_size[1],2),fontsize=20)\n",
        "  plt.show()\n",
        "\n",
        "cols = ['purple', 'aqua']\n",
        "labelled_plot(lattice_size, locs, y_train, cols)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZA0hfBQsWrZ"
      },
      "source": [
        "#Feature importance - can be done for entire network (all_types), or each different class (each_type)\n",
        "#Method explained in Exploring microRNA Regulation of Cancer with Context-Aware Deep Cancer Classifier (Pyman et al. 2018)\n",
        "\n",
        "def feat_vis(X_train): #First find output from AE, then backpropagate output through AE encoder layers to find activation of each input\n",
        "  AE= miRNA_AE(len(X_train[0]), latent_size)\n",
        "  ae_weights = np.array(deep_map.ae_weights)\n",
        "  encoder_output, encoder_params, enc_to_regs = AE._encode(X_train.astype(\"float32\"), ae_weights)\n",
        "  all_summed_grads = []\n",
        "  for i in range(len(X_train)):\n",
        "    y = sigmoid(encoder_output[i])\n",
        "    dy = T.jacobian(y,encoder_params)\n",
        "    get_feat_vis = theano.function(inputs=[], outputs=dy)\n",
        "    grads = get_feat_vis()[0]\n",
        "    summed_grads = np.sum(np.abs(grads[0]), axis=1)\n",
        "    all_summed_grads.append(summed_grads)\n",
        "  final_sum = np.sum(all_summed_grads, axis=0)\n",
        "  return final_sum\n",
        "\n",
        "grad = feat_vis(X_train)\n",
        "#Outputs activation of each miRNA for all samples\n",
        "#Can input certain samples (ie, those of a single class) or individual samples for further analysis"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}