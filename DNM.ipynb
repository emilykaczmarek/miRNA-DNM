{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZICuCyEFKd9y",
        "outputId": "e456f710-e2fd-4f01-94b2-a529bb0d192f"
      },
      "source": [
        "import os\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(3)\n",
        "from keras.datasets import mnist\n",
        "from tempfile import TemporaryFile\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def load_mnist():\n",
        "    (x, _), (_,_) = mnist.load_data()\n",
        "    x = x / 255.\n",
        "    return x.reshape((-1, 1, 28, 28))\n",
        "\n",
        "def save_projection_image(memory, lattice_size, iter):\n",
        "    width = memory.shape[2]\n",
        "    height = memory.shape[1]\n",
        "    proj_map = np.zeros((lattice_size[0] * height, lattice_size[1] * width))\n",
        "    c = 0\n",
        "    for i in range(lattice_size[0]):\n",
        "        for j in range(lattice_size[1]):\n",
        "            patch = memory[c]\n",
        "            proj_map[\n",
        "                i * height:(i + 1) * height, j * width:(j + 1) * width] = patch \n",
        "            c += 1\n",
        "    plt.imshow(255*proj_map)\n",
        "    plt.show()\n",
        "    return proj_map\n",
        "\n",
        "\n",
        "class Adam(object):\n",
        "    def __init__(self, lr=1e-2, b1=9e-1, b2=0.999, e=1e-8, gamma=1 - 1e-8):\n",
        "        self.lr = lr\n",
        "        self.b1 = b1\n",
        "        self.b2 = b2\n",
        "        self.e = e\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def updates(self, cost, params):\n",
        "\n",
        "        updates = []\n",
        "        grads = theano.grad(cost, params)\n",
        "        alpha = self.lr\n",
        "        t = theano.shared(np.float32(1))\n",
        "        b1_t = self.b1 * self.gamma ** (t - 1)  # (Decay the first moment running average coefficient)\n",
        "\n",
        "        for param, grad in zip(params, grads):\n",
        "            m_previous = theano.shared(np.zeros(param.get_value().shape,\n",
        "                                                dtype=theano.config.floatX))\n",
        "            v_previous = theano.shared(np.zeros(param.get_value().shape,\n",
        "                                                dtype=theano.config.floatX))\n",
        "\n",
        "            m = b1_t * m_previous + (1 - b1_t) * grad  # (Update biased first moment estimate)\n",
        "            v = self.b2 * v_previous + (1 - self.b2) * grad ** 2  # (Update biased second raw moment estimate)\n",
        "            m_hat = 1.*m / (1 - self.b1 ** t)  # (Compute bias-corrected first moment estimate)\n",
        "            v_hat = 1.*v / (1 - self.b2 ** t)  # (Compute bias-corrected second raw moment estimate)\n",
        "            theta = param - (alpha * m_hat) / (T.sqrt(v_hat) + self.e)  # (Update parameters)\n",
        "\n",
        "            updates.append((m_previous, m))\n",
        "            updates.append((v_previous, v))\n",
        "            updates.append((param, theta))\n",
        "        updates.append((t, t + 1.))\n",
        "        return updates\n",
        "\n",
        "class BatchFactory(object):\n",
        "    def __init__(self, batch_size, nb_samples, iterations, randomizer=True):\n",
        "        self.BATCH_SIZE = batch_size\n",
        "        self.nb_samples = nb_samples \n",
        "        self.iterations = iterations \n",
        "        self.randomizer = randomizer\n",
        "\n",
        "    def _index_generator(self):\n",
        "        for i in range(self.iterations):\n",
        "            if self.randomizer:\n",
        "                indices = np.random.permutation(self.nb_samples)\n",
        "            else:\n",
        "                indices = np.arange(self.nb_samples)\n",
        "            for indexer in np.arange(self.nb_samples/self.BATCH_SIZE):\n",
        "                indexer = int(indexer)\n",
        "                yield indices[slice(indexer*self.BATCH_SIZE, (indexer+1)*self.BATCH_SIZE)]\n",
        "            if self.nb_samples % self.BATCH_SIZE != 0:\n",
        "                indexer = int(self.nb_samples / self.BATCH_SIZE)\n",
        "                yield indices[slice(indexer*self.BATCH_SIZE, self.nb_samples)]\n",
        "\n",
        "    def generate_batch(self, X, Y=None):\n",
        "        Y = None\n",
        "        for samples in self._index_generator():\n",
        "            if Y is None:\n",
        "                yield X[samples]\n",
        "            else:\n",
        "                yield X[samples], Y[samples]\n",
        "\n",
        "class activations(object):\n",
        "  def sigmoid(x):\n",
        "    return T.nnet.sigmoid(x)\n",
        "\n",
        "  def relu(x):\n",
        "      return T.maximum(0.0, x)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return T.nnet.sigmoid(x)\n",
        "\n",
        "class Ops(object):\n",
        "\n",
        "  def dense(inpt, nb_in, nb_out, layer_name='', init_params=None):\n",
        "      if init_params is None:\n",
        "          w = theano.shared(\n",
        "              np.asarray(\n",
        "                  np.random.normal(\n",
        "                      loc=0, scale=np.sqrt(1. / nb_out), size=[nb_in, nb_out]),\n",
        "                  dtype=theano.config.floatX),\n",
        "              name='w_dense_' + layer_name, borrow=True)\n",
        "          b = theano.shared(np.asarray(np.random.normal(\n",
        "              loc=0.0, scale=1.0, size=[nb_out]),\n",
        "              dtype=theano.config.floatX),\n",
        "              name='b_dense_' + layer_name, borrow=True)\n",
        "      else:\n",
        "          w = init_params[0]\n",
        "          b = init_params[1]\n",
        "      return T.dot(inpt, w) + b, [w, b]\n",
        "\n",
        "\n",
        "  def conv_2d_transpose(inpt, conv2d_params, stride=(1, 1), layer_name='', mode='valid', init_params=None):\n",
        "    output_channel, input_channel, rows, columns = conv2d_params\n",
        "    if init_params is None:\n",
        "        filter_shape = (output_channel, input_channel, rows, columns)\n",
        "        output_shape = (output_channel, input_channel, rows + 2, columns + 2)\n",
        "        receptive_field_size = rows * columns\n",
        "\n",
        "        w = theano.shared(np.asarray(\n",
        "            np.random.normal(\n",
        "                loc=0, scale=np.sqrt(2. / ((input_channel + output_channel) * receptive_field_size)),\n",
        "            size=filter_shape),\n",
        "            dtype=theano.config.floatX), name='w_conv2d_' + layer_name, borrow=True)\n",
        "\n",
        "        b = theano.shared(\n",
        "            np.asarray(\n",
        "                np.random.normal(\n",
        "                    loc=0.0, scale=1.0, size=(\n",
        "                        filter_shape[0],)), dtype=theano.config.floatX),\n",
        "            name='b_conv2d_' + layer_name, borrow=True)\n",
        "    else:\n",
        "        w = init_params[0]\n",
        "        b = init_params[1]\n",
        "    return T.nnet.conv2d_transpose(input=inpt, filters=w, output_shape=())\n",
        "\n",
        "\n",
        "  def conv_2d(inpt, conv2d_params, stride=(1, 1), layer_name='', mode='valid', init_params=None):\n",
        "      output_channel, input_channel, rows, columns = conv2d_params\n",
        "      if init_params is None:\n",
        "          filter_shape = (output_channel, input_channel, rows, columns)\n",
        "          receptive_field_size = rows * columns\n",
        "\n",
        "          w = theano.shared(np.asarray(\n",
        "              np.random.normal(\n",
        "                  loc=0, scale=np.sqrt(2. / ((input_channel + output_channel) * receptive_field_size)),\n",
        "              size=filter_shape),\n",
        "              dtype=theano.config.floatX), name='w_conv2d_' + layer_name, borrow=True)\n",
        "\n",
        "          b = theano.shared(\n",
        "              np.asarray(\n",
        "                  np.random.normal(\n",
        "                      loc=0.0, scale=1.0, size=(\n",
        "                          filter_shape[0],)), dtype=theano.config.floatX),\n",
        "              name='b_conv2d_' + layer_name, borrow=True)\n",
        "      else:\n",
        "          w = init_params[0]\n",
        "          b = init_params[1]\n",
        "      return T.nnet.conv2d(input=inpt, filters=w, border_mode=mode, subsample=stride) + b.dimshuffle('x', 0, 'x', 'x'), [w, b]\n",
        "\n",
        "\n",
        "  def flatten(inpt, ndim=2):\n",
        "    return T.flatten(inpt, ndim)\n",
        "\n",
        "\n",
        "def get_params(layer_name, par_list):\n",
        "  if par_list is None:\n",
        "    return None\n",
        "  pars = []\n",
        "  for i in par_list:\n",
        "    if i.name.split('_')[-1] == layer_name:\n",
        "      pars.append(i)\n",
        "  return None if len(pars) == 0 else pars\n",
        "\n",
        "class SIMPLE_AE(object):\n",
        "\n",
        "\n",
        "    def __init__(self, input_size, latent_size):\n",
        "        self.input_size = input_size\n",
        "        self.latent_size = latent_size\n",
        "\n",
        "    def _encode(self, X, init_params):\n",
        "        params = []\n",
        "        regs = []\n",
        "        e1, pars = Ops.conv_2d(X, (10, 1, 5, 5),\n",
        "                               layer_name='e1',\n",
        "                               mode='half',\n",
        "                               init_params=get_params('e1', init_params))\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        e2, pars = Ops.conv_2d(activations.relu(e1),\n",
        "                               (8, 10, 5, 5),\n",
        "                               layer_name='e2',\n",
        "                               mode='half',\n",
        "                               init_params=get_params('e2', init_params))\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        e3, pars = Ops.conv_2d(activations.relu(e2),\n",
        "                               (5, 8, 5, 5),\n",
        "                               layer_name='e3',\n",
        "                               mode='half',\n",
        "                               init_params=get_params('e3', init_params))\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        e4, pars = Ops.dense(Ops.flatten(activations.relu(e3)),\n",
        "                             self.input_size[0] * self.input_size[1] * 5,\n",
        "                             self.latent_size,\n",
        "                             layer_name='e4',\n",
        "                             init_params=get_params('e4', init_params))\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        return e4, params, regs\n",
        "\n",
        "    def _decode(self, X, init_params):\n",
        "        params = []\n",
        "        regs = []\n",
        "        d4, pars = Ops.dense(\n",
        "            X, self.latent_size, self.input_size[0] * self.input_size[1] * 5,\n",
        "            layer_name='d4',\n",
        "            init_params=get_params('d4', init_params))\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        d4 = T.reshape((d4),\n",
        "                       (-1, 5, self.input_size[0], self.input_size[1]))\n",
        "\n",
        "        d3, pars = Ops.conv_2d(activations.relu(d4),\n",
        "                               (8, 5, 5, 5),\n",
        "                               layer_name='d3',\n",
        "                               mode='half',\n",
        "                               init_params=get_params('d3', init_params))\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        d2, pars = Ops.conv_2d(activations.relu(d3),\n",
        "                               (10, 8, 5, 5),\n",
        "                               layer_name='d2',\n",
        "                               mode='half',\n",
        "                               init_params=get_params('d2', init_params))\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        d1, pars = Ops.conv_2d(activations.relu(d2),\n",
        "                               (1, 10, 5, 5),\n",
        "                               layer_name='d1',\n",
        "                               mode='half',\n",
        "                               init_params=get_params('d1', init_params))\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        return activations.sigmoid(d1), params, regs\n",
        "\n",
        "\n",
        "class miRNA_AE(object): #Autoencoder for 1D data\n",
        "\n",
        "    def __init__(self, input_size, latent_size):\n",
        "        ae_layers = 3.0\n",
        "        self.input_size = input_size\n",
        "        self.latent_size = latent_size\n",
        "        layer_list = []\n",
        "        layer_list.append(self.input_size)\n",
        "\n",
        "        #Arrange autoencoder steps in geometric series\n",
        "        divis = pow((self.input_size / self.latent_size), (1 / ae_layers))\n",
        "        for i in range(2):\n",
        "            layer_list.append(int(layer_list[-1] / divis))\n",
        "        self.layer_list = layer_list\n",
        "\n",
        "    def get_params(layer_name, par_list):\n",
        "      if par_list is None:\n",
        "        return None\n",
        "      pars = []\n",
        "      for i in par_list:\n",
        "        if i.name.split('_')[-1] == layer_name:\n",
        "            pars.append(i)\n",
        "      return None if len(pars) == 0 else pars\n",
        "\n",
        "\n",
        "    def _encode(self, X, init_params):\n",
        "\n",
        "        params = []\n",
        "        regs = []\n",
        "        e1, pars = Ops.dense(X, self.input_size, self.layer_list[1],\n",
        "                               layer_name='e1',\n",
        "                               init_params=get_params('e1', init_params))\n",
        "\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        e2, pars = Ops.dense(activations.relu(e1),\n",
        "                             self.layer_list[1],\n",
        "                             self.layer_list[2],\n",
        "                             layer_name='e2',\n",
        "                             init_params=get_params('e2', init_params))\n",
        "\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        e3, pars = Ops.dense(activations.relu(e2),\n",
        "                             self.layer_list[2],\n",
        "                             self.latent_size,\n",
        "                             layer_name='e3',\n",
        "                             init_params=get_params('e3', init_params))\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        return e3, params, regs\n",
        "\n",
        "    def _decode(self, X, init_params):\n",
        "\n",
        "        params = []\n",
        "        regs = []\n",
        "        d3, pars = Ops.dense(\n",
        "            X, self.latent_size, self.layer_list[2],\n",
        "            layer_name='d3',\n",
        "            init_params=get_params('d3', init_params))\n",
        "\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        d2, pars = Ops.dense(activations.relu(d3),\n",
        "            self.layer_list[2], self.layer_list[1],\n",
        "            layer_name='d2',\n",
        "            init_params=get_params('d2', init_params))\n",
        "\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        d1, pars = Ops.dense(activations.relu(d2),\n",
        "                             self.layer_list[1], self.input_size,\n",
        "                             layer_name='d1',\n",
        "                             init_params=get_params('d1', init_params))\n",
        "\n",
        "        params += pars\n",
        "        regs.append(pars[0])\n",
        "        return sigmoid(d1), params, regs\n",
        "\n",
        "\n",
        "class DNM(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_image_size,\n",
        "                 latent_size,\n",
        "                 lattice_size,\n",
        "                 ae_arch_class,\n",
        "                 name,\n",
        "                 init_params=None,\n",
        "                 sigma=None,\n",
        "                 alpha=None,\n",
        "                 BATCH_SIZE=64,\n",
        "                 ae_lr=1e-3,\n",
        "                 lmbd=1e-6,\n",
        "                 som_pretrain_lr=0.0005,\n",
        "                 dnm_map_lr=0.05):\n",
        "\n",
        "        self.lattice_size = lattice_size\n",
        "        self.name = name\n",
        "        self.latent_size = latent_size\n",
        "        self.sigma = sigma if sigma is not None else max(self.lattice_size[0],\n",
        "                                                         self.lattice_size[1]) / 2.\n",
        "        self.alpha = alpha if alpha is not None else 0.3\n",
        "        self.input_image_size = input_image_size\n",
        "        self.ae_lr = ae_lr\n",
        "        self.lmbd = lmbd\n",
        "        self.ae_arch_obj = ae_arch_class(self.input_image_size,\n",
        "                                         self.latent_size)\n",
        "        self.BATCH_SIZE = BATCH_SIZE\n",
        "        self.som_lr = som_pretrain_lr\n",
        "        self.dnm_map_lr = dnm_map_lr\n",
        "        self.ae_weights = None\n",
        "        if init_params is not None:\n",
        "            self.ae_weights = init_params[0]\n",
        "        if init_params is not None:\n",
        "            self.som_weights = init_params[1]\n",
        "        else:\n",
        "            self.som_weights = theano.shared(np.random.normal(0, 1,\n",
        "                                                              (self.lattice_size[0] *\n",
        "                                                               self.lattice_size[\n",
        "                                                                   1],\n",
        "                                                               self.latent_size), ))\n",
        "        if not os.path.exists('ckpts'):\n",
        "            os.mkdir('ckpts')\n",
        "        self._build()\n",
        "\n",
        "    def __soft_probs(self, sample, clusters):\n",
        "        alpha = 1.0\n",
        "        q = 1.0 / \\\n",
        "            (1.0 + (T.sum(T.square((sample - clusters)), axis=1) / alpha))\n",
        "        q **= (alpha + 1.0) / 2.0\n",
        "        q = T.transpose(T.transpose(q) / T.sum(q, axis=0))\n",
        "        return q\n",
        "\n",
        "    def __kld(self, p, q):\n",
        "        return T.sum(p * T.log(p / q), axis=-1)\n",
        "\n",
        "    def __SOM(self, X, W, n, update_flag):\n",
        "\n",
        "        learning_rate_op = T.exp(-1. * self.som_lr * n)\n",
        "        _alpha_op = self.alpha * learning_rate_op\n",
        "        _sigma_op = self.sigma * learning_rate_op\n",
        "\n",
        "        locations = self.locs\n",
        "        maps = T.sub(X, W)\n",
        "        measure = T.sum(T.pow(T.sub(X, W), 2), axis=1)\n",
        "        err = measure.min()\n",
        "        self.bmu_index = T.argmin(measure)\n",
        "        bmu_loc = locations[self.bmu_index]\n",
        "        dist_square = T.sum(\n",
        "            T.square(T.sub(locations, bmu_loc)), axis=1)\n",
        "        H = T.cast(T.exp(-dist_square / (2 * T.square(_sigma_op))),\n",
        "                   dtype=theano.config.floatX)\n",
        "        w_update = W + _alpha_op * \\\n",
        "            T.tile(H, [self.latent_size, 1]).T * maps\n",
        "        Qs = self.__soft_probs(X, W)\n",
        "        P = Qs ** 2 / Qs.sum()\n",
        "        P = (P.T / P.sum()).T\n",
        "        cost = self.__kld(P, Qs)\n",
        "        return [err, cost, bmu_loc], {W: update_flag * w_update + (1 - update_flag) * W}\n",
        "\n",
        "    def _build_locs(self, m, n):\n",
        "        for i in range(m):\n",
        "            for j in range(n):\n",
        "                yield(np.array([i, j]))\n",
        "\n",
        "    def _build(self):\n",
        "        # Tensors\n",
        "        n = T.scalar()\n",
        "        som_update_flag = T.scalar()\n",
        "        #self.X = T.TensorType(dtype=theano.config.floatX, broadcastable=(False,) * 4)('X')\n",
        "        self.X = T.TensorType(\n",
        "            dtype=theano.config.floatX, broadcastable=(False,) * 2)('X')  #changed for 1D data\n",
        "\n",
        "        # Architecture of the AE\n",
        "        encoder_output, encoder_params, enc_to_regs = self.ae_arch_obj._encode(\n",
        "            self.X, self.ae_weights)\n",
        "        encoder_output_sigmoid = sigmoid(encoder_output)\n",
        "        reconstructed, decoder_params, dec_to_regs = self.ae_arch_obj._decode(\n",
        "            encoder_output_sigmoid, self.ae_weights)\n",
        "\n",
        "        # Architecture of the SOM and updates\n",
        "        self.locs = T.constant(\n",
        "            [i for i in self._build_locs(self.lattice_size[0], self.lattice_size[1])])\n",
        "        [self.__SOM_batch_errs, som_kl_cost, bmus], self.__SOM_updates = theano.scan(\n",
        "            sequences=encoder_output_sigmoid,\n",
        "            non_sequences=[self.som_weights, n, som_update_flag],\n",
        "            fn=self.__SOM)\n",
        "\n",
        "        # Loss of DNM, SOM and AE\n",
        "        ae_optimizer = Adam(lr=self.ae_lr)\n",
        "        self.ae_weights = encoder_params + decoder_params\n",
        "        to_regs = enc_to_regs + dec_to_regs\n",
        "        self.map_err = T.mean(som_kl_cost)\n",
        "        #self.rec_cost = T.mean(\n",
        "        #    T.sum(T.nnet.binary_crossentropy(reconstructed, self.X), axis=1))\n",
        "        self.rec_cost = T.mean(T.sum((reconstructed - self.X) ** 2, axis=1)) #changed for 1D data\n",
        "        self.ae_cost = self.rec_cost + self.lmbd * \\\n",
        "            np.array([T.sum(i) ** 2 for i in to_regs]).sum()\n",
        "\n",
        "        self.dnm_cost = self.ae_cost + self.map_err\n",
        "\n",
        "        dnm_updates = ae_optimizer.updates(cost=self.dnm_cost,\n",
        "                                           params=self.ae_weights)\n",
        "\n",
        "        ae_updates = ae_optimizer.updates(cost=self.ae_cost,\n",
        "                                          params=self.ae_weights)\n",
        "        # Train ops\n",
        "        self.__train_op_dnm = theano.function([self.X, n, som_update_flag],\n",
        "                                              outputs=[self.dnm_cost, T.mean(\n",
        "                                                  self.__SOM_batch_errs)],\n",
        "                                              updates=dnm_updates)\n",
        "        self.__train_op_ae = theano.function([self.X],\n",
        "                                             outputs=self.ae_cost,\n",
        "                                             updates=ae_updates)\n",
        "        self.__train_op_som = theano.function([self.X, n, som_update_flag],\n",
        "                                              outputs=[self.__SOM_batch_errs, bmus],\n",
        "                                              updates=self.__SOM_updates)\n",
        "\n",
        "        self.__get_ae_output = theano.function([self.X],\n",
        "                                               encoder_output_sigmoid)\n",
        "        self.__get_reconstructed = theano.function([self.X],\n",
        "                                                   reconstructed)\n",
        "        self.__decode_embedding = theano.function([encoder_output_sigmoid],\n",
        "                                                  reconstructed)\n",
        "        self.__get_backprojection = theano.function([],\n",
        "                                                     outputs=reconstructed,\n",
        "                                                     givens={encoder_output_sigmoid: self.som_weights.eval().astype('float64')})\n",
        "\n",
        "        self._get_bmu_locs = theano.function(\n",
        "            [self.X, n, som_update_flag], bmus)\n",
        "\n",
        "    def get_bmu_array(self, X):\n",
        "        res = []\n",
        "        weight_array = self.som_weights.get_value()\n",
        "        for i in X:\n",
        "            measure = np.sum((i - weight_array) ** 2, axis=1)\n",
        "            bmu_index = np.argmin(measure)\n",
        "            res.append(weight_array[bmu_index])\n",
        "        return np.array(res)\n",
        "\n",
        "    def train(self, x_train, \n",
        "              dnm_epochs, trial_name, name, location_name,\n",
        "              pre_train_epochs=None):\n",
        "\n",
        "        if pre_train_epochs is not None:\n",
        "            self._pretrain(x_train, pre_train_epochs[\n",
        "                           0], pre_train_epochs[1], trial_name, name, location_name, self.BATCH_SIZE)\n",
        "            \n",
        "        hold_inter = TemporaryFile() #save weights (deletes at the end of session)\n",
        "        hold_int = [self.ae_weights, self.som_weights]\n",
        "        np.save(\"hold_inter2.npy\", hold_int)\n",
        "        np.save(hold_inter, hold_int)\n",
        "\n",
        "\n",
        "        self.som_lr = self.dnm_map_lr\n",
        "        print('\\nTraining DNM')\n",
        "        print(50 * '=')\n",
        "        B = BatchFactory(self.BATCH_SIZE, len(x_train), dnm_epochs)\n",
        "        batcher = B.generate_batch(x_train, None)\n",
        "        iteration = 0\n",
        "        nb_batches = np.ceil(1. * len(x_train) / self.BATCH_SIZE)\n",
        "        errs = []\n",
        "        Ds = []\n",
        "        for idx, batch in enumerate(batcher):\n",
        "            self.__train_op_som(batch, iteration, 1)\n",
        "            er, dists = self.__train_op_dnm(batch, iteration, 0)\n",
        "            errs.append(er)\n",
        "            Ds.append(dists)\n",
        "            if (idx + 1) % (nb_batches) == 0:\n",
        "                print(\n",
        "                    'iter: {} \\tloss: {}, dist: {}'.format(iteration + 1, np.array(errs).mean(),\n",
        "                                                           np.array(Ds).mean()))\n",
        "                print(50 * '-')\n",
        "                iteration += 1\n",
        "                Ds = []\n",
        "                errs = []\n",
        "        \n",
        "        hold_final = TemporaryFile() #save weights (deletes at the end of session)\n",
        "        hold_fin = [self.ae_weights, self.som_weights]\n",
        "        np.save(\"hold_final2.npy\", hold_fin)\n",
        "        np.save(hold_final, hold_fin)\n",
        "\n",
        "\n",
        "    def get_locations(self, input_vects):\n",
        "        to_return = []\n",
        "        weight_values = list(np.array(self.som_weights.eval()))\n",
        "        locations = list(np.array(self.locs.eval()))\n",
        "        test = self.__get_ae_output(input_vects.astype('float32'))\n",
        "        for vect in test:\n",
        "            #v = vect.reshape(1,1, self.input_image_size[0],self.input_image_size[1])\n",
        "\n",
        "            v=vect\n",
        "            min_index = min([i for i in range(len(weight_values))],\n",
        "                             key=lambda x: np.linalg.norm(v -\n",
        "                                                          weight_values[x]))\n",
        "            \n",
        "            to_return.append(locations[min_index])\n",
        "        return to_return\n",
        "\n",
        "    def get_reconstructed(self, X):\n",
        "        return self.__get_reconstructed(X.astype('float32'))\n",
        "\n",
        "    def decode_embedding(self, z):\n",
        "        return self.__decode_embedding(z)\n",
        "\n",
        "    def backproject_map(self):\n",
        "        return self.__get_backprojection()\n",
        "\n",
        "    def _pretrain(self, x_train, AE_epochs, SOM_epochs, trial_name, name, location_name, BATCH_SIZE=64):\n",
        "        print('\\nPre-training AE')\n",
        "        print(50 * '=')\n",
        "        B = BatchFactory(BATCH_SIZE, len(x_train), AE_epochs)\n",
        "        batcher = B.generate_batch(x_train, None)\n",
        "        iteration = 0\n",
        "        nb_batches = np.ceil(1. * len(x_train) / BATCH_SIZE)\n",
        "        errs = []\n",
        "        for idx, batch in enumerate(batcher): \n",
        "            er = self.__train_op_ae(batch)\n",
        "            errs.append(er)\n",
        "            if (idx + 1) % (nb_batches) == 0:\n",
        "                print('iter: {} \\ttrain loss: {}'.format(iteration + 1,\n",
        "                                                         np.array(errs).mean()))\n",
        "                print(50 * '-')\n",
        "                iteration += 1\n",
        "                errs = []\n",
        "      \n",
        "        print('\\nPre-training SOM')\n",
        "        print(50 * '=')\n",
        "        B = BatchFactory(BATCH_SIZE, len(x_train), SOM_epochs)\n",
        "        batcher = B.generate_batch(x_train, None)\n",
        "        iteration = 0\n",
        "        nb_batches = np.ceil(1. * len(x_train) / BATCH_SIZE)\n",
        "        errs = []\n",
        "        \n",
        "        for idx, batch in enumerate(batcher):\n",
        "            ers, dists = self.__train_op_som(batch, iteration, 1)\n",
        "            errs.append(np.mean(ers))\n",
        "            if (idx + 1) % nb_batches == 0:\n",
        "                print('iter: {} \\ttrain loss: {}'.format(iteration + 1,\n",
        "                                                         np.array(errs).mean()))\n",
        "                print(50 * '-')\n",
        "                iteration += 1\n",
        "                errs = []\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "deep_map = DNM(input_image_size= len(X_train[0]),\n",
        "                        latent_size= 25,\n",
        "                        lattice_size= (18,18),\n",
        "                        ae_arch_class= miRNA_AE, \n",
        "                        name='test',\n",
        "                        init_params=None, \n",
        "                        sigma=None,\n",
        "                        alpha=None,\n",
        "                        BATCH_SIZE=64,\n",
        "                        ae_lr=1e-3,\n",
        "                        lmbd=1e-6,\n",
        "                        som_pretrain_lr=0.005,\n",
        "                        dnm_map_lr=0.05)\n",
        "\n",
        "\n",
        "deep_map.train(x_train=X_train.astype('float32'),  \n",
        "                                dnm_epochs=1500, trial_name='test', name='test', location_name=None,\n",
        "                                pre_train_epochs=[2500, 1500]) \n",
        "\n",
        "\n",
        "\n",
        "locs = deep_map.get_locations(X_train.astype('float32')) \n",
        "#Plot locs for visualization of DNM\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pre-training AE\n",
            "==================================================\n",
            "iter: 1 \ttrain loss: 88.00889378798803\n",
            "--------------------------------------------------\n",
            "iter: 2 \ttrain loss: 0.07993989301724883\n",
            "--------------------------------------------------\n",
            "iter: 3 \ttrain loss: 0.062425202643702625\n",
            "--------------------------------------------------\n",
            "iter: 4 \ttrain loss: 0.060665060898205815\n",
            "--------------------------------------------------\n",
            "iter: 5 \ttrain loss: 0.06202209445275551\n",
            "--------------------------------------------------\n",
            "iter: 6 \ttrain loss: 0.06071405412996859\n",
            "--------------------------------------------------\n",
            "iter: 7 \ttrain loss: 0.06165345196506775\n",
            "--------------------------------------------------\n",
            "iter: 8 \ttrain loss: 0.06018015421234831\n",
            "--------------------------------------------------\n",
            "iter: 9 \ttrain loss: 0.06003127898957463\n",
            "--------------------------------------------------\n",
            "iter: 10 \ttrain loss: 0.05809347314217089\n",
            "--------------------------------------------------\n",
            "iter: 11 \ttrain loss: 0.06047853607387734\n",
            "--------------------------------------------------\n",
            "iter: 12 \ttrain loss: 0.05867394551590601\n",
            "--------------------------------------------------\n",
            "iter: 13 \ttrain loss: 0.05965881084469604\n",
            "--------------------------------------------------\n",
            "iter: 14 \ttrain loss: 0.05873660727080604\n",
            "--------------------------------------------------\n",
            "iter: 15 \ttrain loss: 0.05660751680737932\n",
            "--------------------------------------------------\n",
            "iter: 16 \ttrain loss: 0.056367283678612136\n",
            "--------------------------------------------------\n",
            "iter: 17 \ttrain loss: 0.05741962555544359\n",
            "--------------------------------------------------\n",
            "iter: 18 \ttrain loss: 0.05299327434395903\n",
            "--------------------------------------------------\n",
            "iter: 19 \ttrain loss: 0.05384113353434796\n",
            "--------------------------------------------------\n",
            "iter: 20 \ttrain loss: 0.05456250509835988\n",
            "--------------------------------------------------\n",
            "iter: 21 \ttrain loss: 0.05472317639596594\n",
            "--------------------------------------------------\n",
            "iter: 22 \ttrain loss: 0.05237542050429006\n",
            "--------------------------------------------------\n",
            "iter: 23 \ttrain loss: 0.05274845308601217\n",
            "--------------------------------------------------\n",
            "iter: 24 \ttrain loss: 0.05245507901995602\n",
            "--------------------------------------------------\n",
            "iter: 25 \ttrain loss: 0.05263914872541171\n",
            "--------------------------------------------------\n",
            "iter: 26 \ttrain loss: 0.052935983870779156\n",
            "--------------------------------------------------\n",
            "\n",
            "Pre-training SOM\n",
            "==================================================\n",
            "iter: 1 \ttrain loss: 0.020087512476118365\n",
            "--------------------------------------------------\n",
            "iter: 2 \ttrain loss: 1.8163649073350872e-05\n",
            "--------------------------------------------------\n",
            "iter: 3 \ttrain loss: 1.7481702078628545e-05\n",
            "--------------------------------------------------\n",
            "iter: 4 \ttrain loss: 1.7402490127993547e-05\n",
            "--------------------------------------------------\n",
            "iter: 5 \ttrain loss: 1.7603602491945576e-05\n",
            "--------------------------------------------------\n",
            "iter: 6 \ttrain loss: 1.7274635615876312e-05\n",
            "--------------------------------------------------\n",
            "iter: 7 \ttrain loss: 1.819124875846411e-05\n",
            "--------------------------------------------------\n",
            "iter: 8 \ttrain loss: 1.680753294706157e-05\n",
            "--------------------------------------------------\n",
            "iter: 9 \ttrain loss: 1.7115083020361894e-05\n",
            "--------------------------------------------------\n",
            "iter: 10 \ttrain loss: 1.612290063508395e-05\n",
            "--------------------------------------------------\n",
            "iter: 11 \ttrain loss: 1.748443552806312e-05\n",
            "--------------------------------------------------\n",
            "iter: 12 \ttrain loss: 1.719000770807109e-05\n",
            "--------------------------------------------------\n",
            "iter: 13 \ttrain loss: 1.6923390521298662e-05\n",
            "--------------------------------------------------\n",
            "iter: 14 \ttrain loss: 1.635060267427726e-05\n",
            "--------------------------------------------------\n",
            "iter: 15 \ttrain loss: 1.822003598184546e-05\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training DNM\n",
            "==================================================\n",
            "iter: 1 \tloss: 0.06479501083327294, dist: 1.8515480887088784e-05\n",
            "--------------------------------------------------\n",
            "iter: 2 \tloss: 0.05129010562636289, dist: 1.880341986735439e-05\n",
            "--------------------------------------------------\n",
            "iter: 3 \tloss: 0.0504529672676066, dist: 2.2199290803404847e-05\n",
            "--------------------------------------------------\n",
            "iter: 4 \tloss: 0.05051245135184965, dist: 3.131016341317423e-05\n",
            "--------------------------------------------------\n",
            "iter: 5 \tloss: 0.050265778918746264, dist: 4.014674167565442e-05\n",
            "--------------------------------------------------\n",
            "iter: 6 \tloss: 0.05202742675511818, dist: 4.859249472206002e-05\n",
            "--------------------------------------------------\n",
            "iter: 7 \tloss: 0.04771496115635008, dist: 7.718483306974279e-05\n",
            "--------------------------------------------------\n",
            "iter: 8 \tloss: 0.05187564385646804, dist: 0.00014499864608908945\n",
            "--------------------------------------------------\n",
            "iter: 9 \tloss: 0.05154416492360969, dist: 0.00020024144361973435\n",
            "--------------------------------------------------\n",
            "iter: 10 \tloss: 0.04967119958250652, dist: 0.00034957676946544605\n",
            "--------------------------------------------------\n",
            "iter: 11 \tloss: 0.050346821556005734, dist: 0.0005312171441100546\n",
            "--------------------------------------------------\n",
            "iter: 12 \tloss: 0.0502764119449813, dist: 0.0016537710868913995\n",
            "--------------------------------------------------\n",
            "iter: 13 \tloss: 0.047858007113729395, dist: 0.0028727625542094813\n",
            "--------------------------------------------------\n",
            "iter: 14 \tloss: 0.045498521241306905, dist: 0.004951406959900307\n",
            "--------------------------------------------------\n",
            "iter: 15 \tloss: 0.036378099189666606, dist: 0.013286683234297077\n",
            "--------------------------------------------------\n",
            "[array([6, 0]), array([ 0, 17]), array([17,  0]), array([2, 0]), array([ 3, 17]), array([ 0, 17]), array([ 3, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([17,  0]), array([17,  0]), array([17,  0]), array([2, 0]), array([1, 4]), array([5, 0]), array([3, 0]), array([ 9, 13]), array([17,  0]), array([10,  0]), array([2, 0]), array([1, 0]), array([13,  0]), array([ 0, 17]), array([17,  0]), array([16,  0]), array([17,  0]), array([ 9, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([0, 4]), array([ 0, 17]), array([17,  0]), array([10, 15]), array([ 0, 17]), array([17,  0]), array([15,  1]), array([17,  0]), array([8, 0]), array([ 0, 17]), array([ 0, 17]), array([ 3, 17]), array([ 2, 17]), array([1, 5]), array([1, 4]), array([0, 3]), array([10, 16]), array([17,  0]), array([ 0, 17]), array([2, 0]), array([15,  0]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 2, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([4, 0]), array([9, 0]), array([1, 4]), array([ 0, 17]), array([ 9, 10]), array([17,  0]), array([17,  0]), array([17,  0]), array([ 6, 17]), array([ 5, 17]), array([10, 15]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([12,  0]), array([ 0, 17]), array([0, 0]), array([17,  0]), array([0, 1]), array([ 2, 17]), array([13,  0]), array([17,  0]), array([ 0, 17]), array([ 6, 17]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([ 2, 17]), array([0, 1]), array([17,  0]), array([2, 0]), array([ 4, 17]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([ 8, 17]), array([ 9, 14]), array([ 0, 17]), array([10, 11]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([10, 14]), array([0, 0]), array([11, 17]), array([17,  0]), array([ 0, 17]), array([ 6, 17]), array([ 0, 17]), array([12,  0]), array([ 0, 17]), array([ 0, 17]), array([2, 0]), array([ 0, 17]), array([ 7, 17]), array([ 4, 17]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([0, 0]), array([ 0, 17]), array([2, 0]), array([17,  0]), array([17,  0]), array([0, 1]), array([ 3, 17]), array([0, 1]), array([17,  0]), array([4, 0]), array([8, 0]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 4, 17]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([17,  0]), array([15,  0]), array([ 0, 17]), array([ 7, 17]), array([ 0, 17]), array([ 5, 17]), array([1, 4]), array([17,  0]), array([15,  0]), array([ 9, 13]), array([17,  0]), array([0, 1]), array([ 9, 11]), array([17,  0]), array([17,  0]), array([0, 3]), array([10, 15]), array([17,  0]), array([ 3, 17]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([ 1, 17]), array([ 5, 17]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 3, 17]), array([17,  0]), array([ 0, 17]), array([0, 1]), array([2, 0]), array([17,  0]), array([14,  0]), array([ 2, 17]), array([ 1, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([16,  0]), array([17,  0]), array([17,  0]), array([11, 12]), array([17,  0]), array([ 9, 15]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([16,  0]), array([ 0, 17]), array([17,  0]), array([0, 0]), array([ 2, 17]), array([17,  0]), array([0, 3]), array([ 0, 17]), array([ 6, 17]), array([ 7, 17]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([0, 4]), array([10,  0]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([8, 0]), array([17,  0]), array([ 2, 17]), array([3, 0]), array([ 9, 12]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([15,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([0, 4]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([15,  0]), array([17,  0]), array([17,  0]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([2, 0]), array([ 9, 10]), array([ 0, 17]), array([17,  0]), array([2, 0]), array([10, 11]), array([ 6, 17]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([ 1, 17]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([10, 17]), array([ 0, 17]), array([16,  0]), array([ 2, 17]), array([10, 11]), array([4, 0]), array([16,  0]), array([17,  0]), array([1, 5]), array([8, 0]), array([ 9, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 4, 17]), array([5, 0]), array([ 9, 14]), array([ 2, 17]), array([ 0, 17]), array([10, 17]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([3, 0]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([17,  0]), array([0, 5]), array([ 7, 17]), array([0, 2]), array([0, 5]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 1, 17]), array([ 8, 10]), array([ 9, 14]), array([10, 16]), array([0, 2]), array([1, 0]), array([11,  0]), array([11, 14]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([0, 3]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([17,  0]), array([4, 0]), array([14,  0]), array([10, 11]), array([ 7, 17]), array([17,  0]), array([17,  0]), array([7, 0]), array([ 0, 17]), array([ 0, 17]), array([0, 2]), array([ 0, 17]), array([ 0, 17]), array([ 9, 16]), array([ 9, 13]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([0, 3]), array([6, 0]), array([0, 0]), array([ 0, 17]), array([17,  0]), array([3, 0]), array([ 9, 15]), array([ 0, 17]), array([1, 0]), array([17,  0]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([0, 1]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([11,  0]), array([4, 0]), array([ 0, 17]), array([ 9, 17]), array([ 5, 17]), array([17,  0]), array([ 0, 17]), array([5, 0]), array([17,  0]), array([6, 0]), array([0, 3]), array([ 9, 15]), array([ 0, 17]), array([10, 11]), array([17,  0]), array([17,  0]), array([7, 0]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([1, 0]), array([4, 0]), array([17,  0]), array([17,  0]), array([13,  0]), array([ 0, 17]), array([10,  0]), array([8, 0]), array([17,  0]), array([ 0, 17]), array([0, 0]), array([17,  0]), array([13,  0]), array([9, 0]), array([ 0, 17]), array([ 0, 17]), array([ 7, 17]), array([2, 0]), array([ 0, 17]), array([ 3, 17]), array([16,  0]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([ 1, 17]), array([ 0, 17]), array([11, 13]), array([10, 13]), array([ 3, 17]), array([ 0, 17]), array([ 0, 17]), array([1, 4]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([ 2, 17]), array([3, 0]), array([7, 0]), array([ 0, 17]), array([ 0, 17]), array([ 5, 17]), array([16,  0]), array([ 4, 17]), array([17,  0]), array([17,  0]), array([17,  0]), array([15,  1]), array([17,  0]), array([17,  0]), array([17,  0]), array([17,  0]), array([10, 12]), array([10, 12]), array([0, 4]), array([10, 12]), array([3, 0]), array([14,  0]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([8, 0]), array([ 0, 17]), array([11, 17]), array([10, 14]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([16,  1]), array([ 4, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([ 3, 17]), array([0, 5]), array([17,  0]), array([17,  0]), array([17,  0]), array([ 1, 17]), array([17,  0]), array([1, 0]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([ 9, 13]), array([ 0, 17]), array([ 0, 17]), array([ 9, 17]), array([ 0, 17]), array([ 9, 15]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([10, 12]), array([5, 0]), array([11,  0]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([ 3, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([10, 16]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([17,  0]), array([1, 5]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([16,  0]), array([ 0, 17]), array([ 2, 17]), array([17,  0]), array([0, 1]), array([ 0, 17]), array([ 0, 17]), array([3, 0]), array([17,  0]), array([1, 5]), array([ 0, 17]), array([7, 0]), array([ 8, 17]), array([10, 15]), array([ 4, 17]), array([ 0, 17]), array([17,  0]), array([11,  0]), array([1, 0]), array([1, 0]), array([17,  0]), array([17,  0]), array([17,  0]), array([11,  0]), array([12,  0]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 3, 17]), array([14,  0]), array([ 3, 17]), array([8, 0]), array([17,  0]), array([0, 1]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([ 9, 16]), array([ 6, 17]), array([ 9, 13]), array([0, 1]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([11, 15]), array([10, 11]), array([ 2, 17]), array([1, 0]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([10, 11]), array([17,  0]), array([16,  0]), array([ 0, 17]), array([ 0, 17]), array([7, 0]), array([17,  0]), array([10, 12]), array([6, 0]), array([14,  0]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([5, 0]), array([17,  0]), array([ 3, 17]), array([1, 0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 4, 17]), array([17,  0]), array([ 0, 17]), array([10, 17]), array([17,  0]), array([5, 0]), array([17,  0]), array([17,  0]), array([17,  0]), array([0, 0]), array([17,  0]), array([ 0, 17]), array([ 9, 14]), array([13,  0]), array([ 0, 17]), array([ 0, 17]), array([16,  0]), array([17,  0]), array([ 2, 17]), array([17,  0]), array([ 0, 17]), array([12,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 3, 17]), array([17,  0]), array([17,  0]), array([16,  0]), array([8, 0]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([17,  1]), array([10, 15]), array([17,  0]), array([17,  0]), array([11, 16]), array([ 0, 17]), array([17,  0]), array([ 1, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 8, 17]), array([10, 16]), array([0, 0]), array([ 2, 17]), array([ 0, 17]), array([17,  0]), array([1, 0]), array([ 0, 17]), array([10, 12]), array([17,  0]), array([ 9, 13]), array([0, 4]), array([ 8, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([ 9, 14]), array([11, 17]), array([ 4, 17]), array([ 4, 17]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([ 9, 13]), array([9, 0]), array([17,  0]), array([17,  0]), array([ 9, 14]), array([ 8, 17]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 1, 17]), array([ 0, 17]), array([ 0, 17]), array([10, 14]), array([17,  0]), array([10,  0]), array([ 2, 17]), array([16,  0]), array([ 0, 17]), array([ 0, 17]), array([10, 16]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([0, 5]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([11,  0]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([10, 15]), array([17,  0]), array([17,  0]), array([ 3, 17]), array([0, 2]), array([17,  0]), array([ 9, 17]), array([ 0, 17]), array([12,  0]), array([ 1, 17]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([0, 1]), array([17,  0]), array([ 0, 17]), array([ 9, 12]), array([17,  0]), array([10, 13]), array([ 0, 17]), array([5, 0]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([10, 13]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([10, 11]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([17,  0]), array([17,  0]), array([17,  0]), array([ 8, 17]), array([12,  0]), array([ 0, 17]), array([ 9, 11]), array([ 9, 15]), array([0, 4]), array([ 1, 17]), array([17,  0]), array([11, 12]), array([17,  0]), array([17,  0]), array([ 3, 17]), array([17,  0]), array([ 9, 10]), array([ 0, 17]), array([11,  0]), array([10, 16]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 5, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([4, 0]), array([17,  0]), array([1, 5]), array([ 7, 17]), array([1, 0]), array([16,  0]), array([10, 16]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([5, 0]), array([17,  0]), array([ 0, 17]), array([8, 0]), array([ 9, 15]), array([17,  0]), array([ 9, 11]), array([ 0, 17]), array([17,  0]), array([15,  1]), array([ 0, 17]), array([ 4, 17]), array([11, 15]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([14,  0]), array([10, 14]), array([10, 14]), array([ 0, 17]), array([ 1, 17]), array([ 3, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([11,  0]), array([15,  0]), array([ 0, 17]), array([ 0, 17]), array([ 9, 15]), array([0, 1]), array([ 0, 17]), array([17,  0]), array([9, 0]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([11, 17]), array([0, 4]), array([ 0, 17]), array([10, 12]), array([1, 5]), array([ 4, 17]), array([0, 0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 9, 17]), array([ 0, 17]), array([11, 15]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([10, 13]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([1, 0]), array([17,  0]), array([17,  0]), array([10, 12]), array([17,  0]), array([17,  0]), array([17,  0]), array([16,  0]), array([17,  0]), array([17,  0]), array([15,  0]), array([ 4, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([0, 4]), array([ 0, 17]), array([ 9, 17]), array([ 3, 17]), array([10,  0]), array([8, 0]), array([0, 2]), array([10, 13]), array([ 9, 11]), array([ 5, 17]), array([ 8, 16]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([1, 0]), array([ 1, 17]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([13,  0]), array([ 1, 17]), array([ 0, 17]), array([0, 0]), array([17,  0]), array([2, 0]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([10, 12]), array([ 0, 17]), array([ 6, 17]), array([ 0, 17]), array([11, 15]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([3, 0]), array([ 0, 17]), array([17,  0]), array([0, 0]), array([ 9, 11]), array([ 5, 17]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([ 9, 17]), array([ 0, 17]), array([0, 4]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([ 2, 17]), array([17,  0]), array([ 6, 17]), array([ 0, 17]), array([ 9, 16]), array([17,  0]), array([ 8, 10]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 5, 17]), array([17,  0]), array([10, 15]), array([ 9, 11]), array([17,  0]), array([17,  0]), array([10, 15]), array([ 0, 17]), array([17,  0]), array([15,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([8, 0]), array([17,  0]), array([10, 11]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([2, 0]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([10,  0]), array([9, 0]), array([ 8, 10]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([16,  0]), array([ 0, 17]), array([7, 0]), array([17,  0]), array([ 9, 10]), array([ 0, 17]), array([17,  0]), array([ 9, 17]), array([ 0, 17]), array([ 0, 17]), array([8, 9]), array([17,  0]), array([1, 0]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([7, 0]), array([ 0, 17]), array([ 9, 16]), array([10, 12]), array([0, 4]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([0, 1]), array([17,  0]), array([15,  0]), array([ 0, 17]), array([17,  0]), array([6, 0]), array([ 0, 17]), array([12,  0]), array([ 8, 10]), array([ 3, 17]), array([ 0, 17]), array([3, 0]), array([ 0, 17]), array([1, 0]), array([17,  0]), array([11, 13]), array([ 0, 17]), array([0, 4]), array([10, 12]), array([ 0, 17]), array([17,  0]), array([ 6, 17]), array([ 0, 17]), array([17,  0]), array([ 1, 17]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([16,  1]), array([ 9, 11]), array([ 0, 17]), array([ 0, 17]), array([0, 5]), array([17,  0]), array([ 1, 17]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([8, 0]), array([17,  0]), array([ 2, 17]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([17,  0]), array([14,  0]), array([17,  0]), array([ 0, 17]), array([10,  0]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([ 1, 17]), array([7, 0]), array([10, 15]), array([ 0, 17]), array([17,  0]), array([3, 0]), array([17,  0]), array([ 8, 17]), array([ 0, 17]), array([10, 17]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([ 6, 17]), array([17,  0]), array([17,  0]), array([1, 0]), array([ 5, 17]), array([17,  0]), array([10, 14]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([1, 0]), array([ 0, 17]), array([10, 11]), array([ 1, 17]), array([10, 14]), array([ 8, 17]), array([17,  0]), array([17,  0]), array([7, 0]), array([17,  0]), array([5, 0]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([9, 0]), array([ 0, 17]), array([3, 0]), array([11, 14]), array([17,  0]), array([17,  0]), array([0, 3]), array([17,  0]), array([11,  0]), array([ 9, 15]), array([ 2, 17]), array([13,  0]), array([17,  0]), array([ 9, 15]), array([ 0, 17]), array([0, 3]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([1, 4]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([ 9, 16]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([2, 0]), array([17,  0]), array([17,  1]), array([11, 15]), array([5, 0]), array([ 0, 17]), array([17,  0]), array([10, 14]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([13,  0]), array([10, 13]), array([17,  0]), array([ 0, 17]), array([0, 5]), array([0, 2]), array([2, 0]), array([ 0, 17]), array([ 3, 17]), array([17,  0]), array([17,  0]), array([0, 4]), array([15,  0]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 2, 17]), array([ 6, 17]), array([0, 2]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([1, 5]), array([17,  0]), array([17,  0]), array([17,  0]), array([ 4, 17]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([ 1, 17]), array([ 8, 16]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([14,  0]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([ 3, 17]), array([ 0, 17]), array([17,  0]), array([0, 2]), array([ 0, 17]), array([10, 13]), array([17,  0]), array([17,  0]), array([17,  0]), array([14,  0]), array([10, 17]), array([8, 0]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([ 5, 17]), array([ 0, 17]), array([ 0, 17]), array([11, 12]), array([17,  0]), array([ 0, 17]), array([12,  0]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([4, 0]), array([17,  0]), array([0, 4]), array([ 0, 17]), array([ 0, 17]), array([0, 3]), array([ 9, 16]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([11,  0]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([10,  0]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([17,  0]), array([ 3, 17]), array([5, 0]), array([6, 0]), array([17,  0]), array([8, 0]), array([ 0, 17]), array([ 0, 17]), array([ 5, 17]), array([17,  0]), array([ 1, 17]), array([ 0, 17]), array([ 8, 17]), array([3, 0]), array([0, 3]), array([ 0, 17]), array([ 8, 17]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([13,  0]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([17,  0]), array([17,  0]), array([17,  0]), array([ 9, 13]), array([ 0, 17]), array([ 7, 17]), array([8, 0]), array([10, 15]), array([ 9, 11]), array([17,  0]), array([0, 5]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([10, 14]), array([8, 0]), array([ 0, 17]), array([15,  0]), array([ 1, 17]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([ 2, 17]), array([ 0, 17]), array([ 8, 10]), array([17,  0]), array([17,  0]), array([ 9, 12]), array([ 0, 17]), array([13,  0]), array([17,  0]), array([ 5, 17]), array([10, 12]), array([ 0, 17]), array([ 0, 17]), array([ 3, 17]), array([17,  0]), array([17,  0]), array([ 4, 17]), array([2, 0]), array([10,  0]), array([17,  0]), array([0, 5]), array([ 0, 17]), array([ 7, 17]), array([ 0, 17]), array([ 3, 17]), array([ 2, 17]), array([ 0, 17]), array([16,  0]), array([11, 15]), array([16,  1]), array([ 4, 17]), array([17,  0]), array([10, 15]), array([17,  0]), array([ 0, 17]), array([0, 1]), array([17,  0]), array([17,  0]), array([10, 11]), array([ 4, 17]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([ 1, 17]), array([17,  0]), array([10, 14]), array([ 0, 17]), array([ 0, 17]), array([5, 0]), array([ 0, 17]), array([ 5, 17]), array([0, 3]), array([17,  0]), array([17,  0]), array([ 3, 17]), array([17,  0]), array([10, 17]), array([11, 14]), array([ 9, 15]), array([17,  0]), array([2, 0]), array([17,  0]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([17,  0]), array([0, 5]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([14,  0]), array([17,  0]), array([ 5, 17]), array([13,  0]), array([7, 0]), array([17,  0]), array([4, 0]), array([0, 4]), array([17,  0]), array([0, 5]), array([ 0, 17]), array([ 7, 17]), array([17,  0]), array([17,  0]), array([17,  0]), array([ 1, 17]), array([11, 13]), array([ 0, 17]), array([ 0, 17]), array([ 2, 17]), array([17,  0]), array([17,  0]), array([17,  0]), array([17,  0]), array([7, 0]), array([10, 17]), array([7, 0]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([17,  0]), array([ 3, 17]), array([17,  0]), array([ 4, 17]), array([ 0, 17]), array([17,  0]), array([11, 17]), array([ 0, 17]), array([ 2, 17]), array([0, 1]), array([10, 10]), array([ 0, 17]), array([1, 0]), array([ 0, 17]), array([17,  0]), array([11, 14]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 3, 17]), array([17,  0]), array([11, 13]), array([10,  0]), array([ 1, 17]), array([10, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([17,  0]), array([ 4, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([11, 14]), array([ 5, 17]), array([0, 2]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([11, 17]), array([ 0, 17]), array([ 9, 15]), array([17,  0]), array([17,  0]), array([17,  0]), array([17,  0]), array([ 6, 17]), array([13,  0]), array([17,  0]), array([ 0, 17]), array([ 3, 17]), array([0, 3]), array([5, 0]), array([17,  0]), array([11, 16]), array([17,  0]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([ 3, 17]), array([ 1, 17]), array([ 0, 17]), array([ 0, 17]), array([ 7, 17]), array([ 0, 17]), array([17,  0]), array([6, 0]), array([7, 0]), array([13,  0]), array([ 0, 17]), array([17,  0]), array([0, 4]), array([ 0, 17]), array([5, 0]), array([17,  0]), array([ 0, 17]), array([8, 0]), array([ 5, 17]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([10, 14]), array([ 3, 17]), array([17,  0]), array([ 2, 17]), array([10, 11]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([ 0, 17]), array([17,  0]), array([ 0, 17]), array([ 8, 17]), array([ 1, 17]), array([ 0, 17]), array([8, 9]), array([ 0, 17]), array([17,  1]), array([17,  0]), array([ 2, 17]), array([10,  0]), array([ 0, 17]), array([10, 13]), array([17,  1]), array([0, 1]), array([17,  0]), array([ 0, 17]), array([17,  0]), array([17,  0]), array([17,  0]), array([ 0, 17]), array([10, 15]), array([ 9, 14]), array([17,  0]), array([0, 1]), array([0, 0]), array([17,  0]), array([17,  0]), array([17,  0]), array([ 3, 17]), array([8, 0]), array([ 0, 17]), array([ 0, 17]), array([ 0, 17]), array([5, 0]), array([ 8, 17]), array([17,  0]), array([ 8, 17]), array([17,  0]), array([ 0, 17]), array([ 1, 17]), array([ 0, 17]), array([0, 0]), array([17,  0]), array([ 0, 17]), array([ 4, 17]), array([17,  0]), array([16,  0]), array([0, 2]), array([ 9, 15]), array([17,  0]), array([ 0, 17])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZA0hfBQsWrZ"
      },
      "source": [
        "#Feature importance - can be done for entire network (all_types), or each different class (each_type)\n",
        "#Method explained in \n",
        "#Exploring microRNA Regulation of Cancer with Context-Aware Deep Cancer Classifier (Pyman et al. 2018)\n",
        "\n",
        "latent_size = 25\n",
        "\n",
        "def feat_vis(X_train): #First find output from AE, then backpropagate output through AE encoder layers to find activation of each input\n",
        "  AE= miRNA_AE(len(X_train[0]), latent_size)\n",
        "  ae_weights = np.array(deep_map.ae_weights)\n",
        "  encoder_output, encoder_params, enc_to_regs = AE._encode(X_train.astype(\"float32\"), ae_weights)\n",
        "  all_summed_grads = []\n",
        "  for i in range(len(X_train)):\n",
        "    y = sigmoid(encoder_output[i])\n",
        "    dy = T.jacobian(y,encoder_params)\n",
        "    get_feat_vis = theano.function(inputs=[], outputs=dy)\n",
        "    grads = get_feat_vis()[0]\n",
        "    summed_grads = np.sum(np.abs(grads[0]), axis=1)\n",
        "    all_summed_grads.append(summed_grads)\n",
        "  final_sum = np.sum(all_summed_grads, axis=0)\n",
        "  return final_sum\n",
        "\n",
        "grad = feat_vis(X_train)\n",
        "#Outputs activation of each miRNA"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
